{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1.3 - Weight Decay</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PilotLeoYan/inside-deep-learning/blob/main/1-linear-regression/1-3-weight-decay.ipynb\">\n",
    "    <img src=\"../images/colab_logo.png\" width=\"32\">Open in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://nbviewer.org/github/PilotLeoYan/inside-deep-learning/blob/main/1-linear-regression/1-3-weight-decay.ipynb\">\n",
    "    <img src=\"../images/jupyter_logo.png\" width=\"32\">Open in Jupyter NBViewer</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue from our multivariate linear regression. \n",
    "Now let's incorporate the $\\ell_2$ regularization ($L_{2}$) into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.13.2', '2.6.0+cu126')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from platform import python_version\n",
    "python_version(), torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj): setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{X} &\\in \\mathbb{R}^{m \\times n} \\\\\n",
    "\\mathbf{Y} &\\in \\mathbb{R}^{m \\times n_{1}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10100, 6)\n",
      "(10100, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import random\n",
    "\n",
    "M: int = 10_100 # number of samples\n",
    "N: int = 6 # number of input features\n",
    "NO: int = 3 # number of output features\n",
    "\n",
    "X, Y = make_regression(\n",
    "    n_samples=M, \n",
    "    n_features=N, \n",
    "    n_targets=NO, \n",
    "    n_informative=N - 1,\n",
    "    bias=random.random(),\n",
    "    noise=1\n",
    ")\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 6]), torch.Size([100, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(X[:100], device=device)\n",
    "Y_train = torch.tensor(Y[:100], device=device)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 6]), torch.Size([10000, 3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = torch.tensor(X[100:], device=device)\n",
    "Y_valid = torch.tensor(Y[100:], device=device)\n",
    "X_valid.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing we are going to modify is the way in which the model weights are updated. \n",
    "The rest, such as parameter initialization and model training, remain unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, n_features: int, out_features: int, lambd: float):\n",
    "        self.w = torch.randn(n_features, out_features, device=device)\n",
    "        self.b = torch.randn(out_features, device=device)\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def copy_params(self, torch_layer: torch.nn.modules.linear.Linear):\n",
    "        \"\"\"\n",
    "        Copy the parameters from a module.linear to this model.\n",
    "\n",
    "        Args:\n",
    "            torch_layer: Pytorch module from which to copy the parameters.\n",
    "        \"\"\"\n",
    "        self.b.copy_(torch_layer.bias.detach().clone())\n",
    "        self.w.copy_(torch_layer.weight.T.detach().clone())\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict the output for input x\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            y_pred: Predicted output tensor of shape (n_samples, out_features).\n",
    "        \"\"\"\n",
    "        return torch.matmul(x, self.w) + self.b\n",
    "\n",
    "    def mse_loss(self, y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "        \"\"\"\n",
    "        MSE loss function between target y_true and y_pred.\n",
    "\n",
    "        Args:\n",
    "            y_true: Target tensor of shape (n_samples, out_features).\n",
    "            y_pred: Predicted tensor of shape (n_samples, out_features).\n",
    "\n",
    "        Returns:\n",
    "            loss: MSE loss between predictions and true values.\n",
    "        \"\"\"\n",
    "        return ((y_pred - y_true)**2).mean().item()\n",
    "\n",
    "    def evaluate(self, x: torch.Tensor, y_true: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Evaluate the model on input x and target y_true using MSE.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (n_samples, n_features).\n",
    "            y_true: Target tensor of shape (n_samples, out_features).\n",
    "\n",
    "        Returns:\n",
    "            loss: MSE loss between predictions and true values.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(x)\n",
    "        return self.mse_loss(y_true, y_pred)\n",
    "\n",
    "    def fit(self, x_train: torch.Tensor, y_train: torch.Tensor, \n",
    "        epochs: int, lr: float, batch_size: int, \n",
    "        x_valid: torch.Tensor, y_valid: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            x_train: Input tensor of shape (n_samples, n_features).\n",
    "            y_train: Target tensor of shape (n_samples,).\n",
    "            epochs: Number of epochs to fit.\n",
    "            lr: learning rate.\n",
    "            batch_size: Int number of batch.\n",
    "            x_valid: Input tensor of shape (n_valid_samples, n_features).\n",
    "            y_valid: Target tensor of shape (n_valid_samples,)\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            loss = []\n",
    "            for batch in range(0, len(y_train), batch_size):\n",
    "                end_batch = batch + batch_size\n",
    "\n",
    "                y_pred = self.predict(x_train[batch:end_batch])\n",
    "\n",
    "                loss.append(self.mse_loss(\n",
    "                    y_train[batch:end_batch], \n",
    "                    y_pred\n",
    "                ))\n",
    "\n",
    "                self.update(\n",
    "                    x_train[batch:end_batch], \n",
    "                    y_train[batch:end_batch], \n",
    "                    y_pred, \n",
    "                    lr\n",
    "                )\n",
    "\n",
    "            loss = round(sum(loss) / len(loss), 4)\n",
    "            loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "            print(f'epoch: {epoch} - MSE: {loss} - MSE_v: {loss_v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of training the model with the gradient of the loss function, \n",
    "we are going to use the objective function $J$. \n",
    "Typically our objective function is as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\hat{\\mathbf{Y}}, \\mathbf{\\theta}) = \n",
    "L(\\hat{\\mathbf{Y}}) + \\text{regularization}\n",
    "$$\n",
    "where $\\mathbf{\\theta}$ is an arbitrary parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Do not use the objective function to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a regularization technique, we will use weight decay, commonly $\\ell_2$ or $L_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ell_2(\\mathbf{\\theta}) = \n",
    "\\frac{\\lambda}{2} \\left\\| \\mathbf{\\theta} \\right\\|^{2}_{2}\n",
    "$$\n",
    "where commonly $\\mathbf{\\theta} \\in \\mathbb{R}^{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we have $\\mathbf{W} \\in \\mathbb{R}^{n \\times n_{1}}$, then we need to do an equivalence operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\ell_2(\\mathbf{W}) &= \\frac{\\lambda}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n_{1}} w_{ij}^{2} \\\\\n",
    "&= \\frac{\\lambda}{2} \\text{sum} \\left( \\mathbf{W}^{2} \\right) \n",
    "\\end{align*}\n",
    "$$\n",
    "where ${\\mathbf{A}}^2$ is element-wise power or also ${\\mathbf{A}}^2 = \\mathbf{A} \\odot \\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### objective function derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{pq}} =\n",
    "\\frac{\\partial L}{\\partial w_{pq}} +\n",
    "\\frac{\\partial \\ell_2}{\\partial w_{pq}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell_2}{\\partial w_{pq}} &=\n",
    "\\frac{\\lambda}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial w_{pq}} \\left( \n",
    "    w_{ij}^{2}\n",
    "\\right) \\\\\n",
    "&= \\lambda w_{pq}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because\n",
    "$$\n",
    "\\frac{\\partial w_{ij}}{\\partial w_{pq}} = \\begin{cases}\n",
    "    1 & \\text{if } i=p, j=q \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, for all $p = 1, \\ldots, n$ and $q = 1, \\ldots, n_{1}$.\n",
    "$$\n",
    "\\frac{\\partial \\ell_2}{\\partial \\mathbf{W}} =\n",
    "\\lambda \\mathbf{W}\n",
    "$$\n",
    "**Remark**: $\\nabla_{\\mathbf{W}}\\ell_2 \\in \\mathbb{R}^{n \\times n_{1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\mathbf{W}} &=\n",
    "{\\color{Orange} {\\frac{\\partial L}{\\partial \\mathbf{W}}}} +\n",
    "{\\color{Cyan} {\\frac{\\partial \\ell_2}{\\partial \\mathbf{W}}}} \\\\\n",
    "&= {\\color{Orange} {\\nabla_{\\mathbf{W}}L}} + \n",
    "{\\color{Cyan} {\\lambda \\mathbf{W}}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def update(self, x: torch.Tensor, y_true: torch.Tensor, y_pred: torch.Tensor, lr: float):\n",
    "    \"\"\"\n",
    "    Update the model parameters with L2 regularization.\n",
    "\n",
    "    Args:\n",
    "       x: Input tensor of shape (n_samples, n_features).\n",
    "       y_true: Target tensor of shape (n_samples, n_features).\n",
    "       y_pred: Predicted output tensor of shape (n_samples, n_features).\n",
    "       lr: Learning rate. \n",
    "    \"\"\"\n",
    "    delta = 2 * (y_pred - y_true) / y_true.numel()\n",
    "    self.b -= lr * delta.sum(axis=0)\n",
    "    self.w -= lr * (torch.matmul(x.T, delta) + self.lambd * self.w) # L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch vs Torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLinearRegression(nn.Module):\n",
    "    def __init__(self, n_features, n_out_features):\n",
    "        super(TorchLinearRegression, self).__init__()\n",
    "        self.layer = nn.Linear(n_features, n_out_features, device=device)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.forward(x)\n",
    "            return self.loss(y_pred, y).item()\n",
    "    \n",
    "    def fit(self, x, y, epochs, lr, batch_size, x_valid, y_valid, weight_decay):\n",
    "        \n",
    "        optimizer = torch.optim.SGD([\n",
    "            {'params': self.layer.weight, 'weight_decay': weight_decay},\n",
    "            {'params': self.layer.bias} # it is important to specify the weight decay for the bias.\n",
    "        ], lr=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loss_t = []\n",
    "            for batch in range(0, len(y), batch_size):\n",
    "                end_batch = batch + batch_size\n",
    "\n",
    "                y_pred = self.forward(x[batch:end_batch])\n",
    "                loss = self.loss(y_pred, y[batch:end_batch])\n",
    "                loss_t.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            loss_t = round(sum(loss_t) / len(loss_t), 4)\n",
    "            loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "            print(f'epoch: {epoch} - MSE: {loss_t} - MSE_v: {loss_v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = TorchLinearRegression(N, NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAMBD: float = 0.01\n",
    "\n",
    "model = LinearRegression(N, NO, LAMBD)\n",
    "model.lambd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPE modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the module path if running locally\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "try:\n",
    "    # Try importing the module normally (for local execution)\n",
    "    from tools.torch_metrics import torch_mape as mape\n",
    "except ModuleNotFoundError:\n",
    "    # If the module is not found, assume the code is running in Google Colab\n",
    "    import subprocess\n",
    "\n",
    "    repo_url = \"https://raw.githubusercontent.com/PilotLeoYan/inside-deep-learning/main/tools/torch_metrics.py\"\n",
    "    local_file = \"torch_metrics.py\"\n",
    "\n",
    "    # Download the missing file from GitHub\n",
    "    subprocess.run([\"wget\", repo_url, \"-O\", local_file], check=True)\n",
    "\n",
    "    # Import the module after downloading it\n",
    "    import torch_metrics\n",
    "    from torch_metrics import torch_mape as mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2821.8635146478346"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.copy_params(torch_model.layer)\n",
    "parameters = (model.b.clone(), model.w.clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.evaluate(X_valid, Y_valid),\n",
    "    torch_model.evaluate(X_valid, Y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01 # learning rate\n",
    "EPOCHS = 16 # number of epochs\n",
    "BATCH = len(X_train) // 3 # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 13870.4868 - MSE_v: 16318.3121\n",
      "epoch: 1 - MSE: 13194.1362 - MSE_v: 15633.8015\n",
      "epoch: 2 - MSE: 12558.2412 - MSE_v: 14982.5659\n",
      "epoch: 3 - MSE: 11959.8227 - MSE_v: 14362.6399\n",
      "epoch: 4 - MSE: 11396.1592 - MSE_v: 13772.201\n",
      "epoch: 5 - MSE: 10864.7625 - MSE_v: 13209.557\n",
      "epoch: 6 - MSE: 10363.3552 - MSE_v: 12673.1352\n",
      "epoch: 7 - MSE: 9889.8509 - MSE_v: 12161.4721\n",
      "epoch: 8 - MSE: 9442.3365 - MSE_v: 11673.2047\n",
      "epoch: 9 - MSE: 9019.0557 - MSE_v: 11207.0622\n",
      "epoch: 10 - MSE: 8618.3946 - MSE_v: 10761.8582\n",
      "epoch: 11 - MSE: 8238.8689 - MSE_v: 10336.4843\n",
      "epoch: 12 - MSE: 7879.1116 - MSE_v: 9929.9038\n",
      "epoch: 13 - MSE: 7537.8628 - MSE_v: 9541.146\n",
      "epoch: 14 - MSE: 7213.9598 - MSE_v: 9169.3014\n",
      "epoch: 15 - MSE: 6906.3287 - MSE_v: 8813.5165\n"
     ]
    }
   ],
   "source": [
    "torch_model.fit(\n",
    "    X_train, Y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    X_valid, Y_valid,\n",
    "    LAMBD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 13870.4868 - MSE_v: 16318.3121\n",
      "epoch: 1 - MSE: 13194.1362 - MSE_v: 15633.8015\n",
      "epoch: 2 - MSE: 12558.2412 - MSE_v: 14982.5659\n",
      "epoch: 3 - MSE: 11959.8227 - MSE_v: 14362.6399\n",
      "epoch: 4 - MSE: 11396.1592 - MSE_v: 13772.201\n",
      "epoch: 5 - MSE: 10864.7625 - MSE_v: 13209.557\n",
      "epoch: 6 - MSE: 10363.3552 - MSE_v: 12673.1352\n",
      "epoch: 7 - MSE: 9889.8509 - MSE_v: 12161.4721\n",
      "epoch: 8 - MSE: 9442.3365 - MSE_v: 11673.2047\n",
      "epoch: 9 - MSE: 9019.0557 - MSE_v: 11207.0622\n",
      "epoch: 10 - MSE: 8618.3946 - MSE_v: 10761.8582\n",
      "epoch: 11 - MSE: 8238.8689 - MSE_v: 10336.4843\n",
      "epoch: 12 - MSE: 7879.1116 - MSE_v: 9929.9038\n",
      "epoch: 13 - MSE: 7537.8628 - MSE_v: 9541.146\n",
      "epoch: 14 - MSE: 7213.9598 - MSE_v: 9169.3014\n",
      "epoch: 15 - MSE: 6906.3287 - MSE_v: 8813.5165\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, Y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.987696834165537e-14"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1089260444557068e-12"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.w.clone(),\n",
    "    torch_model.layer.weight.detach().T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.703433843016858e-14"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.b.clone(),\n",
    "    torch_model.layer.bias.detach()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
