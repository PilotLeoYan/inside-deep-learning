{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1.2 - Multivariate Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PilotLeoYan/inside-deep-learning/blob/main/1-linear-regression/1-2-multivariate-linear-regression.ipynb\">\n",
    "    <img src=\"../images/colab_logo.png\" />Open in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to increase the complexity, \n",
    "instead of the perceptron having a single output, \n",
    "it will now have multiple outputs. \n",
    "The word \"multivariable\" usually means that the perceptron receives multiple inputs, \n",
    "but here we will use it to describe that the perceptron has multiple outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: black\">\n",
    "<img src=\"../images/multivariate-perceptron.png\" alt=\"One multivariate perceptron\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the multivariate perceptron as a layer of multiple simple perceptrons, \n",
    "and that each perceptron output corresponds to an output feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: black\">\n",
    "<img src=\"../images/multivariate-perceptron-as-layer.png\" alt=\"One layer of simple perceptron\" width=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.12.6', '2.5.1+cu124')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from platform import python_version\n",
    "python_version(), torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj): setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{X} &\\in \\mathbb{R}^{m \\times n} \\\\\n",
    "\\mathbf{Y} &\\in \\mathbb{R}^{m \\times n_{1}}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $n_{1}$ is the number of output features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & \\cdots & x_{mn}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix}\n",
    "    y_{11} & y_{12} & \\cdots & y_{1n_{1}} \\\\\n",
    "    y_{21} & y_{22} & \\cdots & y_{2n_{1}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    y_{m1} & y_{m2} & \\cdots & y_{mn_{1}} \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10100, 6)\n",
      "(10100, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import random\n",
    "\n",
    "M: int = 10_100 # number of samples\n",
    "N: int = 6 # number of input features\n",
    "NO: int = 3 # number of output features\n",
    "\n",
    "X, Y = make_regression(\n",
    "    n_samples=M, \n",
    "    n_features=N, \n",
    "    n_targets=NO, \n",
    "    n_informative=N - 1,\n",
    "    bias=random.random(),\n",
    "    noise=1\n",
    ")\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 6]), torch.Size([100, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(X[:100], device=device)\n",
    "Y_train = torch.tensor(Y[:100], device=device)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 6]), torch.Size([10000, 3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = torch.tensor(X[100:], device=device)\n",
    "Y_valid = torch.tensor(Y[100:], device=device)\n",
    "X_valid.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{W} &\\in \\mathbb{R}^{n \\times n_{1}} \\\\\n",
    "\\mathbf{b} &\\in \\mathbb{R}^{n_{1}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{W} = \\begin{bmatrix}\n",
    "    w_{11} & w_{12} & \\cdots & w_{1n_{1}} \\\\\n",
    "    w_{21} & w_{22} & \\cdots & w_{2n_{1}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{n1} & w_{n2} & \\cdots & w_{nn_{1}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{b} = \\begin{bmatrix}\n",
    "    b_{1} \\\\\n",
    "    b_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    b_{n_{1}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, n_features: int, out_features: int):\n",
    "        self.w = torch.randn(n_features, out_features, device=device)\n",
    "        self.b = torch.randn(out_features, device=device)\n",
    "\n",
    "    def copy_params(self, torch_layer: torch.nn.modules.linear.Linear):\n",
    "        \"\"\"\n",
    "        Copy the parameters from a module.linear to this model.\n",
    "\n",
    "        Args:\n",
    "            torch_layer: Pytorch module from which to copy the parameters.\n",
    "        \"\"\"\n",
    "        self.b.copy_(torch_layer.bias.detach().clone())\n",
    "        self.w.copy_(torch_layer.weight.T.detach().clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weighted sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\hat{Y}}(\\mathbf{X}) = \\mathbf{X}\\mathbf{W} + \\mathbf{b} \\\\\n",
    "\\mathbf{\\hat{Y}} : \\mathbb{R}^{m \\times n} \\rightarrow \n",
    "\\mathbb{R}^{m \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "$$\n",
    "\\hat{y}_{ij} =\n",
    "\\mathbf{x}_{i}^\\top\n",
    "\\mathbf{w}_{:,j}\n",
    "+ b_{j}\n",
    "$$\n",
    "for all $i = 1, \\ldots, m$ and $j = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Predict the output for input x\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        y_pred: Predicted output tensor of shape (n_samples, out_features).\n",
    "    \"\"\"\n",
    "    return torch.matmul(x, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "L(\\mathbf{\\hat{Y}}) &= \\frac{1}{mn_{1}} \n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}(\n",
    "    \\hat{y}_{ij} - y_{ij})^{2} \\\\\n",
    "L &: \\mathbb{R}^{m \\times n_{1}} \\rightarrow \\mathbb{R}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = \\frac{1}{mn_{1}} \\text{sum} \\left(\n",
    "    \\left(\n",
    "        \\mathbf{\\hat{Y} - Y}\n",
    "    \\right)^2\n",
    "\\right)\n",
    "$$\n",
    "where ${\\mathbf{A}}^2$ is element-wise power or also ${\\mathbf{A}}^2 = \\mathbf{A} \\odot \\mathbf{A}$. <br>\n",
    "**Note**: $\\odot$ is called element-wise product or also Hadamard product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def mse_loss(self, y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "    \"\"\"\n",
    "    MSE loss function between target y_true and y_pred.\n",
    "\n",
    "    Args:\n",
    "        y_true: Target tensor of shape (n_samples, out_features).\n",
    "        y_pred: Predicted tensor of shape (n_samples, out_features).\n",
    "\n",
    "    Returns:\n",
    "        loss: MSE loss between predictions and true values.\n",
    "    \"\"\"\n",
    "    return ((y_pred - y_true)**2).mean().item()\n",
    "\n",
    "@add_to_class(LinearRegression)\n",
    "def evaluate(self, x: torch.Tensor, y_true: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Evaluate the model on input x and target y_true using MSE.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples, n_features).\n",
    "        y_true: Target tensor of shape (n_samples, out_features).\n",
    "\n",
    "    Returns:\n",
    "        loss: MSE loss between predictions and true values.\n",
    "    \"\"\"\n",
    "    y_pred = self.predict(x)\n",
    "    return self.mse_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to compute gradients\n",
    "1. Computing each derivative individually and then joining them using the Einstein summation.\n",
    "2. Computing an initial derivative and passing it backwards as an argument.\n",
    "\n",
    "The most common way is to use method 2 \n",
    "because it is easier to visualize and is more optimal. \n",
    "While method 1 needs more computing. \n",
    "We prefer method 2, but we will also use method 1 just for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{pq}} &=\n",
    "\\frac{1}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial \\hat{y}_{pq}} \n",
    "\\left(\n",
    "    (\\hat{y}_{ij} - y_{ij})^2\n",
    "\\right) \\\\\n",
    "&= \\frac{2}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "(\\hat{y}_{ij} - y_{ij})\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial \\hat{y}_{pq}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, \\ldots, m$ and $q = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial \\hat{y}_{pq}} =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } i=p, j=q \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{pq}} &=\n",
    "\\frac{2}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "(\\hat{y}_{ij} - y_{ij})\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial \\hat{y}_{pq}} \\\\\n",
    "&= \\frac{2}{mn_{1}} (\\hat{y}_{pq} - y_{pq})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}} =\n",
    "\\frac{2}{mn_{1}} \\left(\n",
    "    \\mathbf{\\hat{Y}} - \\mathbf{Y}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted sum derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### respect to bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial b_{p}} &=\n",
    "\\frac{1}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial b_{p}} \n",
    "\\left(\n",
    "    (\\hat{y}_{ij} - y_{ij})^2\n",
    "\\right) \\\\\n",
    "&= \\frac{2}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "(\\hat{y}_{ij} - y_{ij})\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}} \\\\\n",
    "&= \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}}\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}} =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } j=p \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial b_{p}} &=\n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}}\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}} \\\\\n",
    "&= \\sum_{i=1}^{m}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ip}}\n",
    "\\end{align*}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} =\n",
    "\\mathbf{1} \\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}\n",
    "$$\n",
    "where $\\mathbf{1} \\in \\mathbb{R}^{m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### respect to weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_{pq}} &=\n",
    "\\frac{1}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial w_{pq}} \n",
    "\\left(\n",
    "    (\\hat{y}_{ij} - y_{ij})^2\n",
    "\\right) \\\\\n",
    "&= \\frac{2}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "(\\hat{y}_{ij} - y_{ij})\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}} \\\\\n",
    "&= \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}}\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, \\ldots, n$ and $q = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}} =\n",
    "\\begin{cases}\n",
    "    x_{ip} & \\text{if } j=q \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_{pq}} &=\n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}}\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}} \\\\\n",
    "&= \\sum_{i=1}^{m} x_{ip}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{iq}} \\\\\n",
    "&= (x_{:,p})^\\top\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{:,q}} \\\\\n",
    "&= x^\\top_{p,:}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{:,q}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} =\n",
    "\\mathbf{X}^\\top\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{b}}L =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &=\n",
    "{\\color{Cyan} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}}}\n",
    "{\\color{Orange} {\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{b}}}} \\\\\n",
    "&= {\\color{Cyan} {\\frac{2}{mn_{1}}}}\n",
    "{\\color{Orange} {\\mathbf{1}}}\n",
    "{\\color{Cyan} {\\left(\\mathbf{\\hat{Y}} - \\mathbf{Y} \\right)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{W}}L =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &=\n",
    "{\\color{Cyan} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}}}\n",
    "{\\color{Magenta} {\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}}}} \\\\\n",
    "&= {\\color{Cyan} {\\frac{2}{mn_{1}}}}\n",
    "{\\color{Magenta} {\\mathbf{X}^\\top}}\n",
    "{\\color{Cyan} {\\left(\\mathbf{\\hat{Y}} - \\mathbf{Y} \\right)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def update(self, x: torch.Tensor, y_true: torch.Tensor, y_pred: torch.Tensor, lr: float):\n",
    "    \"\"\"\n",
    "    Update the model parameters.\n",
    "\n",
    "    Args:\n",
    "       x: Input tensor of shape (n_samples, n_features).\n",
    "       y_true: Target tensor of shape (n_samples, n_features).\n",
    "       y_pred: Predicted output tensor of shape (n_samples, n_features).\n",
    "       lr: Learning rate. \n",
    "    \"\"\"\n",
    "    delta = 2 * (y_pred - y_true) / y_true.numel()\n",
    "    self.b -= lr * delta.sum(axis=0)\n",
    "    self.w -= lr * torch.matmul(x.T, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def fit(self, x_train: torch.Tensor, y_train: torch.Tensor, \n",
    "        epochs: int, lr: float, batch_size: int, \n",
    "        x_valid: torch.Tensor, y_valid: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Fit the model using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        x_train: Input tensor of shape (n_samples, n_features).\n",
    "        y_train: Target tensor of shape (n_samples,).\n",
    "        epochs: Number of epochs to fit.\n",
    "        lr: learning rate.\n",
    "        batch_size: Int number of batch.\n",
    "        x_valid: Input tensor of shape (n_valid_samples, n_features).\n",
    "        y_valid: Target tensor of shape (n_valid_samples,)\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        loss = []\n",
    "        for batch in range(0, len(y_train), batch_size):\n",
    "            end_batch = batch + batch_size\n",
    "\n",
    "            y_pred = self.predict(x_train[batch:end_batch])\n",
    "\n",
    "            loss.append(self.mse_loss(\n",
    "                y_train[batch:end_batch], \n",
    "                y_pred\n",
    "            ))\n",
    "\n",
    "            self.update(\n",
    "                x_train[batch:end_batch], \n",
    "                y_train[batch:end_batch], \n",
    "                y_pred, \n",
    "                lr\n",
    "            )\n",
    "\n",
    "        loss = round(sum(loss) / len(loss), 4)\n",
    "        loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "        print(f'epoch: {epoch} - MSE: {loss} - MSE_v: {loss_v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch vs Torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLinearRegression(nn.Module):\n",
    "    def __init__(self, n_features, n_out_features):\n",
    "        super(TorchLinearRegression, self).__init__()\n",
    "        self.layer = nn.Linear(n_features, n_out_features, device=device)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.forward(x)\n",
    "            return self.loss(y_pred, y).item()\n",
    "    \n",
    "    def fit(self, x, y, epochs, lr, batch_size, x_valid, y_valid):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            loss_t = []\n",
    "            for batch in range(0, len(y), batch_size):\n",
    "                end_batch = batch + batch_size\n",
    "\n",
    "                y_pred = self.forward(x[batch:end_batch])\n",
    "                loss = self.loss(y_pred, y[batch:end_batch])\n",
    "                loss_t.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            loss_t = round(sum(loss_t) / len(loss_t), 4)\n",
    "            loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "            print(f'epoch: {epoch} - MSE: {loss_t} - MSE_v: {loss_v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = TorchLinearRegression(N, NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(N, NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPE modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the module path if running locally\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "try:\n",
    "    # Try importing the module normally (for local execution)\n",
    "    from tools.torch_metrics import torch_mape as mape\n",
    "except ModuleNotFoundError:\n",
    "    # If the module is not found, assume the code is running in Google Colab\n",
    "    import subprocess\n",
    "\n",
    "    repo_url = \"https://raw.githubusercontent.com/PilotLeoYan/inside-deep-learning/main/tools/torch_metrics.py\"\n",
    "    local_file = \"torch_metrics.py\"\n",
    "\n",
    "    # Download the missing file from GitHub\n",
    "    subprocess.run([\"wget\", repo_url, \"-O\", local_file], check=True)\n",
    "\n",
    "    # Import the module after downloading it\n",
    "    import torch_metrics\n",
    "    from torch_metrics import torch_mape as mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801.3082740656382"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.copy_params(torch_model.layer)\n",
    "parameters = (model.b.clone(), model.w.clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.evaluate(X_valid, Y_valid),\n",
    "    torch_model.evaluate(X_valid, Y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01 # learning rate\n",
    "EPOCHS = 16 # number of epochs\n",
    "BATCH = len(X_train) // 3 # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 23450.5454 - MSE_v: 17270.5571\n",
      "epoch: 1 - MSE: 21600.057 - MSE_v: 16147.6228\n",
      "epoch: 2 - MSE: 19916.0383 - MSE_v: 15111.7659\n",
      "epoch: 3 - MSE: 18382.2784 - MSE_v: 14155.2469\n",
      "epoch: 4 - MSE: 16984.2129 - MSE_v: 13271.0748\n",
      "epoch: 5 - MSE: 15708.7527 - MSE_v: 12452.9314\n",
      "epoch: 6 - MSE: 14544.1318 - MSE_v: 11695.1036\n",
      "epoch: 7 - MSE: 13479.7698 - MSE_v: 10992.4219\n",
      "epoch: 8 - MSE: 12506.1498 - MSE_v: 10340.2066\n",
      "epoch: 9 - MSE: 11614.7088 - MSE_v: 9734.2181\n",
      "epoch: 10 - MSE: 10797.739 - MSE_v: 9170.6135\n",
      "epoch: 11 - MSE: 10048.3004 - MSE_v: 8645.9068\n",
      "epoch: 12 - MSE: 9360.1413 - MSE_v: 8156.9337\n",
      "epoch: 13 - MSE: 8727.6283 - MSE_v: 7700.8198\n",
      "epoch: 14 - MSE: 8145.683 - MSE_v: 7274.9523\n",
      "epoch: 15 - MSE: 7609.7249 - MSE_v: 6876.9539\n"
     ]
    }
   ],
   "source": [
    "torch_model.fit(\n",
    "    X_train, Y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 23450.5454 - MSE_v: 17270.5571\n",
      "epoch: 1 - MSE: 21600.057 - MSE_v: 16147.6228\n",
      "epoch: 2 - MSE: 19916.0383 - MSE_v: 15111.7659\n",
      "epoch: 3 - MSE: 18382.2784 - MSE_v: 14155.2469\n",
      "epoch: 4 - MSE: 16984.2129 - MSE_v: 13271.0748\n",
      "epoch: 5 - MSE: 15708.7527 - MSE_v: 12452.9314\n",
      "epoch: 6 - MSE: 14544.1318 - MSE_v: 11695.1036\n",
      "epoch: 7 - MSE: 13479.7698 - MSE_v: 10992.4219\n",
      "epoch: 8 - MSE: 12506.1498 - MSE_v: 10340.2066\n",
      "epoch: 9 - MSE: 11614.7088 - MSE_v: 9734.2181\n",
      "epoch: 10 - MSE: 10797.739 - MSE_v: 9170.6135\n",
      "epoch: 11 - MSE: 10048.3004 - MSE_v: 8645.9068\n",
      "epoch: 12 - MSE: 9360.1413 - MSE_v: 8156.9337\n",
      "epoch: 13 - MSE: 8727.6283 - MSE_v: 7700.8198\n",
      "epoch: 14 - MSE: 8145.683 - MSE_v: 7274.9523\n",
      "epoch: 15 - MSE: 7609.7249 - MSE_v: 6876.9539\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, Y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.278887730502764e-14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.22480178152855e-15"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.w.clone(),\n",
    "    torch_model.layer.weight.detach().T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3582561077298844e-14"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.b.clone(),\n",
    "    torch_model.layer.bias.detach()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute gradient with einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{b}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where their shapes are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{n \\times n_{1}} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{n_{1}} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}} &\\in \\mathbb{R}^{m \\times n_{1}} \\\\\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}\n",
    "{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{(m \\times n_{1}) \\times (n \\times n_{1})} \\\\\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}\n",
    "{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{(m \\times n_{1}) \\times (n_{1})}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: check $\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}}$\n",
    "has four axes. This is an example because this method requires more computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted sum derivative respect to bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}} = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } j=p \\\\ \n",
    "    0 & \\text{if } j\\neq p \n",
    "\\end{cases}\n",
    "$$\n",
    "for all $i = 1, \\ldots, m$ and $j, p = 1, \\ldots, n_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted sum derivative respect to weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}} = \n",
    "\\begin{cases}\n",
    "    x_{ip} & \\text{if } j=q \\\\ \n",
    "    0 & \\text{if } j\\neq q \n",
    "\\end{cases}\n",
    "$$\n",
    "for all $i = 1, \\ldots, m$,<br>\n",
    "$j, q = 1, \\ldots, n_{1}$ and <br>\n",
    "$p = 1, \\ldots, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}} = \n",
    "\\mathbb{I} \\otimes \\mathbf{X}\n",
    "$$\n",
    "where $\\otimes$ is Kronecker product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore using **Einstein summation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "{\\color{Magenta} {\\frac{\\partial L}{\\partial \\mathbf{b}}}} &=\n",
    "{\\color{Orange} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}}}\n",
    "{\\color{Cyan} {\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{b}}}} \\\\\n",
    "&\\in \\mathbb{R}^{\n",
    "    {\\color{Orange} {(m \\times n_{1})}} \\times \n",
    "    {\\color{Cyan} {(m \\times n_{1} \\times n_{1})}}} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} {n_{1}}}\n",
    "\\end{align*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "{\\color{Magenta} {\\frac{\\partial L}{\\partial \\mathbf{W}}}} &=\n",
    "{\\color{Orange} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}}}\n",
    "{\\color{Cyan} {\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}}}} \\\\\n",
    "&\\in \\mathbb{R}^{\n",
    "    {\\color{Orange} {(m \\times n_{1})}} \\times \n",
    "    {\\color{Cyan} {(m \\times n_{1} \\times n \\times n_{1})}}} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} {n \\times n_{1}}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EinsumLinearRegression(LinearRegression):\n",
    "    def update(self, x: torch.Tensor, y_true: torch.Tensor, y_pred: torch.Tensor, lr: float):\n",
    "        \"\"\"\n",
    "        Update the model parameters.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (n_samples, n_features).\n",
    "            y_true: Target tensor of shape (n_samples, n_features).\n",
    "            y_pred: Predicted output tensor of shape (n_samples, n_features).\n",
    "            lr: Learning rate. \n",
    "        \"\"\"\n",
    "        delta = 2 * (y_pred - y_true) / y_true.numel()\n",
    "        # d L / d b\n",
    "        self.b -= lr * delta.sum(axis=0)\n",
    "        # d L / d W\n",
    "        identity = torch.eye(y_true.shape[-1], device=device)\n",
    "        w_der = torch.kron(\n",
    "            x.unsqueeze(1).unsqueeze(3),\n",
    "            identity.unsqueeze(0).unsqueeze(2)\n",
    "        )\n",
    "        self.w -= lr * torch.einsum('pq,pqij->ij', delta, w_der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4061, -0.1745, -0.1816],\n",
       "        [ 0.3046, -0.1626,  0.3205],\n",
       "        [ 0.0931,  0.1911, -0.3605],\n",
       "        [ 0.1585,  0.0218,  0.3361],\n",
       "        [ 0.2438, -0.1821,  0.4021],\n",
       "        [-0.1901,  0.0661,  0.0360]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einsum_model = EinsumLinearRegression(N, NO)\n",
    "einsum_model.b.copy_(parameters[0])\n",
    "einsum_model.w.copy_(parameters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 23450.5454 - MSE_v: 17270.5571\n",
      "epoch: 1 - MSE: 21600.057 - MSE_v: 16147.6228\n",
      "epoch: 2 - MSE: 19916.0383 - MSE_v: 15111.7659\n",
      "epoch: 3 - MSE: 18382.2784 - MSE_v: 14155.2469\n",
      "epoch: 4 - MSE: 16984.2129 - MSE_v: 13271.0748\n",
      "epoch: 5 - MSE: 15708.7527 - MSE_v: 12452.9314\n",
      "epoch: 6 - MSE: 14544.1318 - MSE_v: 11695.1036\n",
      "epoch: 7 - MSE: 13479.7698 - MSE_v: 10992.4219\n",
      "epoch: 8 - MSE: 12506.1498 - MSE_v: 10340.2066\n",
      "epoch: 9 - MSE: 11614.7088 - MSE_v: 9734.2181\n",
      "epoch: 10 - MSE: 10797.739 - MSE_v: 9170.6135\n",
      "epoch: 11 - MSE: 10048.3004 - MSE_v: 8645.9068\n",
      "epoch: 12 - MSE: 9360.1413 - MSE_v: 8156.9337\n",
      "epoch: 13 - MSE: 8727.6283 - MSE_v: 7700.8198\n",
      "epoch: 14 - MSE: 8145.683 - MSE_v: 7274.9523\n",
      "epoch: 15 - MSE: 7609.7249 - MSE_v: 6876.9539\n"
     ]
    }
   ],
   "source": [
    "einsum_model.fit(\n",
    "    X_train, Y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0018713882983006e-14"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    einsum_model.w.clone(),\n",
    "    torch_model.layer.weight.detach().T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3582561077298844e-14"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    einsum_model.b.clone(),\n",
    "    torch_model.layer.bias.detach()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
