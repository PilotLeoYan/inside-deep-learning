{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 - Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PilotLeoYan/inside-deep-learning/blob/main/1-linear-regression/1-2-multivariate-linear-regression.ipynb\">\n",
    "    <img src=\"../images/colab_logo.png\" width=\"32\">Open in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://nbviewer.org/github/PilotLeoYan/inside-deep-learning/blob/main/1-linear-regression/1-1-simple-linear-regression.ipynb\">\n",
    "    <img src=\"../images/jupyter_logo.png\" width=\"32\">Open in Jupyter NBViewer</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to increase the complexity, \n",
    "instead of the perceptron having a single output, \n",
    "it will now have multiple outputs. \n",
    "The word \"multivariable\" usually means that the perceptron receives multiple inputs, \n",
    "but here we will use it to describe that the perceptron has multiple outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: black\">\n",
    "<img src=\"../images/multivariate-perceptron.png\" alt=\"One multivariate perceptron\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the multivariate perceptron as a layer of multiple simple perceptrons, \n",
    "and that each perceptron output corresponds to an output feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: black\">\n",
    "<img src=\"../images/multivariate-perceptron-as-layer.png\" alt=\"One layer of simple perceptron\" width=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose of this Notebook**:\n",
    "\n",
    "The purposes of this notebook are:\n",
    "1. Create a dataset for multivariate linear regression task\n",
    "2. Create our own Multivariate Perceptron class from scratch\n",
    "3. Calculate the gradient descent from scratch\n",
    "4. Train our Multivariate Perceptron\n",
    "5. Compare our Perceptron to the one prebuilt by PyTorch\n",
    "6. [Extra] Calculate the gradient descent by other way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.13.5', '2.7.1+cu128')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from platform import python_version\n",
    "python_version(), torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj): setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{X} &\\in \\mathbb{R}^{m \\times n} \\\\\n",
    "\\mathbf{Y} &\\in \\mathbb{R}^{m \\times n_{1}}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $n_{1}$ is the number of output features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & \\cdots & x_{mn}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix}\n",
    "    y_{11} & y_{12} & \\cdots & y_{1n_{1}} \\\\\n",
    "    y_{21} & y_{22} & \\cdots & y_{2n_{1}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    y_{m1} & y_{m2} & \\cdots & y_{mn_{1}} \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10100, 6)\n",
      "(10100, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import random\n",
    "\n",
    "M: int = 10_100 # number of samples\n",
    "N: int = 6 # number of input features\n",
    "NO: int = 3 # number of output features\n",
    "\n",
    "X, Y = make_regression(\n",
    "    n_samples=M, \n",
    "    n_features=N, \n",
    "    n_targets=NO, \n",
    "    n_informative=N - 1,\n",
    "    bias=random.random(),\n",
    "    noise=1\n",
    ")\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 6]), torch.Size([100, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(X[:100], device=device)\n",
    "Y_train = torch.tensor(Y[:100], device=device)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 6]), torch.Size([10000, 3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = torch.tensor(X[100:], device=device)\n",
    "Y_valid = torch.tensor(Y[100:], device=device)\n",
    "X_valid.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{W} &\\in \\mathbb{R}^{n \\times n_{1}} \\\\\n",
    "\\mathbf{b} &\\in \\mathbb{R}^{n_{1}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{W} = \\begin{bmatrix}\n",
    "    w_{11} & w_{12} & \\cdots & w_{1n_{1}} \\\\\n",
    "    w_{21} & w_{22} & \\cdots & w_{2n_{1}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{n1} & w_{n2} & \\cdots & w_{nn_{1}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{b} = \\begin{bmatrix}\n",
    "    b_{1} \\\\\n",
    "    b_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    b_{n_{1}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, n_features: int, out_features: int):\n",
    "        self.w = torch.randn(n_features, out_features, device=device)\n",
    "        self.b = torch.randn(out_features, device=device)\n",
    "\n",
    "    def copy_params(self, torch_layer: torch.nn.modules.linear.Linear):\n",
    "        \"\"\"\n",
    "        Copy the parameters from a module.linear to this model.\n",
    "\n",
    "        Args:\n",
    "            torch_layer: Pytorch module from which to copy the parameters.\n",
    "        \"\"\"\n",
    "        self.b.copy_(torch_layer.bias.detach().clone())\n",
    "        self.w.copy_(torch_layer.weight.T.detach().clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weighted sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\hat{Y}}(\\mathbf{X}) = \\mathbf{X}\\mathbf{W} + \\mathbf{b} \\\\\n",
    "\\mathbf{\\hat{Y}} : \\mathbb{R}^{m \\times n} \\rightarrow \n",
    "\\mathbb{R}^{m \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "$$\n",
    "\\hat{y}_{ij} =\n",
    "\\mathbf{x}_{i}^\\top\n",
    "\\mathbf{w}_{:,j}\n",
    "+ b_{j}\n",
    "$$\n",
    "for all $i = 1, \\ldots, m$ and $j = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Predict the output for input x\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        y_pred: Predicted output tensor of shape (n_samples, out_features).\n",
    "    \"\"\"\n",
    "    return torch.matmul(x, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "L(\\mathbf{\\hat{Y}}) &= \\frac{1}{mn_{1}} \n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}(\n",
    "    \\hat{y}_{ij} - y_{ij})^{2} \\\\\n",
    "L &: \\mathbb{R}^{m \\times n_{1}} \\rightarrow \\mathbb{R}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = \\frac{1}{mn_{1}} \\text{sum} \\left(\n",
    "    \\left(\n",
    "        \\mathbf{\\hat{Y} - Y}\n",
    "    \\right)^2\n",
    "\\right)\n",
    "$$\n",
    "where ${\\mathbf{A}}^2$ is element-wise power or also ${\\mathbf{A}}^2 = \\mathbf{A} \\odot \\mathbf{A}$. <br>\n",
    "**Note**: $\\odot$ is called element-wise product or also Hadamard product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def mse_loss(self, y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "    \"\"\"\n",
    "    MSE loss function between target y_true and y_pred.\n",
    "\n",
    "    Args:\n",
    "        y_true: Target tensor of shape (n_samples, out_features).\n",
    "        y_pred: Predicted tensor of shape (n_samples, out_features).\n",
    "\n",
    "    Returns:\n",
    "        loss: MSE loss between predictions and true values.\n",
    "    \"\"\"\n",
    "    return ((y_pred - y_true)**2).mean().item()\n",
    "\n",
    "@add_to_class(LinearRegression)\n",
    "def evaluate(self, x: torch.Tensor, y_true: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Evaluate the model on input x and target y_true using MSE.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples, n_features).\n",
    "        y_true: Target tensor of shape (n_samples, out_features).\n",
    "\n",
    "    Returns:\n",
    "        loss: MSE loss between predictions and true values.\n",
    "    \"\"\"\n",
    "    y_pred = self.predict(x)\n",
    "    return self.mse_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to compute gradients\n",
    "1. Computing each derivative individually and then joining them using the Einstein summation.\n",
    "2. Computing an initial derivative and passing it backwards as an argument.\n",
    "\n",
    "The most common way is to use method 2 \n",
    "because it is easier to visualize and is more optimal. \n",
    "While method 1 needs more computing. \n",
    "We prefer method 2, but we will also use method 1 just for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{pq}} &=\n",
    "\\frac{1}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial \\hat{y}_{pq}} \n",
    "\\left(\n",
    "    (\\hat{y}_{ij} - y_{ij})^2\n",
    "\\right) \\\\\n",
    "&= \\frac{2}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "(\\hat{y}_{ij} - y_{ij})\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial \\hat{y}_{pq}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, \\ldots, m$ and $q = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial \\hat{y}_{pq}} =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } i=p, j=q \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{pq}} &=\n",
    "\\frac{2}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "(\\hat{y}_{ij} - y_{ij})\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial \\hat{y}_{pq}} \\\\\n",
    "&= \\frac{2}{mn_{1}} (\\hat{y}_{pq} - y_{pq})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}} =\n",
    "\\frac{2}{mn_{1}} \\left(\n",
    "    \\mathbf{\\hat{Y}} - \\mathbf{Y}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted sum derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### respect to bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial b_{p}} &=\n",
    "\\frac{1}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial b_{p}} \n",
    "\\left(\n",
    "    (\\hat{y}_{ij} - y_{ij})^2\n",
    "\\right) \\\\\n",
    "&= \\frac{2}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "(\\hat{y}_{ij} - y_{ij})\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}} \\\\\n",
    "&= \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}}\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}} =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } j=p \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial b_{p}} &=\n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}}\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}} \\\\\n",
    "&= \\sum_{i=1}^{m}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ip}}\n",
    "\\end{align*}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} =\n",
    "\\mathbf{1} \\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}\n",
    "$$\n",
    "where $\\mathbf{1} \\in \\mathbb{R}^{m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### respect to weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_{pq}} &=\n",
    "\\frac{1}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial w_{pq}} \n",
    "\\left(\n",
    "    (\\hat{y}_{ij} - y_{ij})^2\n",
    "\\right) \\\\\n",
    "&= \\frac{2}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "(\\hat{y}_{ij} - y_{ij})\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}} \\\\\n",
    "&= \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}}\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, \\ldots, n$ and $q = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}} =\n",
    "\\begin{cases}\n",
    "    x_{ip} & \\text{if } j=q \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_{pq}} &=\n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}}\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}} \\\\\n",
    "&= \\sum_{i=1}^{m} x_{ip}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{iq}} \\\\\n",
    "&= (x_{:,p})^\\top\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{:,q}} \\\\\n",
    "&= x^\\top_{p,:}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{:,q}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} =\n",
    "\\mathbf{X}^\\top\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{b}}L =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &=\n",
    "{\\color{Cyan} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}}}\n",
    "{\\color{Orange} {\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{b}}}} \\\\\n",
    "&= {\\color{Cyan} {\\frac{2}{mn_{1}}}}\n",
    "{\\color{Orange} {\\mathbf{1}}}\n",
    "{\\color{Cyan} {\\left(\\mathbf{\\hat{Y}} - \\mathbf{Y} \\right)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{W}}L =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &=\n",
    "{\\color{Cyan} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}}}\n",
    "{\\color{Magenta} {\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}}}} \\\\\n",
    "&= {\\color{Cyan} {\\frac{2}{mn_{1}}}}\n",
    "{\\color{Magenta} {\\mathbf{X}^\\top}}\n",
    "{\\color{Cyan} {\\left(\\mathbf{\\hat{Y}} - \\mathbf{Y} \\right)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def update(self, x: torch.Tensor, y_true: torch.Tensor, y_pred: torch.Tensor, lr: float):\n",
    "    \"\"\"\n",
    "    Update the model parameters.\n",
    "\n",
    "    Args:\n",
    "       x: Input tensor of shape (n_samples, n_features).\n",
    "       y_true: Target tensor of shape (n_samples, n_features).\n",
    "       y_pred: Predicted output tensor of shape (n_samples, n_features).\n",
    "       lr: Learning rate. \n",
    "    \"\"\"\n",
    "    delta = 2 * (y_pred - y_true) / y_true.numel()\n",
    "    self.b -= lr * delta.sum(axis=0)\n",
    "    self.w -= lr * torch.matmul(x.T, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def fit(self, x_train: torch.Tensor, y_train: torch.Tensor, \n",
    "        epochs: int, lr: float, batch_size: int, \n",
    "        x_valid: torch.Tensor, y_valid: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Fit the model using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        x_train: Input tensor of shape (n_samples, n_features).\n",
    "        y_train: Target tensor of shape (n_samples,).\n",
    "        epochs: Number of epochs to fit.\n",
    "        lr: learning rate.\n",
    "        batch_size: Int number of batch.\n",
    "        x_valid: Input tensor of shape (n_valid_samples, n_features).\n",
    "        y_valid: Target tensor of shape (n_valid_samples,)\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        loss = []\n",
    "        for batch in range(0, len(y_train), batch_size):\n",
    "            end_batch = batch + batch_size\n",
    "\n",
    "            y_pred = self.predict(x_train[batch:end_batch])\n",
    "\n",
    "            loss.append(self.mse_loss(\n",
    "                y_train[batch:end_batch], \n",
    "                y_pred\n",
    "            ))\n",
    "\n",
    "            self.update(\n",
    "                x_train[batch:end_batch], \n",
    "                y_train[batch:end_batch], \n",
    "                y_pred, \n",
    "                lr\n",
    "            )\n",
    "\n",
    "        loss = round(sum(loss) / len(loss), 4)\n",
    "        loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "        print(f'epoch: {epoch} - MSE: {loss} - MSE_v: {loss_v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch vs Torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLinearRegression(nn.Module):\n",
    "    def __init__(self, n_features, n_out_features):\n",
    "        super(TorchLinearRegression, self).__init__()\n",
    "        self.layer = nn.Linear(n_features, n_out_features, device=device)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.forward(x)\n",
    "            return self.loss(y_pred, y).item()\n",
    "    \n",
    "    def fit(self, x, y, epochs, lr, batch_size, x_valid, y_valid):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            loss_t = []\n",
    "            for batch in range(0, len(y), batch_size):\n",
    "                end_batch = batch + batch_size\n",
    "\n",
    "                y_pred = self.forward(x[batch:end_batch])\n",
    "                loss = self.loss(y_pred, y[batch:end_batch])\n",
    "                loss_t.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            loss_t = round(sum(loss_t) / len(loss_t), 4)\n",
    "            loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "            print(f'epoch: {epoch} - MSE: {loss_t} - MSE_v: {loss_v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = TorchLinearRegression(N, NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(N, NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import MAPE modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import importlib.util\n",
    "from collections.abc import Callable\n",
    "\n",
    "def import_mape(module_path: str = '..') -> Callable:\n",
    "    \"\"\"\n",
    "    Tries to import the 'torch_mape' function from a local project structure.\n",
    "    If it fails (ModuleNotFoundError), it assumes a cloud environment (like Colab),\n",
    "    downloads the module from GitHub, imports it, and returns the function.\n",
    "\n",
    "    Args:\n",
    "        module_path (str): The relative path to the project's root directory\n",
    "                           for the local search.\n",
    "\n",
    "    Returns:\n",
    "        The imported 'torch_mape' function, or None if it fails.\n",
    "    \"\"\"\n",
    "    GITHUB_RAW_URL = 'https://raw.githubusercontent.com/PilotLeoYan/inside-deep-learning/main/tools/torch_metrics.py'\n",
    "    MODULE_NAME = 'torch_metrics'\n",
    "    LOCAL_FILE_NAME = f'{MODULE_NAME}.py'\n",
    "\n",
    "    try:\n",
    "        # Attempt 1: Standard import (if the package is installed or in PYTHONPATH)\n",
    "        from tools.torch_metrics import torch_mape\n",
    "        print(\"‚úÖ Module 'tools.torch_metrics' successfully imported from the environment.\")\n",
    "        return torch_mape\n",
    "\n",
    "    except ModuleNotFoundError:\n",
    "        # Attempt 2: Search in the specified local path (original behavior)\n",
    "        # This is useful for local development without installing the package.\n",
    "        project_path = os.path.abspath(os.path.join(module_path))\n",
    "        if project_path not in sys.path:\n",
    "            sys.path.insert(0, project_path)\n",
    "\n",
    "        try:\n",
    "            from tools.torch_metrics import torch_mape\n",
    "            print(\"‚úÖ Local module 'tools.torch_metrics' imported after adjusting the path.\")\n",
    "            # Remove the added path to avoid side effects\n",
    "            sys.path.pop(0)\n",
    "            return torch_mape\n",
    "        except ModuleNotFoundError:\n",
    "            # If both local attempts fail, proceed with the download\n",
    "            if project_path in sys.path:\n",
    "                sys.path.pop(0) # Clean up the path if it was added\n",
    "            print(f\"‚ö†Ô∏è Local module not found. Proceeding to download from GitHub...\")\n",
    "\n",
    "    # Download and Dynamic Loading Logic}\n",
    "    if not os.path.exists(LOCAL_FILE_NAME):\n",
    "        try:\n",
    "            print(f\"‚¨áÔ∏è  Downloading '{LOCAL_FILE_NAME}' from GitHub...\")\n",
    "            response = requests.get(GITHUB_RAW_URL)\n",
    "            response.raise_for_status()  # This will raise an error if the HTTP request failed\n",
    "            with open(LOCAL_FILE_NAME, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text)\n",
    "            print(\"üëç Download complete.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Error downloading the file: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Dynamically load the module using importlib\n",
    "    spec = importlib.util.spec_from_file_location(MODULE_NAME, LOCAL_FILE_NAME)\n",
    "    dynamic_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(dynamic_module)\n",
    "    \n",
    "    print(f\"‚úÖ Module '{MODULE_NAME}' successfully loaded from the downloaded file.\")\n",
    "    return dynamic_module.torch_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Local module 'tools.torch_metrics' imported after adjusting the path.\n"
     ]
    }
   ],
   "source": [
    "mape = import_mape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2755.8281930910707"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.copy_params(torch_model.layer)\n",
    "parameters = (model.b.clone(), model.w.clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.evaluate(X_valid, Y_valid),\n",
    "    torch_model.evaluate(X_valid, Y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01 # learning rate\n",
    "EPOCHS = 16 # number of epochs\n",
    "BATCH = len(X_train) // 3 # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 10489.8677 - MSE_v: 13423.9767\n",
      "epoch: 1 - MSE: 9907.9363 - MSE_v: 12905.274\n",
      "epoch: 2 - MSE: 9380.4891 - MSE_v: 12413.495\n",
      "epoch: 3 - MSE: 8899.9842 - MSE_v: 11946.2902\n",
      "epoch: 4 - MSE: 8460.0964 - MSE_v: 11501.6265\n",
      "epoch: 5 - MSE: 8055.512 - MSE_v: 11077.7368\n",
      "epoch: 6 - MSE: 7681.7585 - MSE_v: 10673.0767\n",
      "epoch: 7 - MSE: 7335.0639 - MSE_v: 10286.2903\n",
      "epoch: 8 - MSE: 7012.2393 - MSE_v: 9916.1802\n",
      "epoch: 9 - MSE: 6710.5818 - MSE_v: 9561.6835\n",
      "epoch: 10 - MSE: 6427.794 - MSE_v: 9221.8512\n",
      "epoch: 11 - MSE: 6161.9172 - MSE_v: 8895.8317\n",
      "epoch: 12 - MSE: 5911.2753 - MSE_v: 8582.8564\n",
      "epoch: 13 - MSE: 5674.4291 - MSE_v: 8282.228\n",
      "epoch: 14 - MSE: 5450.1379 - MSE_v: 7993.3106\n",
      "epoch: 15 - MSE: 5237.3273 - MSE_v: 7715.5214\n"
     ]
    }
   ],
   "source": [
    "torch_model.fit(\n",
    "    X_train, Y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 10489.8677 - MSE_v: 13423.9767\n",
      "epoch: 1 - MSE: 9907.9363 - MSE_v: 12905.274\n",
      "epoch: 2 - MSE: 9380.4891 - MSE_v: 12413.495\n",
      "epoch: 3 - MSE: 8899.9842 - MSE_v: 11946.2902\n",
      "epoch: 4 - MSE: 8460.0964 - MSE_v: 11501.6265\n",
      "epoch: 5 - MSE: 8055.512 - MSE_v: 11077.7368\n",
      "epoch: 6 - MSE: 7681.7585 - MSE_v: 10673.0767\n",
      "epoch: 7 - MSE: 7335.0639 - MSE_v: 10286.2903\n",
      "epoch: 8 - MSE: 7012.2393 - MSE_v: 9916.1802\n",
      "epoch: 9 - MSE: 6710.5818 - MSE_v: 9561.6835\n",
      "epoch: 10 - MSE: 6427.794 - MSE_v: 9221.8512\n",
      "epoch: 11 - MSE: 6161.9172 - MSE_v: 8895.8317\n",
      "epoch: 12 - MSE: 5911.2753 - MSE_v: 8582.8564\n",
      "epoch: 13 - MSE: 5674.4291 - MSE_v: 8282.228\n",
      "epoch: 14 - MSE: 5450.1379 - MSE_v: 7993.3106\n",
      "epoch: 15 - MSE: 5237.3273 - MSE_v: 7715.5214\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, Y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5151110652358848e-14"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1373335505084408e-14"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.w.clone(),\n",
    "    torch_model.layer.weight.detach().T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.445753264039646e-14"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.b.clone(),\n",
    "    torch_model.layer.bias.detach()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute gradient with einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{b}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where their shapes are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{n \\times n_{1}} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{n_{1}} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}} &\\in \\mathbb{R}^{m \\times n_{1}} \\\\\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}\n",
    "{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{(m \\times n_{1}) \\times (n \\times n_{1})} \\\\\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}\n",
    "{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{(m \\times n_{1}) \\times (n_{1})}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: check $\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}}$\n",
    "has four axes. This is an example because this method requires more computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted sum derivative respect to bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}} = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } j=p \\\\ \n",
    "    0 & \\text{if } j\\neq p \n",
    "\\end{cases}\n",
    "$$\n",
    "for all $i = 1, \\ldots, m$ and $j, p = 1, \\ldots, n_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted sum derivative respect to weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}} = \n",
    "\\begin{cases}\n",
    "    x_{ip} & \\text{if } j=q \\\\ \n",
    "    0 & \\text{if } j\\neq q \n",
    "\\end{cases}\n",
    "$$\n",
    "for all $i = 1, \\ldots, m$,<br>\n",
    "$j, q = 1, \\ldots, n_{1}$ and <br>\n",
    "$p = 1, \\ldots, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}} = \n",
    "\\mathbb{I} \\otimes \\mathbf{X}\n",
    "$$\n",
    "where $\\otimes$ is Kronecker product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore using **Einstein summation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "{\\color{Magenta} {\\frac{\\partial L}{\\partial \\mathbf{b}}}} &=\n",
    "{\\color{Orange} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}}}\n",
    "{\\color{Cyan} {\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{b}}}} \\\\\n",
    "&\\in \\mathbb{R}^{\n",
    "    {\\color{Orange} {(m \\times n_{1})}} \\times \n",
    "    {\\color{Cyan} {(m \\times n_{1} \\times n_{1})}}} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} {n_{1}}}\n",
    "\\end{align*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "{\\color{Magenta} {\\frac{\\partial L}{\\partial \\mathbf{W}}}} &=\n",
    "{\\color{Orange} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}}}\n",
    "{\\color{Cyan} {\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}}}} \\\\\n",
    "&\\in \\mathbb{R}^{\n",
    "    {\\color{Orange} {(m \\times n_{1})}} \\times \n",
    "    {\\color{Cyan} {(m \\times n_{1} \\times n \\times n_{1})}}} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} {n \\times n_{1}}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EinsumLinearRegression(LinearRegression):\n",
    "    def update(self, x: torch.Tensor, y_true: torch.Tensor, y_pred: torch.Tensor, lr: float):\n",
    "        \"\"\"\n",
    "        Update the model parameters.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (n_samples, n_features).\n",
    "            y_true: Target tensor of shape (n_samples, n_features).\n",
    "            y_pred: Predicted output tensor of shape (n_samples, n_features).\n",
    "            lr: Learning rate. \n",
    "        \"\"\"\n",
    "        delta = 2 * (y_pred - y_true) / y_true.numel()\n",
    "        # d L / d b\n",
    "        self.b -= lr * delta.sum(axis=0)\n",
    "        # d L / d W\n",
    "        identity = torch.eye(y_true.shape[-1], device=device)\n",
    "        w_der = torch.kron(\n",
    "            x.unsqueeze(1).unsqueeze(3),\n",
    "            identity.unsqueeze(0).unsqueeze(2)\n",
    "        )\n",
    "        self.w -= lr * torch.einsum('pq,pqij->ij', delta, w_der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2958, -0.1393, -0.1585],\n",
       "        [-0.2927, -0.2159,  0.1224],\n",
       "        [ 0.3049,  0.3151,  0.2705],\n",
       "        [-0.2140, -0.2987,  0.1171],\n",
       "        [-0.1359, -0.1334,  0.1145],\n",
       "        [ 0.2041,  0.2827, -0.1440]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einsum_model = EinsumLinearRegression(N, NO)\n",
    "einsum_model.b.copy_(parameters[0])\n",
    "einsum_model.w.copy_(parameters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 10489.8677 - MSE_v: 13423.9767\n",
      "epoch: 1 - MSE: 9907.9363 - MSE_v: 12905.274\n",
      "epoch: 2 - MSE: 9380.4891 - MSE_v: 12413.495\n",
      "epoch: 3 - MSE: 8899.9842 - MSE_v: 11946.2902\n",
      "epoch: 4 - MSE: 8460.0964 - MSE_v: 11501.6265\n",
      "epoch: 5 - MSE: 8055.512 - MSE_v: 11077.7368\n",
      "epoch: 6 - MSE: 7681.7585 - MSE_v: 10673.0767\n",
      "epoch: 7 - MSE: 7335.0639 - MSE_v: 10286.2903\n",
      "epoch: 8 - MSE: 7012.2393 - MSE_v: 9916.1802\n",
      "epoch: 9 - MSE: 6710.5818 - MSE_v: 9561.6835\n",
      "epoch: 10 - MSE: 6427.794 - MSE_v: 9221.8512\n",
      "epoch: 11 - MSE: 6161.9172 - MSE_v: 8895.8317\n",
      "epoch: 12 - MSE: 5911.2753 - MSE_v: 8582.8564\n",
      "epoch: 13 - MSE: 5674.4291 - MSE_v: 8282.228\n",
      "epoch: 14 - MSE: 5450.1379 - MSE_v: 7993.3106\n",
      "epoch: 15 - MSE: 5237.3273 - MSE_v: 7715.5214\n"
     ]
    }
   ],
   "source": [
    "einsum_model.fit(\n",
    "    X_train, Y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.019670707304044e-15"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    einsum_model.w.clone(),\n",
    "    torch_model.layer.weight.detach().T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9174206062379856e-14"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    einsum_model.b.clone(),\n",
    "    torch_model.layer.bias.detach()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-vs-idl-3-13-5 (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
