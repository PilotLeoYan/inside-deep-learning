{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1.1 - Simple Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PilotLeoYan/inside-deep-learning/blob/main/1-linear-regression/1-1-simple-linear-regression.ipynb\">\n",
    "    <img src=\"../images/colab_logo.png\" />Open in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to start with a topic before getting into deep learning, \n",
    "the perceptron is a good place to start, \n",
    "as it is the basic unit with which artificial neural networks (ANNs) are built.\n",
    "We can then use multiple perceptrons in parallel to form a dense layer. \n",
    "By using multiple dense layers, we can build a deep neural network (DNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: black\">\n",
    "<img src=\"../images/simple-perceptron.png\" alt=\"One multivariate perceptron\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first topic is to train a perceptron for the simple linear regression task. \n",
    "We refer to “simple” as predicting a single output from multiple inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{R}^{n} \\rightarrow \\mathbb{R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.12.6', '2.5.1+cu124')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from platform import python_version\n",
    "python_version(), torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj): setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our supervised task, we need two set\n",
    "$\\mathbf{X}$ and $\\mathbf{y}$, where\n",
    "$\\mathbf{X}$ is called input data and $\\mathbf{y}$ is called target data. <br>\n",
    "In this case, $\\mathbf{X}$ and $\\mathbf{y}$ are a matrix and a vector respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{X} &\\in \\mathbb{R}^{m \\times n} \\\\\n",
    "\\mathbf{y} &\\in \\mathbb{R}^{m}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $m$ is the number of samples and\n",
    "$n$ is the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & \\cdots & x_{mn}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix}\n",
    "    y_{1} \\\\\n",
    "    y_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    y_{m}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10100, 4)\n",
      "(10100,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import random\n",
    "\n",
    "M: int = 10_100 # number of samples\n",
    "N: int = 4 # number of features\n",
    "\n",
    "X, Y = make_regression(\n",
    "    n_samples=M, \n",
    "    n_features=N, \n",
    "    n_targets=1,\n",
    "    n_informative=N - 1,\n",
    "    bias=random.random(), # random true bias\n",
    "    noise=1\n",
    ")\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 4]), torch.Size([100]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(X[:100], device=device)\n",
    "Y_train = torch.tensor(Y[:100], device=device)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 4]), torch.Size([10000]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = torch.tensor(X[100:], device=device)\n",
    "Y_valid = torch.tensor(Y[100:], device=device)\n",
    "X_valid.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete raw dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w} &\\in \\mathbb{R}^{n} \\\\\n",
    "b &\\in \\mathbb{R}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{w}$ is called weight and $b$ is called bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{w} = \\begin{bmatrix}\n",
    "    w_{1} \\\\\n",
    "    w_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    w_{n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegression:\n",
    "    def __init__(self, n_features: int):\n",
    "        self.w = torch.randn(n_features, device=device)\n",
    "        self.b = torch.randn(1, device=device)\n",
    "\n",
    "    def copy_params(self, torch_layer: nn.modules.linear.Linear):\n",
    "        \"\"\"\n",
    "        Copy the parameters from a module.linear to this model.\n",
    "\n",
    "        Args:\n",
    "            torch_layer: Pytorch module from which to copy the parameters.\n",
    "        \"\"\"\n",
    "        self.b.copy_(torch_layer.bias.detach().clone())\n",
    "        self.w.copy_(torch_layer.weight[0,:].detach().clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weighted sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{\\hat{y}}(\\mathbf{X}) = \\mathbf{X}\\mathbf{w} + b \\\\\n",
    "\\mathbf{\\hat{y}} : \\mathbb{R}^{m \\times n} \\rightarrow \n",
    "\\mathbb{R}^{m}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{\\hat{y}}$ is called predicted output data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{\\hat{y}} &= \\mathbf{X} \\mathbf{w} + b \\\\\n",
    "&= \\begin{bmatrix}\n",
    "        \\mathbf{x}_{1}^\\top \\\\\n",
    "        \\mathbf{x}_{2}^\\top \\\\\n",
    "        \\vdots \\\\\n",
    "        \\mathbf{x}_{m}^\\top \\\\\n",
    "    \\end{bmatrix} \\mathbf{w} + b \\\\\n",
    "&= \\begin{bmatrix}\n",
    "        \\mathbf{x}_{1}^\\top \\mathbf{w} + b \\\\\n",
    "        \\mathbf{x}_{2}^\\top \\mathbf{w} + b \\\\\n",
    "        \\vdots \\\\\n",
    "        \\mathbf{x}_{m}^\\top \\mathbf{w} + b \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "$$\n",
    "\\mathbf{x}_{i}^\\top = \\begin{bmatrix}\n",
    "        x_{i1} & x_{i2} & \\cdots & x_{in}\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Predict the output for input x.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        y_pred: Predicted output tensor of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    return torch.matmul(x, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a loss function. We will use Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "L(\\mathbf{\\hat{y}}) &= \\frac{1}{m} \\sum_{i=1}^{m}(\n",
    "    \\hat{y}_i - y_i)^{2} \\\\\n",
    "L &: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "L(\\mathbf{\\hat{y}}) &= \\frac{1}{m} \n",
    "    \\left\\| \\mathbf{e} \\right\\|_{2}^2 \\\\\n",
    "\\mathbf{e} &:= \\mathbf{\\hat{y}} - \\mathbf{y}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def mse_loss(self, y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "    \"\"\"\n",
    "    MSE loss function between target y_true and y_pred.\n",
    "\n",
    "    Args:\n",
    "        y_true: Target tensor of shape (n_samples,).\n",
    "        y_pred: Predicted tensor of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        loss: MSE loss between predictions and true values.\n",
    "    \"\"\"\n",
    "    return ((y_pred - y_true)**2).mean().item()\n",
    "\n",
    "@add_to_class(SimpleLinearRegression)\n",
    "def evaluate(self, x: torch.Tensor, y_true: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Evaluate the model on input x and target y_true using MSE.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples, n_features).\n",
    "        y_true: Target tensor of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        loss: MSE loss between predictions and true values.\n",
    "    \"\"\"\n",
    "    y_pred = self.predict(x)\n",
    "    return self.mse_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial b} =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}}\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}}\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where their shapes are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} \\in \\mathbb{R}^{n},\n",
    "\\frac{\\partial L}{\\partial b} \\in \\mathbb{R},\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}} \\in \\mathbb{R}^{m},\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}} \\in \\mathbb{R}^{m \\times n},\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial b} \\in \\mathbb{R}^{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}_{1}} &\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}_{m}}\n",
    "\\end{bmatrix}^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{p}} &=\n",
    "\\frac{\\partial}{\\partial \\hat{y}_{p}} \\left(\n",
    "    \\frac{1}{m} \\sum_{i=1}^{m}(\n",
    "    \\hat{y}_i - y_i)^{2}\n",
    "\\right) \\\\\n",
    "&= \\frac{2}{m} (\\hat{y}_{p} - y_{p})\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, \\ldots, m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}} =\n",
    "\\frac{2}{m} \\left(\n",
    "    \\mathbf{\\hat{y}} - \\mathbf{y}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted sum derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### respect to bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial b} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\hat{y}_{1}}{\\partial b} &\n",
    "    \\frac{\\partial \\hat{y}_{2}}{\\partial b} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\hat{y}_{m}}{\\partial b}\n",
    "\\end{bmatrix}^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\hat{y}_{p}}{\\partial b} &=\n",
    "\\frac{\\partial}{\\partial b} \\left(\n",
    "    \\mathbf{x}_{p}^\\top \\mathbf{w} + b\n",
    "\\right) \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, \\ldots, m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial b} =\n",
    "\\mathbf{1} \\in \\mathbb{R}^{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### respect to weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\hat{y}_{1}}{\\partial w_{1}} & \\frac{\\partial \\hat{y}_{1}}{\\partial w_{2}}\n",
    "    & \\cdots & \\frac{\\partial \\hat{y}_{1}}{\\partial w_{n}} \\\\\n",
    "    \\frac{\\partial \\hat{y}_{2}}{\\partial w_{1}} & \\frac{\\partial \\hat{y}_{2}}{\\partial w_{2}}\n",
    "    & \\cdots & \\frac{\\partial \\hat{y}_{2}}{\\partial w_{n}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial \\hat{y}_{m}}{\\partial w_{1}} & \\frac{\\partial \\hat{y}_{m}}{\\partial w_{2}}\n",
    "    & \\cdots & \\frac{\\partial \\hat{y}_{m}}{\\partial w_{n}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\hat{y}_{p}}{\\partial w_{q}} &=\n",
    "\\frac{\\partial}{\\partial w_{q}} \\left(\n",
    "    \\mathbf{x}_{p}^\\top \\mathbf{w} + b\n",
    "\\right) \\\\\n",
    "&= x_{pq}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, \\ldots, m$ and $q = 1, \\ldots, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}} =\n",
    "\\mathbf{X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{b}L =\n",
    "\\frac{\\partial L}{\\partial b} &=\n",
    "{\\color{Cyan} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}}}}\n",
    "{\\color{Orange} {\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial b}}} \\\\\n",
    "&= {\\color{Cyan} {\\frac{2}{m} \\left(\\mathbf{\\hat{y}} - \\mathbf{y} \\right)}}\n",
    "{\\color{Orange} {\\mathbf{1}}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{w}}L =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} &=\n",
    "{\\color{Cyan} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}}}}\n",
    "{\\color{Magenta} {\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}}}} \\\\\n",
    "&= {\\color{Cyan} {\\frac{2}{m} \\left(\\mathbf{\\hat{y}} - \\mathbf{y} \\right)}}\n",
    "{\\color{Magenta} {\\mathbf{X}}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w} := \\mathbf{w} -\\eta \\nabla_{\\mathbf{w}}L &=\n",
    "\\mathbf{w} -\\eta \\left(\n",
    "    \\frac{2}{m} (\\mathbf{\\hat{y}} - \\mathbf{y}) \\mathbf{X}\n",
    "\\right)\n",
    "\\\\\n",
    "b := b -\\eta \\nabla_{b}L &=\n",
    "b -\\eta \\left(\n",
    "    \\frac{2}{m} (\\mathbf{\\hat{y}} - \\mathbf{y}) \\mathbf{1}\n",
    "\\right)\n",
    "\\end{align*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def update(self, x: torch.Tensor, y_true: torch.Tensor, y_pred: torch.Tensor, lr: float):\n",
    "    \"\"\"\n",
    "    Update the model parameters.\n",
    "\n",
    "    Args:\n",
    "       x: Input tensor of shape (n_samples, n_features).\n",
    "       y_true: Target tensor of shape (n_samples,).\n",
    "       y_pred: Predicted output tensor of shape (n_samples,).\n",
    "       lr: Learning rate. \n",
    "    \"\"\"\n",
    "    delta = 2 * (y_pred - y_true) / len(y_true)\n",
    "    self.b -= lr * delta.sum()\n",
    "    self.w -= lr * torch.matmul(delta, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have assumed that we will use the entire train dataset to update our parameters, \n",
    "but we can use only a fraction of the samples in our train dataset to update our parameters.\n",
    "There are mainly 3 ways to use Gradient descent (GD).\n",
    "- batch GD\n",
    "- stochastic GD (SGD)\n",
    "- mini-batch GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch GD uses all samples of train dataset to update our parameters:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Algorithm 1: batch Gradient Descent} \\\\\n",
    "\\textbf{for } t = 1 \\text{ to } T \\textbf{ do}\\\\\n",
    "\\quad \\mathbf{\\theta} \\leftarrow \\text{update}(\\mathbf{X}, \\mathbf{y}; \\mathbf{\\theta}) \\\\\n",
    "\\textbf{end for}\n",
    "\\end{array}\n",
    "$$\n",
    "where $T$ is the number of epochs. <br>\n",
    "**Remark**: $\\mathbf{\\theta}$ is an arbitrary parameter, for this model we have to update $\\mathbf{w}$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stochastic GD (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGD for each epoch, we update our parameters for each \n",
    "sample in our train dataset:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Algorithm 2: stochastic Gradient Descent (SGD)} \\\\\n",
    "\\textbf{for } t = 1 \\text{ to } T \\textbf{ do}\\\\\n",
    "\\quad \\textbf{for } i = 1 \\text{ to } m \\textbf{ do} \\\\\n",
    "\\quad \\quad \\mathbf{\\theta} \\leftarrow \\text{update}(\\mathbf{X}_{i,:}, \\mathbf{y}_{i}; \\mathbf{\\theta}) \\\\\n",
    "\\textbf{end for}\n",
    "\\end{array}\n",
    "$$\n",
    "where $\\mathbf{X}_{i,:}$ and $\\mathbf{y}_{i}$ are the $i$-th \n",
    "input and output sample of the train dataset respectly. <br>\n",
    "**Note**: $\\mathbf{X}_{i,:} \\in \\mathbb{R}^{1 \\times n}$ and $\\mathbf{y}_{i} \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini-batch GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mini-batch GD is intermediate between SGD and batch GD since a fragment of \n",
    "the dataset larger than SGD but smaller than batch GD is used to update our parameters per epoch:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Algorithm 3: mini-batch Gradient Descent} \\\\\n",
    "\\textbf{for } t = 1 \\text{ to } T \\textbf{ do} \\\\\n",
    "\\quad i \\leftarrow 1 \\\\\n",
    "\\quad j \\leftarrow \\mathcal{B} \\\\\n",
    "\\quad \\textbf{while } i < m \\textbf{ do} \\\\\n",
    "\\quad \\quad \\mathbf{\\theta} \\leftarrow \\text{update}(\\mathbf{X}_{i:j,:}, \\mathbf{y}_{i:j}; \\mathbf{\\theta}) \\\\\n",
    "\\quad \\quad i \\leftarrow i + \\mathcal{B} \\\\\n",
    "\\quad \\quad j \\leftarrow j + \\mathcal{B} \\\\\n",
    "\\textbf{end for}\n",
    "\\end{array}\n",
    "$$\n",
    "where $\\mathcal{B}$ is the number of samples per minibatch and \n",
    "$\\mathbf{X}_{i:j,:}$ and $\\mathbf{y}_{i:j}$ are the $i$-th to $j$-th samples. <br>\n",
    "**Note**: If $\\mathcal{B}=1$, then mini-batch GD becomes SGD. \n",
    "And if $\\mathcal{B}=m$, then mini-batch GD becomes batch GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def fit(self, x: torch.Tensor, y: torch.Tensor, \n",
    "        epochs: int, lr: float, batch_size: int, \n",
    "        x_valid: torch.Tensor, y_valid: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Fit the model using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples, n_features).\n",
    "        y: Target tensor of shape (n_samples,).\n",
    "        epochs: Number of epochs to fit.\n",
    "        lr: learning rate.\n",
    "        batch_size: Int number of batch.\n",
    "        x_valid: Input tensor of shape (n_valid_samples, n_features).\n",
    "        y_valid: Target tensor of shape (n_valid_samples,).\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        loss = []\n",
    "        for batch in range(0, len(y), batch_size):\n",
    "            end_batch = batch + batch_size\n",
    "\n",
    "            y_pred = self.predict(x[batch:end_batch])\n",
    "\n",
    "            loss.append(self.mse_loss(\n",
    "                y[batch:end_batch],\n",
    "                y_pred\n",
    "            ))\n",
    "\n",
    "            self.update(\n",
    "                x[batch:end_batch], \n",
    "                y[batch:end_batch], \n",
    "                y_pred, \n",
    "                lr\n",
    "            )\n",
    "\n",
    "        loss = round(sum(loss) / len(loss), 4)\n",
    "        loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "        print(f'epoch: {epoch} - MSE: {loss} - MSE_v: {loss_v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch vs Torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLinearRegression(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(TorchLinearRegression, self).__init__()\n",
    "        self.layer = nn.Linear(n_features, 1, device=device)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.forward(x)\n",
    "            return self.loss(y_pred, y).item()\n",
    "    \n",
    "    def fit(self, x, y, epochs, lr, batch_size, x_valid, y_valid):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            loss_t = [] # train loss\n",
    "            for batch in range(0, len(y), batch_size):\n",
    "                end_batch = batch + batch_size\n",
    "\n",
    "                y_pred = self.forward(x[batch:end_batch])\n",
    "                loss = self.loss(y_pred, y[batch:end_batch])\n",
    "                loss_t.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            loss_t = round(sum(loss_t) / len(loss_t), 4)\n",
    "            loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "            print(f'epoch: {epoch} - MSE: {loss_t} - MSE_v: {loss_v}')\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = TorchLinearRegression(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLinearRegression(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPE modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the module path if running locally\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "try:\n",
    "    # Try importing the module normally (for local execution)\n",
    "    from tools.torch_metrics import torch_mape as mape\n",
    "except ModuleNotFoundError:\n",
    "    # If the module is not found, assume the code is running in Google Colab\n",
    "    import subprocess\n",
    "\n",
    "    repo_url = \"https://raw.githubusercontent.com/PilotLeoYan/inside-deep-learning/main/tools/torch_metrics.py\"\n",
    "    local_file = \"torch_metrics.py\"\n",
    "\n",
    "    # Download the missing file from GitHub\n",
    "    subprocess.run([\"wget\", repo_url, \"-O\", local_file], check=True)\n",
    "\n",
    "    # Import the module after downloading it\n",
    "    import torch_metrics\n",
    "    from torch_metrics import torch_mape as mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021.5510615206426"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid).squeeze(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.copy_params(torch_model.layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid).squeeze(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.evaluate(X_valid, Y_valid),\n",
    "    torch_model.evaluate(X_valid, Y_valid.unsqueeze(-1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01 # learning rate\n",
    "EPOCHS = 16 # number of epochs\n",
    "BATCH = len(X_train) // 3 # number of minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 11626.5375 - MSE_v: 14594.5351\n",
      "epoch: 1 - MSE: 10246.5538 - MSE_v: 12985.4412\n",
      "epoch: 2 - MSE: 9045.1297 - MSE_v: 11565.4583\n",
      "epoch: 3 - MSE: 7996.4675 - MSE_v: 10310.47\n",
      "epoch: 4 - MSE: 7079.0141 - MSE_v: 9199.7727\n",
      "epoch: 5 - MSE: 6274.6636 - MSE_v: 8215.5283\n",
      "epoch: 6 - MSE: 5568.1283 - MSE_v: 7342.3173\n",
      "epoch: 7 - MSE: 4946.4382 - MSE_v: 6566.7714\n",
      "epoch: 8 - MSE: 4398.5411 - MSE_v: 5877.2692\n",
      "epoch: 9 - MSE: 3914.9807 - MSE_v: 5263.6831\n",
      "epoch: 10 - MSE: 3487.6361 - MSE_v: 4717.1679\n",
      "epoch: 11 - MSE: 3109.51 - MSE_v: 4229.9827\n",
      "epoch: 12 - MSE: 2774.554 - MSE_v: 3795.3407\n",
      "epoch: 13 - MSE: 2477.5261 - MSE_v: 3407.2815\n",
      "epoch: 14 - MSE: 2213.8712 - MSE_v: 3060.5624\n",
      "epoch: 15 - MSE: 1979.6222 - MSE_v: 2750.5657\n"
     ]
    }
   ],
   "source": [
    "torch_model.fit(\n",
    "    X_train, \n",
    "    Y_train.unsqueeze(-1),\n",
    "    EPOCHS, LR, BATCH,\n",
    "    X_valid,\n",
    "    Y_valid.unsqueeze(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 11626.5375 - MSE_v: 14594.5351\n",
      "epoch: 1 - MSE: 10246.5538 - MSE_v: 12985.4412\n",
      "epoch: 2 - MSE: 9045.1297 - MSE_v: 11565.4583\n",
      "epoch: 3 - MSE: 7996.4675 - MSE_v: 10310.47\n",
      "epoch: 4 - MSE: 7079.0141 - MSE_v: 9199.7727\n",
      "epoch: 5 - MSE: 6274.6636 - MSE_v: 8215.5283\n",
      "epoch: 6 - MSE: 5568.1283 - MSE_v: 7342.3173\n",
      "epoch: 7 - MSE: 4946.4382 - MSE_v: 6566.7714\n",
      "epoch: 8 - MSE: 4398.5411 - MSE_v: 5877.2692\n",
      "epoch: 9 - MSE: 3914.9807 - MSE_v: 5263.6831\n",
      "epoch: 10 - MSE: 3487.6361 - MSE_v: 4717.1679\n",
      "epoch: 11 - MSE: 3109.51 - MSE_v: 4229.9827\n",
      "epoch: 12 - MSE: 2774.554 - MSE_v: 3795.3407\n",
      "epoch: 13 - MSE: 2477.5261 - MSE_v: 3407.2815\n",
      "epoch: 14 - MSE: 2213.8712 - MSE_v: 3060.5624\n",
      "epoch: 15 - MSE: 1979.6222 - MSE_v: 2750.5657\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, Y_train,\n",
    "    EPOCHS, LR, BATCH,\n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.735396087063425e-15"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid).squeeze(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3870518082667405e-14"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.w.clone(),\n",
    "    torch_model.layer.weight.detach().squeeze(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7463440146360812e-14"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.b.clone(),\n",
    "    torch_model.layer.bias.detach()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
