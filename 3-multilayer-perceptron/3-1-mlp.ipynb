{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 - MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PilotLeoYan/inside-deep-learning/blob/main/3-multilayer-perceptron/3-1-mlp.ipynb\">\n",
    "    <img src=\"../images/colab_logo.png\" width=\"32\">Open in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://nbviewer.org/github/PilotLeoYan/inside-deep-learning/blob/main/3-multilayer-perceptron/3-1-mlp.ipynb\">\n",
    "    <img src=\"../images/jupyter_logo.png\" width=\"32\">Open in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first advance we will make towards deep learning \n",
    "will be the multilayer perceptron (MLP). \n",
    "It consists of interconnecting several dense layers \n",
    "and superimposing them to obtain a deep neural network (DNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: black\">\n",
    "<img src=\"../images/mlp.png\" alt=\"deep neuronal network\" width=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose of this Notebook**:\n",
    "\n",
    "The purposes of this notebook are:\n",
    "1. Create a dataset for linear regression task\n",
    "2. Create our own Layer classes from scratch\n",
    "    - Dense\n",
    "    - Activation functions\n",
    "3. Create our own MLP class from scratch\n",
    "4. Calculate the backpropagation from scratch\n",
    "5. Train our MLP\n",
    "6. Compare our MLP to the one prebuilt by PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.14.0', '2.9.0+cu126')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from platform import python_version\n",
    "python_version(), torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{X} \\in \\mathbb{R}^{m \\times n} \\\\\n",
    "\\mathbf{Y} \\in \\mathbb{R}^{m \\times n_{o}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10100, 6)\n",
      "(10100, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import random\n",
    "\n",
    "M: int = 10_100 # number of samples\n",
    "N: int = 6 # number of input features\n",
    "NO: int = 3 # number of output features\n",
    "\n",
    "X, Y = make_regression(\n",
    "    n_samples=M, \n",
    "    n_features=N, \n",
    "    n_targets=NO, \n",
    "    n_informative=N - 1,\n",
    "    bias=random.random(),\n",
    "    noise=1\n",
    ")\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset into train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 6]), torch.Size([9100, 6]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(X[:1000], device=device)\n",
    "X_valid = torch.tensor(X[1000:], device=device)\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 3]), torch.Size([9100, 3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = torch.tensor(Y[:1000], device=device)\n",
    "Y_valid = torch.tensor(Y[1000:], device=device)\n",
    "Y_train.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    is_trainable: bool = False\n",
    "    pass\n",
    "\n",
    "\n",
    "class Activation:\n",
    "    pass\n",
    "\n",
    "\n",
    "class Losses:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scratch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model as such will be the container of all our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers: list[Layer], loss_f: Losses = None):\n",
    "        self.layers = layers[1:] # do not save the input layer\n",
    "        self.loss_f = MSE() if loss_f is None else loss_f\n",
    "\n",
    "        # initialize all parameters\n",
    "        out = layers[0].construct()\n",
    "        for layer in self.layers:\n",
    "            out = layer.construct(out)\n",
    "\n",
    "    def copy_parameters(self, parameters) -> None:\n",
    "        params = list(parameters())\n",
    "        for layer in self.layers:\n",
    "            if layer.is_trainable:\n",
    "                layer.set_params(params.pop(0), params.pop(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense or full conect layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{W}^{(k)} &\\in \n",
    "\\mathbb{R}^{n_{k-1} \\times n_{k}} \\\\\n",
    "\\mathbf{b}^{(k)} &\\in \n",
    "\\mathbb{R}^{n_{k}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $k = 1, \\ldots, l$. Where $l$ is the number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, units: int, act_f: Activation = None):\n",
    "        self.units = units\n",
    "        self.act_f = act_f if act_f is not None else Linear()\n",
    "        self.is_trainable = True\n",
    "\n",
    "    def set_params(self, w: torch.Tensor, b: torch.Tensor) -> None:\n",
    "        self.w.copy_(w.T.detach().clone())\n",
    "        self.b.copy_(b.detach().clone())\n",
    "\n",
    "    def construct(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Initialize the parameters.\n",
    "        self.w := tensor (n_features, units).\n",
    "        self.b := tensor (units).\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor of shape (m_samples, n_features).\n",
    "        \n",
    "        Return:\n",
    "            z: out tensor of shape (m_samples, units).\n",
    "        \"\"\"\n",
    "        n_features = x.shape[-1]\n",
    "        self.w = torch.randn(n_features, self.units, device=device)\n",
    "        self.b = torch.randn(self.units, device=device)\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}^{(k)} : \\mathbb{R}^{m \\times n_{k}} \\rightarrow\n",
    "\\mathbb{R}^{m \\times n_{k}}\n",
    "$$\n",
    "for all $k = 1, \\ldots, l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Activation):\n",
    "    pass\n",
    "\n",
    "\n",
    "class RelU(Activation):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Softmax(Activation):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Algorithm 1: Forward propagation} \\\\\n",
    "\\mathbf{A}^{(0)} := \\mathbf{X} \\\\\n",
    "\\textbf{for } k = 1 \\text{ to } l \\textbf{ do}\\\\\n",
    "\\quad \\mathbf{Z}^{(k)} = \n",
    "\\mathbf{A}^{(k-1)} \\mathbf{W}^{(k)} + \\mathbf{b}^{(k)} \\\\\n",
    "\\quad \\mathbf{A}^{(k)} = \n",
    "f(\\mathbf{Z}^{(k)}) \\\\\n",
    "\\textbf{end for}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Model)\n",
    "def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Forward propagation.\n",
    "    \n",
    "    Args:\n",
    "        x: tensor of shape (m_samples, n_input_features).\n",
    "        \n",
    "    Return:\n",
    "        y_pred: tensor of shape (m_samples, n_out_features).\n",
    "    \"\"\"\n",
    "    out = x\n",
    "    for layer in self.layers:\n",
    "        out = layer.forward(out)\n",
    "    return out\n",
    "\n",
    "@add_to_class(Model)\n",
    "def __forward__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    out = x\n",
    "    for layer in self.layers:\n",
    "        out = layer.__forward__(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Z}^{(k)}(\\mathbf{A}^{(k-1)}) = \n",
    "\\mathbf{A}^{(k-1)} \\mathbf{W}^{(k)} + \\mathbf{b}^{(k)} \\\\\n",
    "\\mathbf{Z}^{(k)} : \\mathbb{R}^{m \\times n_{k-1}} \\rightarrow\n",
    "\\mathbb{R}^{m \\times n_{k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Dense)\n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute weighted sum Z = XW+b and activation function A = f(Z).\n",
    "    \n",
    "    Args:\n",
    "        x: input tensor of shape (m_samples, n_features).\n",
    "        \n",
    "    Return:\n",
    "        a: out tensor of shape (m_samples, units).\n",
    "    \"\"\"\n",
    "    return self.act_f(torch.matmul(x, self.w) + self.b)\n",
    "\n",
    "@add_to_class(Dense)\n",
    "def __forward__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Forward propagation for training step.\"\"\"\n",
    "    self.input = x.clone()\n",
    "    self.a = self.forward(x)\n",
    "    return self.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Linear}^{(k)}(\\mathbf{Z}^{(k)}) = \n",
    "\\mathbf{Z}^{(k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Linear)\n",
    "def __call__(self, z: torch.Tensor) -> torch.Tensor:\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{ReLU}^{(k)}(\\mathbf{Z^{(k)}}) = \n",
    "\\max(\\mathbf{Z^{(k)}}, 0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(RelU)\n",
    "def __call__(self, z: torch.Tensor) -> torch.Tensor:\n",
    "    #return torch.relu(z)\n",
    "    return torch.max(z, torch.zeros_like(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Sigmoid}^{(k)}(\\mathbf{Z}^{(k)}) = \n",
    "\\frac{1}{1 + \\exp(-\\mathbf{Z}^{(k)})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Sigmoid)\n",
    "def __call__(self, z: torch.Tensor) -> torch.Tensor:\n",
    "    #return torch.sigmoid(z)\n",
    "    return 1 / (1 + torch.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tanh^{(k)}(\\mathbf{Z}^{(k)}) = \n",
    "\\frac{1 - \\exp(-2 \\mathbf{Z}^{(k)})}\n",
    "{1 + \\exp(-2 \\mathbf{Z}^{(k)})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Tanh)\n",
    "def __call__(self, z: torch.Tensor) -> torch.Tensor:\n",
    "    #return torch.tanh(z)\n",
    "    exp = torch.exp(-2 * z)\n",
    "    return (1 - exp) / (1 + exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Softmax}^{(k)} (\\mathbf{Z}^{(k)}) =\n",
    "\\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{z}_{m,:})\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Softmax)\n",
    "def __call__(self, z: torch.Tensor) -> torch.Tensor:\n",
    "    exp = torch.exp(z - torch.max(z, dim=1, keepdims=True)[0])\n",
    "    return exp / exp.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this layer is simply to create a random dataset \n",
    "to initialize all the parameters of the layers. \n",
    "This way we do not have to manually specify the dimensions of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer(Layer):\n",
    "    def __init__(self, n_input_features: int):\n",
    "        self.m = 10\n",
    "        self.n = n_input_features\n",
    "\n",
    "    def construct(self) -> torch.Tensor:\n",
    "        return torch.randn(self.m, self.n, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{MSE}(\\mathbf{A}^{(l)}) = \n",
    "\\frac{1}{m n_{o}} \n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{o}} \\left(\n",
    "    (a^{(l)}_{ij} - y_{ij})^2\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{A}^{(l)}$ is the activation of the last layer of the model and\n",
    "$n_{o}$ is the number of output features of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Losses):\n",
    "    def loss(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "        return ((y_pred - y_true)**2).mean().item()\n",
    "\n",
    "    def __call__(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "        return self.loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Model)\n",
    "def evaluate(self, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model between input x and target y\n",
    "    \n",
    "    Args:\n",
    "        x: tensor (m_samples, n_input_features).\n",
    "        y: target tensor (m_samples, n_out_features).\n",
    "        \n",
    "    Return:\n",
    "        loss: error between y_pred and target y.\n",
    "    \"\"\"\n",
    "    y_pred = self.predict(x)\n",
    "    return self.loss_f(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the derivatives/gradients \n",
    "of each parameter in the model using **backpropagation**\n",
    "and update each parameter using **gradient descent** (gd)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of ​​backpropagation is to calculate these derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta^{(l)}} = \n",
    "{\\color{Lime} {\\frac{\\partial L}\n",
    "{\\partial \\mathbf{A}^{(l)}}}}\n",
    "{\\color{Cyan} {\\frac{\\partial \\mathbf{A}^{(l)}}\n",
    "{\\partial \\mathbf{Z}^{(l)}}}}\n",
    "{\\color{Orange} {\\frac{\\partial \\mathbf{Z}^{(l)}}\n",
    "{\\partial \\theta^{(l)}}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta^{(l-1)}} = \n",
    "{\\color{Lime} {\\frac{\\partial L}\n",
    "{\\partial \\mathbf{A}^{(l)}}}}\n",
    "{\\color{Cyan} {\\frac{\\partial \\mathbf{A}^{(l)}}\n",
    "{\\partial \\mathbf{Z}^{(l)}}}}\n",
    "{\\color{Magenta} {\\frac{\\partial \\mathbf{Z}^{(l)}}\n",
    "{\\partial \\mathbf{A}^{(l-1)}}}}\n",
    "{\\color{Cyan} {\\frac{\\partial \\mathbf{A}^{(l-1)}}\n",
    "{\\partial \\mathbf{Z}^{(l-1)}}}}\n",
    "{\\color{Orange} {\\frac{\\partial \\mathbf{Z}^{(l-1)}}\n",
    "{\\partial \\theta^{(l-1)}}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta^{(k)}} = \n",
    "{\\color{Lime} {\\frac{\\partial L}\n",
    "{\\partial \\mathbf{A}^{(l)}}}}\n",
    "{\\color{Cyan} {\\frac{\\partial \\mathbf{A}^{(l)}}\n",
    "{\\partial \\mathbf{Z}^{(l)}}}}\n",
    "{\\color{Magenta} {\\frac{\\partial \\mathbf{Z}^{(l)}}\n",
    "{\\partial \\mathbf{A}^{(l-1)}}}}\n",
    "\\cdots\n",
    "{\\color{Cyan} {\\frac{\\partial \\mathbf{A}^{(k)}}\n",
    "{\\partial \\mathbf{Z}^{(k)}}}}\n",
    "{\\color{Orange} {\\frac{\\partial \\mathbf{Z}^{(k)}}\n",
    "{\\partial \\theta^{(k)}}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\theta^{(k)} = (\\mathbf{b}^{(k)}, \\mathbf{W}^{(k)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there are many different derivatives. \n",
    "However, many of them are the same.\n",
    "We only need to know 4 derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\color{Lime} {\\frac{\\partial L}\n",
    "{\\partial \\mathbf{A}^{(l)}}}}, \n",
    "{\\color{Cyan} {\\frac{\\partial \\mathbf{A}^{(k)}}\n",
    "{\\partial \\mathbf{Z}^{(k)}}}},\n",
    "{\\color{Magenta} {\\frac{\\partial \\mathbf{Z}^{(k)}}\n",
    "{\\partial \\mathbf{A}^{(k-1)}}}},\n",
    "{\\color{Orange} {\\frac{\\partial \\mathbf{Z}^{(k)}}\n",
    "{\\partial \\theta^{(k)}}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these 4 derivatives we can compute \n",
    "$\\nabla_{\\theta^{(k)}} L$ for all \n",
    "$k = l, \\ldots, 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Algorithm 2: Backpropagation} \\\\\n",
    "\\mathbf{\\Delta} := \\nabla_{\\mathbf{A}^{(l)}}L \\\\\n",
    "\\textbf{for } k = l, l-1, ..., 1 \\textbf{ do}\\\\\n",
    "\\quad \\mathbf{\\Delta} := \\mathbf{\\Delta} \n",
    "\\nabla_{\\mathbf{Z}^{(k)}} \\mathbf{A}^{(k)} \\\\\n",
    "\\quad \\nabla_{\\theta^{(k)}}L = \\mathbf{\\Delta}\n",
    "\\nabla_{\\theta^{(k)}} \\mathbf{Z}^{(k)} \\\\\n",
    "\\quad \\mathbf{\\Delta} := \\mathbf{\\Delta}\n",
    "\\nabla_{\\mathbf{A}^{(k-1)}} \\mathbf{Z}^{(k)} \\\\\n",
    "\\textbf{end for}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Model)    \n",
    "def update(self, y_pred: torch.Tensor, y_true: torch.Tensor, lr: float) -> None:\n",
    "    delta = self.loss_f.backward(y_pred, y_true)\n",
    "    for layer in reversed(self.layers):\n",
    "        delta = layer.backward(delta, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(MSE)\n",
    "def backward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    return 2 * (y_pred - y_true) / y_true.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about the derivatives of these activation functions, \n",
    "see [gradients and activation functions](gradients-and-activation-functions.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Linear)\n",
    "def backward(self, delta, a):\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(RelU)\n",
    "def backward(self, delta, a):\n",
    "    return delta * (1 * (a > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Sigmoid)\n",
    "def backward(self, delta, a):\n",
    "    return delta * (a * (1 - a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Tanh)\n",
    "def backward(self, delta, a):\n",
    "    return delta * (1 - a**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Softmax)\n",
    "def backward(self, delta, a):\n",
    "    return a * (delta - (delta * a).sum(axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### respect to bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}^{(k)}} &= \n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}^{(k)}}\n",
    "{\\color{Orange} {\\frac{\\partial \\mathbf{Z}^{(k)}}\n",
    "{\\partial \\mathbf{b}^{(k)}}}} \\\\\n",
    "&= {\\color{Orange} {\\mathbf{1}}}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}^{(k)}}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{1} \\in \\mathbb{R}^{m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### respect to weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(k)}} &=\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}^{(k)}}\n",
    "{\\color{Orange} {\\frac{\\partial \\mathbf{Z}^{(k)}}\n",
    "{\\partial \\mathbf{W}^{(k)}}}} \\\\\n",
    "&= {\\color{Orange} {\\left( \\mathbf{A}^{(k-1)} \\right)^\\top}}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}^{(k)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### respect to input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{A}^{(k-1)}} &=\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}^{(k)}}\n",
    "{\\color{Magenta} {\\frac{\\partial \\mathbf{Z}^{(k)}}\n",
    "{\\partial \\mathbf{A}^{(k-1)}}}} \\\\\n",
    "&= {\\color{Magenta} {\\left( \\mathbf{W}^{(k)} \\right)^\\top}}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}^{(k)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{W}^{(k)} := \\mathbf{W}^{(k)} -\\eta \n",
    "\\nabla_{\\mathbf{W}^{(k)}}L \\\\\n",
    "\\mathbf{b}^{(k)} := \\mathbf{b}^{(k)} -\\eta \n",
    "\\nabla_{\\mathbf{b}^{(k)}}L \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Dense)\n",
    "def backward(self, delta, lr: float) -> torch.Tensor:\n",
    "    # activation function derivative\n",
    "    delta = self.act_f.backward(delta, self.a)\n",
    "    # bias der and update\n",
    "    self.b -= lr * torch.sum(delta, axis=0)\n",
    "    # weight derivative (update weight after compute input der)\n",
    "    w_der = torch.matmul(self.input.T, delta)\n",
    "    # input derivative\n",
    "    delta = torch.matmul(delta, self.w.T)\n",
    "    # weight update\n",
    "    self.w -= lr * w_der\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Model)    \n",
    "def fit(self, x_train: torch.Tensor, y_train: torch.Tensor, \n",
    "        epochs: int, lr: float, batch_size: int, \n",
    "        x_valid: torch.Tensor, y_valid: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Fit the model using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        x_train: Input tensor of shape (n_samples, n_in_features).\n",
    "        y_train: Target tensor one hot of shape (n_samples, n_out_features).\n",
    "        epochs: Number of epochs to train.\n",
    "        lr: learning rate).\n",
    "        batch_size: Int number of batch.\n",
    "        x_valid: Input tensor of shape (n_valid_samples, n_in_features).\n",
    "        y_valid: Input tensor one hot of shape (n_valid_samples, n_out_features).\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        loss_t = [] # train loss\n",
    "        for batch in range(0, len(y_train), batch_size):\n",
    "            end_batch = batch + batch_size\n",
    "\n",
    "            y_pred = self.__forward__(x_train[batch:end_batch])\n",
    "            loss_t.append(self.loss_f(y_pred, y_train[batch:end_batch]))\n",
    "\n",
    "            self.update(y_pred, Y_train[batch:end_batch], lr)\n",
    "            \n",
    "        loss_t = sum(loss_t) / len(loss_t)\n",
    "        loss_v = self.evaluate(x_valid, y_valid) # valid loss\n",
    "        print('Epoch: {} - L: {:.4f} - L_v {:.4f}'.format(epoch, loss_t, loss_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSequential(nn.Module):\n",
    "    def __init__(self, layers: list[nn.Module], loss_fn=None):\n",
    "        super(TorchSequential, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        for layer in self.layers:\n",
    "            layer.to(device)\n",
    "        self.loss_fn = loss_fn if loss_fn is not None else nn.MSELoss()\n",
    "        self.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.clone()\n",
    "        for l in self.layers:\n",
    "            out = l(out)\n",
    "        return out\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(x)\n",
    "            return self.loss_fn(y_pred, y).item()\n",
    "        \n",
    "    def fit(self, x: torch.Tensor, y: torch.Tensor, \n",
    "            epochs: int, lr: float, batch_size: int, \n",
    "            x_valid: torch.Tensor, y_valid: torch.Tensor):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr, momentum=0.0)\n",
    "        for epoch in range(epochs):\n",
    "            loss_t = []\n",
    "            for batch in range(0, len(y), batch_size):\n",
    "                end_batch = batch + batch_size\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_pred = self(x[batch:end_batch])\n",
    "                loss = self.loss_fn(y_pred, y[batch:end_batch])\n",
    "                loss_t.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            loss_t = sum(loss_t) / len(loss_t)\n",
    "            loss_v = self.evaluate(x_valid, y_valid)\n",
    "            print('Epoch: {} - L: {:.4f} - L_v {:.4f}'.format(epoch, loss_t, loss_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = TorchSequential([\n",
    "    nn.Linear(N, 32), nn.Tanh(),\n",
    "    nn.Linear(32, 32), nn.Softmax(dim=1),\n",
    "    nn.Linear(32, 32), nn.Sigmoid(),\n",
    "    nn.Linear(32, 32), nn.ReLU(),\n",
    "    nn.Linear(32, NO)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch vs Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([\n",
    "    InputLayer(N),\n",
    "    Dense(32, Tanh()),\n",
    "    Dense(32, Softmax()),\n",
    "    Dense(32, Sigmoid()),\n",
    "    Dense(32, RelU()),\n",
    "    Dense(NO, Linear())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import MAPE modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape imported locally.\n"
     ]
    }
   ],
   "source": [
    "# This cell imports torch_mape \n",
    "# if you are running this notebook locally \n",
    "# or from Google Colab.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "try:\n",
    "    from tools.torch_metrics import torch_mape as mape\n",
    "    print('mape imported locally.')\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess\n",
    "\n",
    "    repo_url = 'https://raw.githubusercontent.com/PilotLeoYan/inside-deep-learning/main/tools/torch_metrics.py'\n",
    "    local_file = 'torch_metrics.py'\n",
    "    \n",
    "    subprocess.run(['wget', repo_url, '-O', local_file], check=True)\n",
    "    try:\n",
    "        from torch_metrics import torch_mape as mape # type: ignore\n",
    "        print('mape imported from GitHub.')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8047.589528447594"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.copy_parameters(torch_model.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.126390850703315e-15"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.evaluate(X_valid, Y_valid),\n",
    "    torch_model.evaluate(X_valid, Y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR: float = 0.01\n",
    "EPOCHS: int = 32\n",
    "BATCH_SIZE: int = len(Y_train) // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - L: 16162.1496 - L_v 17214.9913\n",
      "Epoch: 1 - L: 15945.6268 - L_v 23203.3293\n",
      "Epoch: 2 - L: 17419.7603 - L_v 17194.3458\n",
      "Epoch: 3 - L: 16079.4770 - L_v 17196.8575\n",
      "Epoch: 4 - L: 16003.8208 - L_v 17253.7927\n",
      "Epoch: 5 - L: 15834.9678 - L_v 18168.9330\n",
      "Epoch: 6 - L: 15958.6110 - L_v 18359.7932\n",
      "Epoch: 7 - L: 15976.3229 - L_v 18232.5636\n",
      "Epoch: 8 - L: 15888.2818 - L_v 18376.8835\n",
      "Epoch: 9 - L: 15213.9964 - L_v 30823.8419\n",
      "Epoch: 10 - L: 19134.5431 - L_v 17204.1569\n",
      "Epoch: 11 - L: 15934.2614 - L_v 17211.3571\n",
      "Epoch: 12 - L: 15889.7132 - L_v 17231.7224\n",
      "Epoch: 13 - L: 15817.7397 - L_v 17321.0218\n",
      "Epoch: 14 - L: 15720.9583 - L_v 17649.6312\n",
      "Epoch: 15 - L: 15719.2805 - L_v 18062.9273\n",
      "Epoch: 16 - L: 15792.7839 - L_v 18169.7939\n",
      "Epoch: 17 - L: 15810.0013 - L_v 18165.7491\n",
      "Epoch: 18 - L: 15804.3063 - L_v 18152.0780\n",
      "Epoch: 19 - L: 15796.5655 - L_v 18138.2186\n",
      "Epoch: 20 - L: 15788.7022 - L_v 18124.7967\n",
      "Epoch: 21 - L: 15780.7994 - L_v 18111.8729\n",
      "Epoch: 22 - L: 15772.7959 - L_v 18099.5089\n",
      "Epoch: 23 - L: 15764.5701 - L_v 18087.8237\n",
      "Epoch: 24 - L: 15755.8537 - L_v 18077.0532\n",
      "Epoch: 25 - L: 15745.9053 - L_v 18067.6445\n",
      "Epoch: 26 - L: 15731.6037 - L_v 18058.7701\n",
      "Epoch: 27 - L: 15663.0903 - L_v 17821.0832\n",
      "Epoch: 28 - L: 14464.7307 - L_v 19206.8643\n",
      "Epoch: 29 - L: 15983.4905 - L_v 17266.0911\n",
      "Epoch: 30 - L: 15707.4707 - L_v 17274.9689\n",
      "Epoch: 31 - L: 15688.6380 - L_v 17284.1127\n"
     ]
    }
   ],
   "source": [
    "torch_model.fit(\n",
    "    X_train, Y_train.double(), \n",
    "    EPOCHS, LR, BATCH_SIZE, \n",
    "    X_valid, Y_valid.double()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - L: 16162.1496 - L_v 17214.9913\n",
      "Epoch: 1 - L: 15945.6268 - L_v 23203.3293\n",
      "Epoch: 2 - L: 17419.7603 - L_v 17194.3458\n",
      "Epoch: 3 - L: 16079.4770 - L_v 17196.8575\n",
      "Epoch: 4 - L: 16003.8208 - L_v 17253.7927\n",
      "Epoch: 5 - L: 15834.9678 - L_v 18168.9330\n",
      "Epoch: 6 - L: 15958.6110 - L_v 18359.7932\n",
      "Epoch: 7 - L: 15976.3229 - L_v 18232.5636\n",
      "Epoch: 8 - L: 15888.2818 - L_v 18376.8835\n",
      "Epoch: 9 - L: 15213.9964 - L_v 30823.8419\n",
      "Epoch: 10 - L: 19134.5431 - L_v 17204.1569\n",
      "Epoch: 11 - L: 15934.2614 - L_v 17211.3571\n",
      "Epoch: 12 - L: 15889.7132 - L_v 17231.7224\n",
      "Epoch: 13 - L: 15817.7397 - L_v 17321.0218\n",
      "Epoch: 14 - L: 15720.9583 - L_v 17649.6312\n",
      "Epoch: 15 - L: 15719.2805 - L_v 18062.9273\n",
      "Epoch: 16 - L: 15792.7839 - L_v 18169.7939\n",
      "Epoch: 17 - L: 15810.0013 - L_v 18165.7491\n",
      "Epoch: 18 - L: 15804.3063 - L_v 18152.0780\n",
      "Epoch: 19 - L: 15796.5655 - L_v 18138.2186\n",
      "Epoch: 20 - L: 15788.7022 - L_v 18124.7967\n",
      "Epoch: 21 - L: 15780.7994 - L_v 18111.8729\n",
      "Epoch: 22 - L: 15772.7959 - L_v 18099.5089\n",
      "Epoch: 23 - L: 15764.5701 - L_v 18087.8237\n",
      "Epoch: 24 - L: 15755.8537 - L_v 18077.0532\n",
      "Epoch: 25 - L: 15745.9053 - L_v 18067.6445\n",
      "Epoch: 26 - L: 15731.6037 - L_v 18058.7701\n",
      "Epoch: 27 - L: 15663.0903 - L_v 17821.0832\n",
      "Epoch: 28 - L: 14464.7307 - L_v 19206.8643\n",
      "Epoch: 29 - L: 15983.4905 - L_v 17266.0911\n",
      "Epoch: 30 - L: 15707.4707 - L_v 17274.9689\n",
      "Epoch: 31 - L: 15688.6380 - L_v 17284.1127\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, Y_train, \n",
    "    EPOCHS, LR, BATCH_SIZE, \n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know that both models are experiencing overfitting during training, \n",
    "but the goal of this notebook is not to create good predictors on synthetic data, \n",
    "but to understand their inner workings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.342460856129081e-12"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch layer #0 - torch layer #0\n",
      "1.291949737701154e-09\n",
      "scratch layer #1 - torch layer #1\n",
      "2.447136237846293e-09\n",
      "scratch layer #2 - torch layer #2\n",
      "1.1777100127918876e-11\n",
      "scratch layer #3 - torch layer #3\n",
      "1.9483473209870755e-11\n",
      "scratch layer #4 - torch layer #4\n",
      "5.342460856129081e-12\n"
     ]
    }
   ],
   "source": [
    "filtered_layers = filter(\n",
    "    lambda x: isinstance(x, nn.modules.linear.Linear), \n",
    "    torch_model.layers\n",
    ")\n",
    "\n",
    "for i, layer in enumerate(filtered_layers):\n",
    "    print(f'scratch layer #{i} - torch layer #{i}')\n",
    "    print(mape(model.layers[i].b, layer.bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch layer #0 - torch layer #0\n",
      "4.393451636888655e-09\n",
      "scratch layer #1 - torch layer #1\n",
      "3.7729780577496506e-09\n",
      "scratch layer #2 - torch layer #2\n",
      "3.90202939900523e-12\n",
      "scratch layer #3 - torch layer #3\n",
      "2.1878831688037285e-12\n",
      "scratch layer #4 - torch layer #4\n",
      "3.7314983252805996e-10\n"
     ]
    }
   ],
   "source": [
    "filtered_layers = filter(\n",
    "    lambda x: isinstance(x, nn.modules.linear.Linear), \n",
    "    torch_model.layers\n",
    ")\n",
    "\n",
    "for i, layer in enumerate(filtered_layers):\n",
    "    print(f'scratch layer #{i} - torch layer #{i}')\n",
    "    print(mape(model.layers[i].w, layer.weight.T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idl-3-14-0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
