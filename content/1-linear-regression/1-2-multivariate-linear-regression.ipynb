{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 - Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{grid} 1 1 2 2\n",
    "```{card} [Open in Google Colab](https://colab.research.google.com/github/PilotLeoYan/inside-deep-learning/blob/main/content/1-linear-regression/1-2-multivariate-linear-regression.ipynb)\n",
    "```{image} ../figures/colab_logo.png\n",
    ":align: center\n",
    "```\n",
    "```{card} [Open in Jupyter NBViewer](https://nbviewer.org/github/PilotLeoYan/inside-deep-learning/blob/main/content/1-linear-regression/1-2-multivariate-linear-regression.ipynb)\n",
    "```{image} ../figures/jupyter_logo.png\n",
    ":align: center\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to increase the complexity, \n",
    "instead of the perceptron having a single output, \n",
    "it will now have multiple outputs. \n",
    "The word \"multivariable\" usually means that the perceptron receives multiple inputs, \n",
    "but here we will use it to describe that the perceptron has multiple outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} ../figures/multivariate-perceptron.png\n",
    ":width: 300\n",
    ":class: hidden dark:block\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} ../figures/multivariate-perceptron-light.png\n",
    ":width: 300\n",
    ":class: dark:hidden\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the multivariate perceptron as a layer of multiple simple perceptrons, \n",
    "and that each perceptron output corresponds to an output feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} ../figures/multivariate-perceptron-as-layer.png\n",
    ":width: 300\n",
    ":class: hidden dark:block\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} ../figures/multivariate-perceptron-as-layer-light.png\n",
    ":width: 300\n",
    ":class: dark:hidden\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose of this Notebook**:\n",
    "\n",
    "The purposes of this notebook are:\n",
    "1. Create a dataset for multivariate linear regression task\n",
    "2. Create our own Multivariate Perceptron class from scratch\n",
    "3. Calculate the gradient descent from scratch\n",
    "4. Train our Multivariate Perceptron\n",
    "5. Compare our Perceptron to the one prebuilt by PyTorch\n",
    "6. [Extra] Calculate the gradient descent by other way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.14.0', '2.9.0+cu126')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from platform import python_version\n",
    "python_version(), torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj): setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{X} &\\in \\mathbb{R}^{m \\times n} \\\\\n",
    "\\mathbf{Y} &\\in \\mathbb{R}^{m \\times n_{1}}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $n_{1}$ is the number of output features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & \\cdots & x_{mn}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix}\n",
    "    y_{11} & y_{12} & \\cdots & y_{1n_{1}} \\\\\n",
    "    y_{21} & y_{22} & \\cdots & y_{2n_{1}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    y_{m1} & y_{m2} & \\cdots & y_{mn_{1}} \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10100, 6)\n",
      "(10100, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import random\n",
    "\n",
    "M: int = 10_100 # number of samples\n",
    "N: int = 6 # number of input features\n",
    "NO: int = 3 # number of output features\n",
    "\n",
    "X, Y = make_regression(\n",
    "    n_samples=M, \n",
    "    n_features=N, \n",
    "    n_targets=NO, \n",
    "    n_informative=N - 1,\n",
    "    bias=random.random(),\n",
    "    noise=1\n",
    ")\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 6]), torch.Size([100, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(X[:100], device=device)\n",
    "Y_train = torch.tensor(Y[:100], device=device)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 6]), torch.Size([10000, 3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = torch.tensor(X[100:], device=device)\n",
    "Y_valid = torch.tensor(Y[100:], device=device)\n",
    "X_valid.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{W} &\\in \\mathbb{R}^{n \\times n_{1}} \\\\\n",
    "\\mathbf{b} &\\in \\mathbb{R}^{n_{1}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{W} = \\begin{bmatrix}\n",
    "    w_{11} & w_{12} & \\cdots & w_{1n_{1}} \\\\\n",
    "    w_{21} & w_{22} & \\cdots & w_{2n_{1}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{n1} & w_{n2} & \\cdots & w_{nn_{1}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{b} = \\begin{bmatrix}\n",
    "    b_{1} \\\\\n",
    "    b_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    b_{n_{1}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, n_features: int, out_features: int):\n",
    "        self.w = torch.randn(n_features, out_features, device=device)\n",
    "        self.b = torch.randn(out_features, device=device)\n",
    "\n",
    "    def copy_params(self, torch_layer: torch.nn.modules.linear.Linear):\n",
    "        \"\"\"\n",
    "        Copy the parameters from a module.linear to this model.\n",
    "\n",
    "        Args:\n",
    "            torch_layer: Pytorch module from which to copy the parameters.\n",
    "        \"\"\"\n",
    "        self.b.copy_(torch_layer.bias.detach().clone())\n",
    "        self.w.copy_(torch_layer.weight.T.detach().clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weighted sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\hat{Y}}(\\mathbf{X}) = \\mathbf{X}\\mathbf{W} + \\mathbf{b} \\\\\n",
    "\\mathbf{\\hat{Y}} : \\mathbb{R}^{m \\times n} \\rightarrow \n",
    "\\mathbb{R}^{m \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y}_{ij} =\n",
    "\\mathbf{x}_{i}^\\top\n",
    "\\mathbf{w}_{:,j}\n",
    "+ b_{j}\n",
    "$$\n",
    "for all $i = 1, \\ldots, m$ and $j = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Predict the output for input x\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        y_pred: Predicted output tensor of shape (n_samples, out_features).\n",
    "    \"\"\"\n",
    "    return torch.matmul(x, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "L(\\mathbf{\\hat{Y}}) &= \\frac{1}{mn_{1}} \n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}(\n",
    "    \\hat{y}_{ij} - y_{ij})^{2} \\\\\n",
    "L &: \\mathbb{R}^{m \\times n_{1}} \\rightarrow \\mathbb{R}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = \\frac{1}{mn_{1}} \\text{sum} \\left(\n",
    "    \\left(\n",
    "        \\mathbf{\\hat{Y} - Y}\n",
    "    \\right)^2\n",
    "\\right)\n",
    "$$\n",
    "where ${\\mathbf{A}}^2$ is element-wise power or also ${\\mathbf{A}}^2 = \\mathbf{A} \\odot \\mathbf{A}$. <br>\n",
    "**Note**: $\\odot$ is called element-wise product or also Hadamard product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def mse_loss(self, y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "    \"\"\"\n",
    "    MSE loss function between target y_true and y_pred.\n",
    "\n",
    "    Args:\n",
    "        y_true: Target tensor of shape (n_samples, out_features).\n",
    "        y_pred: Predicted tensor of shape (n_samples, out_features).\n",
    "\n",
    "    Returns:\n",
    "        loss: MSE loss between predictions and true values.\n",
    "    \"\"\"\n",
    "    return ((y_pred - y_true)**2).mean().item()\n",
    "\n",
    "@add_to_class(LinearRegression)\n",
    "def evaluate(self, x: torch.Tensor, y_true: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Evaluate the model on input x and target y_true using MSE.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples, n_features).\n",
    "        y_true: Target tensor of shape (n_samples, out_features).\n",
    "\n",
    "    Returns:\n",
    "        loss: MSE loss between predictions and true values.\n",
    "    \"\"\"\n",
    "    y_pred = self.predict(x)\n",
    "    return self.mse_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to compute gradients\n",
    "1. Computing each derivative individually and then joining them using the Einstein summation.\n",
    "2. Computing an initial derivative and passing it backwards as an argument.\n",
    "\n",
    "The most common way is to use method 2 \n",
    "because it is easier to visualize and is more optimal. \n",
    "While method 1 needs more computing. \n",
    "We prefer method 2, but we will also use method 1 just for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{pq}} &=\n",
    "\\frac{1}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial \\hat{y}_{pq}} \n",
    "\\left(\n",
    "    (\\hat{y}_{ij} - y_{ij})^2\n",
    "\\right) \\\\\n",
    "&= \\frac{2}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "(\\hat{y}_{ij} - y_{ij})\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial \\hat{y}_{pq}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, \\ldots, m$ and $q = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial \\hat{y}_{pq}} =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } i=p, j=q \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{pq}} &=\n",
    "\\frac{2}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "(\\hat{y}_{ij} - y_{ij})\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial \\hat{y}_{pq}} \\\\\n",
    "&= \\frac{2}{mn_{1}} (\\hat{y}_{pq} - y_{pq})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}} =\n",
    "\\frac{2}{mn_{1}} \\left(\n",
    "    \\mathbf{\\hat{Y}} - \\mathbf{Y}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted sum derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### respect to bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial b_{p}} &=\n",
    "\\frac{1}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial b_{p}} \n",
    "\\left(\n",
    "    (\\hat{y}_{ij} - y_{ij})^2\n",
    "\\right) \\\\\n",
    "&= \\frac{2}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "(\\hat{y}_{ij} - y_{ij})\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}} \\\\\n",
    "&= \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}}\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}} =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } j=p \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial b_{p}} &=\n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}}\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}} \\\\\n",
    "&= \\sum_{i=1}^{m}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ip}}\n",
    "\\end{align*}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} =\n",
    "\\mathbf{1} \\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}\n",
    "$$\n",
    "where $\\mathbf{1} \\in \\mathbb{R}^{m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### respect to weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_{pq}} &=\n",
    "\\frac{1}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial w_{pq}} \n",
    "\\left(\n",
    "    (\\hat{y}_{ij} - y_{ij})^2\n",
    "\\right) \\\\\n",
    "&= \\frac{2}{mn_{1}} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "(\\hat{y}_{ij} - y_{ij})\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}} \\\\\n",
    "&= \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}}\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, \\ldots, n$ and $q = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}} =\n",
    "\\begin{cases}\n",
    "    x_{ip} & \\text{if } j=q \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_{pq}} &=\n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}}\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}} \\\\\n",
    "&= \\sum_{i=1}^{m} x_{ip}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{iq}} \\\\\n",
    "&= (x_{:,p})^\\top\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{:,q}} \\\\\n",
    "&= x^\\top_{p,:}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{:,q}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} =\n",
    "\\mathbf{X}^\\top\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{b}}L =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &=\n",
    "{\\color{Cyan} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}}}\n",
    "{\\color{Orange} {\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{b}}}} \\\\\n",
    "&= {\\color{Cyan} {\\frac{2}{mn_{1}}}}\n",
    "{\\color{Orange} {\\mathbf{1}}}\n",
    "{\\color{Cyan} {\\left(\\mathbf{\\hat{Y}} - \\mathbf{Y} \\right)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{W}}L =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &=\n",
    "{\\color{Cyan} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}}}\n",
    "{\\color{Magenta} {\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}}}} \\\\\n",
    "&= {\\color{Cyan} {\\frac{2}{mn_{1}}}}\n",
    "{\\color{Magenta} {\\mathbf{X}^\\top}}\n",
    "{\\color{Cyan} {\\left(\\mathbf{\\hat{Y}} - \\mathbf{Y} \\right)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def update(self, x: torch.Tensor, y_true: torch.Tensor, y_pred: torch.Tensor, lr: float):\n",
    "    \"\"\"\n",
    "    Update the model parameters.\n",
    "\n",
    "    Args:\n",
    "       x: Input tensor of shape (n_samples, n_features).\n",
    "       y_true: Target tensor of shape (n_samples, n_features).\n",
    "       y_pred: Predicted output tensor of shape (n_samples, n_features).\n",
    "       lr: Learning rate. \n",
    "    \"\"\"\n",
    "    delta = 2 * (y_pred - y_true) / y_true.numel()\n",
    "    self.b -= lr * delta.sum(axis=0)\n",
    "    self.w -= lr * torch.matmul(x.T, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(LinearRegression)\n",
    "def fit(self, x_train: torch.Tensor, y_train: torch.Tensor, \n",
    "        epochs: int, lr: float, batch_size: int, \n",
    "        x_valid: torch.Tensor, y_valid: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Fit the model using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        x_train: Input tensor of shape (n_samples, n_features).\n",
    "        y_train: Target tensor of shape (n_samples,).\n",
    "        epochs: Number of epochs to fit.\n",
    "        lr: learning rate.\n",
    "        batch_size: Int number of batch.\n",
    "        x_valid: Input tensor of shape (n_valid_samples, n_features).\n",
    "        y_valid: Target tensor of shape (n_valid_samples,)\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        loss = []\n",
    "        for batch in range(0, len(y_train), batch_size):\n",
    "            end_batch = batch + batch_size\n",
    "\n",
    "            y_pred = self.predict(x_train[batch:end_batch])\n",
    "\n",
    "            loss.append(self.mse_loss(\n",
    "                y_train[batch:end_batch], \n",
    "                y_pred\n",
    "            ))\n",
    "\n",
    "            self.update(\n",
    "                x_train[batch:end_batch], \n",
    "                y_train[batch:end_batch], \n",
    "                y_pred, \n",
    "                lr\n",
    "            )\n",
    "\n",
    "        loss = round(sum(loss) / len(loss), 4)\n",
    "        loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "        print(f'epoch: {epoch} - MSE: {loss} - MSE_v: {loss_v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch vs Torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLinearRegression(nn.Module):\n",
    "    def __init__(self, n_features, n_out_features):\n",
    "        super(TorchLinearRegression, self).__init__()\n",
    "        self.layer = nn.Linear(n_features, n_out_features, device=device)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.forward(x)\n",
    "            return self.loss(y_pred, y).item()\n",
    "    \n",
    "    def fit(self, x, y, epochs, lr, batch_size, x_valid, y_valid):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            loss_t = []\n",
    "            for batch in range(0, len(y), batch_size):\n",
    "                end_batch = batch + batch_size\n",
    "\n",
    "                y_pred = self.forward(x[batch:end_batch])\n",
    "                loss = self.loss(y_pred, y[batch:end_batch])\n",
    "                loss_t.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            loss_t = round(sum(loss_t) / len(loss_t), 4)\n",
    "            loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "            print(f'epoch: {epoch} - MSE: {loss_t} - MSE_v: {loss_v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = TorchLinearRegression(N, NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(N, NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import MAPE modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape imported locally.\n"
     ]
    }
   ],
   "source": [
    "# This cell imports torch_mape \n",
    "# if you are running this notebook locally \n",
    "# or from Google Colab.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "try:\n",
    "    from tools.torch_metrics import torch_mape as mape\n",
    "    print('mape imported locally.')\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess\n",
    "\n",
    "    repo_url = 'https://raw.githubusercontent.com/PilotLeoYan/inside-deep-learning/main/content/tools/torch_metrics.py'\n",
    "    local_file = 'torch_metrics.py'\n",
    "    \n",
    "    subprocess.run(['wget', repo_url, '-O', local_file], check=True)\n",
    "    try:\n",
    "        from torch_metrics import torch_mape as mape # type: ignore\n",
    "        print('mape imported from GitHub.')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2197.4368197282774"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.copy_params(torch_model.layer)\n",
    "parameters = (model.b.clone(), model.w.clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.evaluate(X_valid, Y_valid),\n",
    "    torch_model.evaluate(X_valid, Y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01 # learning rate\n",
    "EPOCHS = 16 # number of epochs\n",
    "BATCH = len(X_train) // 3 # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 10946.1981 - MSE_v: 13886.847\n",
      "epoch: 1 - MSE: 10406.2912 - MSE_v: 13335.2266\n",
      "epoch: 2 - MSE: 9899.9783 - MSE_v: 12810.1987\n",
      "epoch: 3 - MSE: 9424.6867 - MSE_v: 12310.1493\n",
      "epoch: 4 - MSE: 8978.0703 - MSE_v: 11833.5854\n",
      "epoch: 5 - MSE: 8557.9883 - MSE_v: 11379.1252\n",
      "epoch: 6 - MSE: 8162.4854 - MSE_v: 10945.4878\n",
      "epoch: 7 - MSE: 7789.7747 - MSE_v: 10531.4851\n",
      "epoch: 8 - MSE: 7438.2213 - MSE_v: 10136.0133\n",
      "epoch: 9 - MSE: 7106.3289 - MSE_v: 9758.0464\n",
      "epoch: 10 - MSE: 6792.726 - MSE_v: 9396.6291\n",
      "epoch: 11 - MSE: 6496.1552 - MSE_v: 9050.8717\n",
      "epoch: 12 - MSE: 6215.4618 - MSE_v: 8719.9438\n",
      "epoch: 13 - MSE: 5949.5854 - MSE_v: 8403.0706\n",
      "epoch: 14 - MSE: 5697.5505 - MSE_v: 8099.5273\n",
      "epoch: 15 - MSE: 5458.4591 - MSE_v: 7808.6362\n"
     ]
    }
   ],
   "source": [
    "torch_model.fit(\n",
    "    X_train, Y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 10946.1981 - MSE_v: 13886.847\n",
      "epoch: 1 - MSE: 10406.2912 - MSE_v: 13335.2266\n",
      "epoch: 2 - MSE: 9899.9783 - MSE_v: 12810.1987\n",
      "epoch: 3 - MSE: 9424.6867 - MSE_v: 12310.1493\n",
      "epoch: 4 - MSE: 8978.0703 - MSE_v: 11833.5854\n",
      "epoch: 5 - MSE: 8557.9883 - MSE_v: 11379.1252\n",
      "epoch: 6 - MSE: 8162.4854 - MSE_v: 10945.4878\n",
      "epoch: 7 - MSE: 7789.7747 - MSE_v: 10531.4851\n",
      "epoch: 8 - MSE: 7438.2213 - MSE_v: 10136.0133\n",
      "epoch: 9 - MSE: 7106.3289 - MSE_v: 9758.0464\n",
      "epoch: 10 - MSE: 6792.726 - MSE_v: 9396.6291\n",
      "epoch: 11 - MSE: 6496.1552 - MSE_v: 9050.8717\n",
      "epoch: 12 - MSE: 6215.4618 - MSE_v: 8719.9438\n",
      "epoch: 13 - MSE: 5949.5854 - MSE_v: 8403.0706\n",
      "epoch: 14 - MSE: 5697.5505 - MSE_v: 8099.5273\n",
      "epoch: 15 - MSE: 5458.4591 - MSE_v: 7808.6362\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, Y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.346029948619964e-14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1763214378726547e-14"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.w.clone(),\n",
    "    torch_model.layer.weight.detach().T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.474097185420875e-15"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.b.clone(),\n",
    "    torch_model.layer.bias.detach()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute gradient with einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{b}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where their shapes are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{n \\times n_{1}} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{n_{1}} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}} &\\in \\mathbb{R}^{m \\times n_{1}} \\\\\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}\n",
    "{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{(m \\times n_{1}) \\times (n \\times n_{1})} \\\\\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}\n",
    "{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{(m \\times n_{1}) \\times (n_{1})}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: check $\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}}$\n",
    "has four axes. This is an example because this method requires more computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted sum derivative respect to bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial b_{p}} = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } j=p \\\\ \n",
    "    0 & \\text{if } j\\neq p \n",
    "\\end{cases}\n",
    "$$\n",
    "for all $i = 1, \\ldots, m$ and $j, p = 1, \\ldots, n_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted sum derivative respect to weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\hat{y}_{ij}}{\\partial w_{pq}} = \n",
    "\\begin{cases}\n",
    "    x_{ip} & \\text{if } j=q \\\\ \n",
    "    0 & \\text{if } j\\neq q \n",
    "\\end{cases}\n",
    "$$\n",
    "for all $i = 1, \\ldots, m$,<br>\n",
    "$j, q = 1, \\ldots, n_{1}$ and <br>\n",
    "$p = 1, \\ldots, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}} = \n",
    "\\mathbb{I} \\otimes \\mathbf{X}\n",
    "$$\n",
    "where $\\otimes$ is Kronecker product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore using **Einstein summation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "{\\color{Magenta} {\\frac{\\partial L}{\\partial \\mathbf{b}}}} &=\n",
    "{\\color{Orange} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}}}\n",
    "{\\color{Cyan} {\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{b}}}} \\\\\n",
    "&\\in \\mathbb{R}^{\n",
    "    {\\color{Orange} {(m \\times n_{1})}} \\times \n",
    "    {\\color{Cyan} {(m \\times n_{1} \\times n_{1})}}} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} {n_{1}}}\n",
    "\\end{align*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "{\\color{Magenta} {\\frac{\\partial L}{\\partial \\mathbf{W}}}} &=\n",
    "{\\color{Orange} {\\frac{\\partial L}{\\partial \\mathbf{\\hat{Y}}}}}\n",
    "{\\color{Cyan} {\\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{W}}}} \\\\\n",
    "&\\in \\mathbb{R}^{\n",
    "    {\\color{Orange} {(m \\times n_{1})}} \\times \n",
    "    {\\color{Cyan} {(m \\times n_{1} \\times n \\times n_{1})}}} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} {n \\times n_{1}}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EinsumLinearRegression(LinearRegression):\n",
    "    def update(self, x: torch.Tensor, y_true: torch.Tensor, y_pred: torch.Tensor, lr: float):\n",
    "        \"\"\"\n",
    "        Update the model parameters.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (n_samples, n_features).\n",
    "            y_true: Target tensor of shape (n_samples, n_features).\n",
    "            y_pred: Predicted output tensor of shape (n_samples, n_features).\n",
    "            lr: Learning rate. \n",
    "        \"\"\"\n",
    "        delta = 2 * (y_pred - y_true) / y_true.numel()\n",
    "        # d L / d b\n",
    "        self.b -= lr * delta.sum(axis=0)\n",
    "        # d L / d W\n",
    "        identity = torch.eye(y_true.shape[-1], device=device)\n",
    "        w_der = torch.kron(\n",
    "            x.unsqueeze(1).unsqueeze(3),\n",
    "            identity.unsqueeze(0).unsqueeze(2)\n",
    "        )\n",
    "        self.w -= lr * torch.einsum('pq,pqij->ij', delta, w_der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0704, -0.2301, -0.0023],\n",
       "        [-0.3462,  0.3585,  0.3758],\n",
       "        [ 0.2033, -0.0865,  0.0792],\n",
       "        [ 0.2388,  0.2081,  0.3940],\n",
       "        [ 0.2684,  0.2192,  0.2693],\n",
       "        [ 0.0848,  0.1943, -0.2620]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einsum_model = EinsumLinearRegression(N, NO)\n",
    "einsum_model.b.copy_(parameters[0])\n",
    "einsum_model.w.copy_(parameters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 10946.1981 - MSE_v: 13886.847\n",
      "epoch: 1 - MSE: 10406.2912 - MSE_v: 13335.2266\n",
      "epoch: 2 - MSE: 9899.9783 - MSE_v: 12810.1987\n",
      "epoch: 3 - MSE: 9424.6867 - MSE_v: 12310.1493\n",
      "epoch: 4 - MSE: 8978.0703 - MSE_v: 11833.5854\n",
      "epoch: 5 - MSE: 8557.9883 - MSE_v: 11379.1252\n",
      "epoch: 6 - MSE: 8162.4854 - MSE_v: 10945.4878\n",
      "epoch: 7 - MSE: 7789.7747 - MSE_v: 10531.4851\n",
      "epoch: 8 - MSE: 7438.2213 - MSE_v: 10136.0133\n",
      "epoch: 9 - MSE: 7106.3289 - MSE_v: 9758.0464\n",
      "epoch: 10 - MSE: 6792.726 - MSE_v: 9396.6291\n",
      "epoch: 11 - MSE: 6496.1552 - MSE_v: 9050.8717\n",
      "epoch: 12 - MSE: 6215.4618 - MSE_v: 8719.9438\n",
      "epoch: 13 - MSE: 5949.5854 - MSE_v: 8403.0706\n",
      "epoch: 14 - MSE: 5697.5505 - MSE_v: 8099.5273\n",
      "epoch: 15 - MSE: 5458.4591 - MSE_v: 7808.6362\n"
     ]
    }
   ],
   "source": [
    "einsum_model.fit(\n",
    "    X_train, Y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.094846515726872e-15"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    einsum_model.w.clone(),\n",
    "    torch_model.layer.weight.detach().T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.474097185420875e-15"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    einsum_model.b.clone(),\n",
    "    torch_model.layer.bias.detach()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idl-3-14-0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
