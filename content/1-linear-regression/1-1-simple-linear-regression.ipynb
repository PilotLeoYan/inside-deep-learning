{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 - Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{grid} 1 1 2 2\n",
    "```{card} [Open in Google Colab](https://colab.research.google.com/github/PilotLeoYan/inside-deep-learning/blob/main/content/1-linear-regression/1-1-simple-linear-regression.ipynb)\n",
    "```{image} ../figures/colab_logo.png\n",
    ":align: center\n",
    "```\n",
    "```{card} [Open in Jupyter NBViewer](https://nbviewer.org/github/PilotLeoYan/inside-deep-learning/blob/main/content/1-linear-regression/1-1-simple-linear-regression.ipynb)\n",
    "```{image} ../figures/jupyter_logo.png\n",
    ":align: center\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to start with a topic before getting into deep learning, \n",
    "the perceptron is a good place to start, \n",
    "as it is the basic unit with which artificial neural networks (ANNs) are built.\n",
    "We can then use multiple perceptrons in parallel to form a dense layer. \n",
    "By using multiple dense layers, we can build a deep neural network (DNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} ../figures/simple-perceptron.png\n",
    ":width: 300\n",
    ":class: hidden dark:block\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} ../figures/simple-perceptron-light.png\n",
    ":width: 300\n",
    ":class: dark:hidden\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of simple linear regression is to predict \n",
    "the target data $\\mathbf{y}$ based on \n",
    "the input data $\\mathbf{x}$\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = f(\\mathbf{x}) + \\epsilon\n",
    "$$\n",
    "\n",
    "where $f(\\cdot)$ is the true function, but it is unknown,\n",
    "and $\\epsilon$ is a intrinsic noise independent of $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose of this Notebook**:\n",
    "\n",
    "1. Create a dataset for simple linear regression task\n",
    "2. Create our own Perceptron class from scratch\n",
    "3. Calculate the gradient descent from scratch\n",
    "4. Train our Perceptron\n",
    "5. Compare our Perceptron to the one prebuilt by PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start package installation...\n"
     ]
    }
   ],
   "source": [
    "print('Start package installation...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "print('Packages installed successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.14.0', '2.9.0+cu126')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from platform import python_version\n",
    "python_version(), torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj): setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our supervised task, we have a *dataset* denoted\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\left\\{\n",
    "    (x_{1}, y_{1}), \\cdots, (x_{m}, y_{m})\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of samples in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that $x_i$ predicts $y_i$, and \n",
    "$(x_{1}, y_{1}), \\cdots, (x_{m}, y_{m})$ is \n",
    "*independent and identical distributed* (iid assumption).\n",
    "Independent means that two samples \n",
    "$(x_i, y_{i}), (x_{j}, y_{j}), \\; i \\neq j$\n",
    "do not statistically depende on each other,\n",
    "and identical distributed means that all $(x_{i}, y_{i})$\n",
    "is distributed from the same unknown distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data $x_{i}$ can be represented as a vector\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix}\n",
    "    x_{1} \\\\ \\vdots \\\\ x_{m}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the target data $y_{i}$ can be also represented as a vector\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix}\n",
    "    y_{1} \\\\ \\vdots \\\\ y_{m}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10100,)\n",
      "(10100,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import random\n",
    "\n",
    "M: int = 10_100 # number of samples\n",
    "\n",
    "X, Y = make_regression(\n",
    "    n_samples=M, \n",
    "    n_features=1, \n",
    "    n_targets=1,\n",
    "    bias=random.random(), # random true bias\n",
    "    noise=1\n",
    ")\n",
    "\n",
    "X = X.squeeze() # remove the axis of length 1\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split the dataset $\\mathcal{D}$ into two sets,\n",
    "the *training dataset* $\\mathcal{D}_{\\text{train}}$ \n",
    "and *test dataset* $\\mathcal{D}_{\\text{test}}$.\n",
    "\n",
    "+ Train dataset dataset is used to train and calibrate our models\n",
    "+ Test dataset is utilized for the purpose of evaluating our pre-trained models\n",
    "\n",
    "**Remark**: $\\mathcal{D}_{\\text{train}}$ and\n",
    "$\\mathcal{D}_{\\text{test}}$ are disjoint, \n",
    "$\\mathcal{D}_{\\text{train}} \\cap \\mathcal{D}_{\\text{test}} = \\varnothing$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refer $\\mathbf{x}_{\\text{train}}, \\mathbf{y}_{\\text{train}}$ as \n",
    "training input and target data respectively, and $\\mathbf{x}_{\\text{test}}, \\mathbf{y}_{\\text{test}}$ \n",
    "as test input and target data respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100]), torch.Size([100]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(X[:100], device=device)\n",
    "Y_train = torch.tensor(Y[:100], device=device)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000]), torch.Size([10000]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = torch.tensor(X[100:], device=device)\n",
    "Y_valid = torch.tensor(Y[100:], device=device)\n",
    "X_valid.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We left more examples in the test set for better comparison purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete raw dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected $\\hat{y}(\\cdot)$ to approximate the true function $f(\\cdot)$\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = b + xw\n",
    "$$\n",
    "\n",
    "where our model $\\hat{y}$ has two *trainable parameters* \n",
    "$b, w \\in \\mathbb{R}$ are called *bias* and *weight* respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegression:\n",
    "    def __init__(self):\n",
    "        self.w = torch.randn(1, device=device)\n",
    "        self.b = torch.randn(1, device=device)\n",
    "\n",
    "    def copy_params(self, torch_layer: nn.modules.linear.Linear):\n",
    "        \"\"\"\n",
    "        Copy the parameters from a module.linear to this model.\n",
    "\n",
    "        Args:\n",
    "            torch_layer: Pytorch module from which to copy the parameters.\n",
    "        \"\"\"\n",
    "        self.b.copy_(torch_layer.bias.detach().clone())\n",
    "        self.w.copy_(torch_layer.weight[0,:].detach().clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weighted sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will refer $\\hat{y}(\\cdot)$ as simply *weighted sum* function $\\hat{\\mathbf{y}}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{y}}: \\mathbb{R}^{m} &\\to \\mathbb{R}^{m} \\\\\n",
    "\\mathbf{x} &\\mapsto \\hat{\\mathbf{y}}(\\mathbf{x}), \\;\n",
    "\\mathbf{x} \\in \\mathbb{R}^{m}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Note**: we remark $\\hat{\\mathbf{y}}$ with **bold** because \n",
    "given a vector $\\mathbf{x}$, $\\hat{\\mathbf{y}}$ is a vector too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an input $\\mathbf{x}$ (not necessary the training dataset)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{y}} &= b + w \\mathbf{x} \\\\\n",
    "&= b + w \\begin{bmatrix}\n",
    "    x_{1} \\\\ \\vdots \\\\ x_{m}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    b + wx_{1} \\\\ \\vdots \\\\ b + wx_{m}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Note**: we are going to call $\\hat{\\mathbf{y}}$ as *predicted output data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Predict the output for input x.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        y_pred: Predicted output tensor of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    return self.b + self.w * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a loss function. We will use Mean Squared Error (MSE) \n",
    "as $L$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L: \\mathbb{R}^{m} &\\to \\mathbb{R} \\\\\n",
    "\\hat{\\mathbf{y}} &\\mapsto L(\\hat{\\mathbf{y}}), \\;\n",
    "\\hat{\\mathbf{y}} \\in \\mathbb{R}^{m}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "this will help us to fit our trainables parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE is defined as\n",
    "\n",
    "$$\n",
    "L(\\hat{\\mathbf{y}}) = \\frac{1}{m} \\sum_{i=1}^{m}\n",
    "\\left( \\hat{y}_{i} - y_{i} \\right)^{2}\n",
    "$$\n",
    "\n",
    "or using a vectorized form\n",
    "\n",
    "$$\n",
    "L(\\hat{\\mathbf{y}}) = \\frac{1}{m} \n",
    "\\left\\| \\hat{\\mathbf{y}} - \\mathbf{y} \\right\\|_{2}^2\n",
    "$$\n",
    "\n",
    "**Note**: $\\|\\cdot\\|_{2}$ is the *Euclidean norm* or\n",
    "also called $\\ell_{2}$ norm (L2 norm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def mse_loss(self, y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "    \"\"\"\n",
    "    MSE loss function between target y_true and y_pred.\n",
    "\n",
    "    Args:\n",
    "        y_true: Target tensor of shape (n_samples,).\n",
    "        y_pred: Predicted tensor of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        loss: MSE loss between predictions and true values.\n",
    "    \"\"\"\n",
    "    return ((y_pred - y_true)**2).mean().item()\n",
    "\n",
    "@add_to_class(SimpleLinearRegression)\n",
    "def evaluate(self, x: torch.Tensor, y_true: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Evaluate the model on input x and target y_true using MSE.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples,).\n",
    "        y_true: Target tensor of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        loss: MSE loss between predictions and true values.\n",
    "    \"\"\"\n",
    "    y_pred = self.predict(x)\n",
    "    return self.mse_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make adjustments to our model, it is necessary to compute derivatives. \n",
    "+ First, we must determine the derivatives to be computed\n",
    "+ Then, we must ascertain the size of each derivative\n",
    "+ Finally, we can compute the derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain rule, we can determine the derivatives\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} =\n",
    "\\frac{\\partial L}{\\partial \\hat{\\mathbf{y}}}\n",
    "\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial b}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} =\n",
    "\\frac{\\partial L}{\\partial \\hat{\\mathbf{y}}}\n",
    "\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can determine the size of each derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} \\in \\mathbb{R},\n",
    "\\frac{\\partial L}{\\partial w} \\in \\mathbb{R},\n",
    "\\frac{\\partial L}{\\partial \\hat{\\mathbf{y}}} \\in \\mathbb{R}^{m},\n",
    "\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial b} \\in \\mathbb{R}^{m},\n",
    "\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial w} \\in \\mathbb{R}^{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of MSE respect to $\\mathbf{\\hat{y}}$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{\\mathbf{y}}} = \\begin{bmatrix}\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}_{1}} \\\\ \\vdots \\\\\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}_{m}} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{p}} &=\n",
    "\\frac{\\partial}{\\partial \\hat{y}_{p}} \\left(\n",
    "    \\frac{1}{m} \\sum_{i=1}^{m} \\left(\n",
    "    \\hat{y}_{i} - y_{i} \\right)^{2}\n",
    "\\right) \\\\\n",
    "&= \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial \\hat{y}_{p}} \\left(\n",
    "    \\left(\\hat{y}_{i} - y_{i} \\right)^{2}\n",
    "\\right) \\\\\n",
    "&= \\frac{2}{m} (\\hat{y}_{p} - y_{p})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for all $p = 1, \\ldots, m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{m} \\frac{\\partial}{\\partial \\hat{y}_{p}} \\left(\n",
    "    \\left(\\hat{y}_{i} - y_{i} \\right)^{2}\n",
    "\\right) &=\n",
    "\\frac{\\partial}{\\partial \\hat{y}_{p}} \\left(\n",
    "    (\\hat{y}_{1} - y_{1})^{2} + \\ldots + (\\hat{y}_{p} - y_{p})^{2}\n",
    "    + \\ldots + (\\hat{y}_{m} - y_{m})^{2}\n",
    "\\right) \\\\\n",
    "&= 0 + \\ldots + \\frac{\\partial}{\\partial \\hat{y}_{p}} \\left(\n",
    "    (\\hat{y}_{p} - y_{p})^{2}\n",
    "\\right) + \\ldots + 0 \\\\\n",
    "&= 2 (\\hat{y}_{p} - y_{p})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\hat{\\mathbf{y}}} &= \\begin{bmatrix}\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}_{1}} \\\\ \\vdots \\\\\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}_{m}} \n",
    "\\end{bmatrix} \\\\\n",
    "&= \\frac{2}{m} \\begin{bmatrix}\n",
    "    \\hat{y}_{1} - y_{1} \\\\ \\vdots \\\\\n",
    "    \\hat{y}_{m} - y_{m}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\frac{2}{m} \\left(\n",
    "    \\mathbf{\\hat{y}} - \\mathbf{y}\n",
    "\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted sum derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### respect to bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of weighted sum respect to bias is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial b} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\hat{y}_{1}}{\\partial b} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial \\hat{y}_{m}}{\\partial b}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\hat{y}_{p}}{\\partial b} &= \n",
    "\\frac{\\partial}{\\partial b} \\left( \n",
    "    b + w x_{p}\n",
    "\\right) \\\\\n",
    "&= 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for all $m = 1, \\ldots, m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial b} &=\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\hat{y}_{1}}{\\partial b} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial \\hat{y}_{m}}{\\partial b}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    1 \\\\ \\vdots \\\\ 1\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\mathbf{1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Remark**: $\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial b} = \\mathbf{1} \\in \\mathbb{R}^{m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### respect to weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of weighted sum respecto to weight is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial w} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\hat{y}_{1}}{\\partial w} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial \\hat{y}_{m}}{\\partial w}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\hat{y}_{p}}{\\partial w} &=\n",
    "\\frac{\\partial}{\\partial w} \\left(\n",
    "    b + w x_{p}\n",
    "\\right) \\\\\n",
    "&= 0 + \\frac{\\partial}{\\partial w} \\left(\n",
    "    w x_{p}\n",
    "\\right) \\\\\n",
    "&= x_{p}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for all $p = 1, \\ldots, m$.\n",
    "\n",
    "**Note**: Remember the $i$-th predicted output $\\hat{y}_{i}$ is\n",
    "based on $x_{i}$, $\\hat{y}_{i} = b + w x_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial w} &=\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\hat{y}_{1}}{\\partial w} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial \\hat{y}_{m}}{\\partial w}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    x_{1} \\\\ \\vdots \\\\ x_{m}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\mathbf{x}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed all the derivatives, \n",
    "we can find the gradients by composing these derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{b}L =\n",
    "\\frac{\\partial L}{\\partial b} &=\n",
    "{\\color{Cyan} {\\frac{\\partial L}{\\partial \\hat{\\mathbf{y}}}}}\n",
    "{\\color{Orange} {\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial b}}} \\\\\n",
    "&= {\\color{Cyan} {\\frac{2}{m} \\left(\\hat{\\mathbf{y}} - \\mathbf{y} \\right)}}\n",
    "{\\color{Orange} {\\mathbf{1}}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{w}L =\n",
    "\\frac{\\partial L}{\\partial w} &=\n",
    "{\\color{Cyan} {\\frac{\\partial L}{\\partial \\hat{\\mathbf{y}}}}}\n",
    "{\\color{Magenta} {\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial w}}} \\\\\n",
    "&= {\\color{Cyan} {\\frac{2}{m} \\left(\\hat{\\mathbf{y}} - \\mathbf{y} \\right)}}\n",
    "{\\color{Magenta} {\\mathbf{x}}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's update the trainable parameters using **gradient descent** (GD) as follows\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "b &\\leftarrow b -\\eta \\nabla_{b}L \\\\ &=\n",
    "b -\\eta \\left(\n",
    "    \\frac{2}{m} (\\hat{\\mathbf{y}} - \\mathbf{y}) \\mathbf{1}\n",
    "\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "w &\\leftarrow w -\\eta \\nabla_{w}L \\\\ &=\n",
    "w -\\eta \\left(\n",
    "    \\frac{2}{m} (\\hat{\\mathbf{y}} - \\mathbf{y}) \\mathbf{x}\n",
    "\\right)\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "where $\\eta \\in \\mathbb{R}^{+}$ is called *learning rate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def update(self, x: torch.Tensor, y_true: torch.Tensor, \n",
    "           y_pred: torch.Tensor, lr: float):\n",
    "    \"\"\"\n",
    "    Update the model parameters.\n",
    "\n",
    "    Args:\n",
    "       x: Input tensor of shape (n_samples,).\n",
    "       y_true: Target tensor of shape (n_samples,).\n",
    "       y_pred: Predicted output tensor of shape (n_samples,).\n",
    "       lr: Learning rate. \n",
    "    \"\"\"\n",
    "    delta = 2 * (y_pred - y_true) / len(y_true)\n",
    "    self.b -= lr * delta.sum()\n",
    "    self.w -= lr * torch.matmul(delta, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use *mini-batch gradient descent* (mini-batch GD) to adjust the parameters of our model\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Algorithm: mini-batch Gradient Descent} \\\\\n",
    "\\textbf{for } t = 1 \\text{ to } T \\textbf{ do} \\\\\n",
    "\\quad i \\leftarrow 1 \\\\\n",
    "\\quad j \\leftarrow \\mathcal{B} \\\\\n",
    "\\quad \\textbf{while } i < m \\textbf{ do} \\\\\n",
    "\\quad \\quad \\mathbf{\\theta} \\leftarrow \n",
    "\\text{update}(\\mathbf{x}_{\\text{train } i:j,:}, \n",
    "\\mathbf{y}_{\\text{train } i:j}; \\mathbf{\\theta}) \\\\\n",
    "\\quad \\quad i \\leftarrow i + \\mathcal{B} \\\\\n",
    "\\quad \\quad j \\leftarrow j + \\mathcal{B} \\\\\n",
    "\\textbf{end for}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where:\n",
    "+ $T$ is the number of epochs\n",
    "+ $\\theta$ is an arbitrary model's parameter, in our case are $w$ and $b$\n",
    "+ $\\mathcal{B}$ is the number of samples per minibatch\n",
    "+ $\\mathbf{x}_{\\text{train } i:j,:}$ and $\\mathbf{y}_{\\text{train } i:j}$ \n",
    "are the $i$-th to $j$-th train samples\n",
    "\n",
    "**Note**: $\\eta, T, \\mathcal{B}$ are called *hyperparameters*, because\n",
    "they are adjusted by the developer rather than the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about types of gradient descents, please watch \n",
    "[gradient descents](./gradient-descents.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def fit(self, x: torch.Tensor, y: torch.Tensor, \n",
    "        epochs: int, lr: float, batch_size: int, \n",
    "        x_valid: torch.Tensor, y_valid: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Fit the model using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples,).\n",
    "        y: Target tensor of shape (n_samples,).\n",
    "        epochs: Number of epochs to fit.\n",
    "        lr: learning rate.\n",
    "        batch_size: Int number of batch.\n",
    "        x_valid: Input tensor of shape (n_valid_samples,).\n",
    "        y_valid: Target tensor of shape (n_valid_samples,).\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        loss = []\n",
    "        for batch in range(0, len(y), batch_size):\n",
    "            end_batch = batch + batch_size\n",
    "\n",
    "            y_pred = self.predict(x[batch:end_batch])\n",
    "\n",
    "            loss.append(self.mse_loss(\n",
    "                y[batch:end_batch],\n",
    "                y_pred\n",
    "            ))\n",
    "\n",
    "            self.update(\n",
    "                x[batch:end_batch], \n",
    "                y[batch:end_batch], \n",
    "                y_pred, \n",
    "                lr\n",
    "            )\n",
    "\n",
    "        loss = round(sum(loss) / len(loss), 4)\n",
    "        loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "        print(f'epoch: {epoch} - MSE: {loss} - MSE_v: {loss_v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch vs Torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be implementing a model created with PyTorch's pre-built classes for linear regression. \n",
    "This will allow us to compare our model from scratch with the PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLinearRegression(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(TorchLinearRegression, self).__init__()\n",
    "        self.layer = nn.Linear(n_features, 1, device=device)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.forward(x)\n",
    "            return self.loss(y_pred, y).item()\n",
    "    \n",
    "    def fit(self, x, y, epochs, lr, batch_size, x_valid, y_valid):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            loss_t = [] # train loss\n",
    "            for batch in range(0, len(y), batch_size):\n",
    "                end_batch = batch + batch_size\n",
    "\n",
    "                y_pred = self.forward(x[batch:end_batch])\n",
    "                loss = self.loss(y_pred, y[batch:end_batch])\n",
    "                loss_t.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            loss_t = round(sum(loss_t) / len(loss_t), 4)\n",
    "            loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "            print(f'epoch: {epoch} - MSE: {loss_t} - MSE_v: {loss_v}')\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = TorchLinearRegression(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a *metric* to compare our model with the PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import MAPE modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a modification of *MAPE* as a metric\n",
    "\n",
    "$$\n",
    "\\text{MAPE}(\\mathbf{y}, \\hat{\\mathbf{y}}) =\n",
    "\\frac{1}{m} \\sum^{m}_{i=1} \\mathcal{L} (y_{i}, \\hat{y}_{i})\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathcal{L} (y_{i}, \\hat{y}_{i}) = \\begin{cases}\n",
    "    \\left| \\frac{y_{i} - \\hat{y}_{i}}{y_{i}} \\right|\n",
    "    & \\text{if } y_{i} \\neq 0 \\\\\n",
    "    \\left| \\hat{y}_{i} \\right| & \\text{if } \\hat{y}_{i} = 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape imported locally.\n"
     ]
    }
   ],
   "source": [
    "# This cell imports torch_mape \n",
    "# if you are running this notebook locally \n",
    "# or from Google Colab.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "try:\n",
    "    from tools.torch_metrics import torch_mape as mape\n",
    "    print('mape imported locally.')\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess\n",
    "\n",
    "    repo_url = 'https://raw.githubusercontent.com/PilotLeoYan/inside-deep-learning/main/content/tools/torch_metrics.py'\n",
    "    local_file = 'torch_metrics.py'\n",
    "    \n",
    "    subprocess.run(['wget', repo_url, '-O', local_file], check=True)\n",
    "    try:\n",
    "        from torch_metrics import torch_mape as mape # type: ignore\n",
    "        print('mape imported from GitHub.')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the predictions of our model and PyTorch's using modified MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.991354499247294"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid.unsqueeze(-1)).squeeze(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "they differ considerably because each model has its own parameters \n",
    "initialized randomly and independently of the other model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy the values of the PyTorch model parameters to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.copy_params(torch_model.layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after copy parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the difference between the predictions of both models again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.968012456432162e-16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid.unsqueeze(-1)).squeeze(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that their predictions do not differ greatly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.evaluate(X_valid, Y_valid),\n",
    "    torch_model.evaluate(X_valid.unsqueeze(-1), Y_valid.unsqueeze(-1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train both models using the same hyperparameters. \n",
    "If our model is well designed, then starting from the same parameters, \n",
    "it should arrive at the same parameters as the PyTorch model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01 # learning rate\n",
    "EPOCHS = 16 # number of epochs\n",
    "BATCH = len(X_train) // 3 # number of minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 727.0301 - MSE_v: 1094.5279\n",
      "epoch: 1 - MSE: 652.7683 - MSE_v: 990.9578\n",
      "epoch: 2 - MSE: 586.7811 - MSE_v: 897.8346\n",
      "epoch: 3 - MSE: 528.0545 - MSE_v: 814.0122\n",
      "epoch: 4 - MSE: 475.71 - MSE_v: 738.483\n",
      "epoch: 5 - MSE: 428.9855 - MSE_v: 670.3594\n",
      "epoch: 6 - MSE: 387.2188 - MSE_v: 608.858\n",
      "epoch: 7 - MSE: 349.8329 - MSE_v: 553.2864\n",
      "epoch: 8 - MSE: 316.3249 - MSE_v: 503.0315\n",
      "epoch: 9 - MSE: 286.2552 - MSE_v: 457.5494\n",
      "epoch: 10 - MSE: 259.2391 - MSE_v: 416.3571\n",
      "epoch: 11 - MSE: 234.9392 - MSE_v: 379.0245\n",
      "epoch: 12 - MSE: 213.0589 - MSE_v: 345.1686\n",
      "epoch: 13 - MSE: 193.3376 - MSE_v: 314.4474\n",
      "epoch: 14 - MSE: 175.5452 - MSE_v: 286.5551\n",
      "epoch: 15 - MSE: 159.4786 - MSE_v: 261.2183\n"
     ]
    }
   ],
   "source": [
    "torch_model.fit(\n",
    "    X_train.unsqueeze(-1), \n",
    "    Y_train.unsqueeze(-1),\n",
    "    EPOCHS, LR, BATCH,\n",
    "    X_valid.unsqueeze(-1),\n",
    "    Y_valid.unsqueeze(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 727.0301 - MSE_v: 1094.5279\n",
      "epoch: 1 - MSE: 652.7683 - MSE_v: 990.9578\n",
      "epoch: 2 - MSE: 586.7811 - MSE_v: 897.8346\n",
      "epoch: 3 - MSE: 528.0545 - MSE_v: 814.0122\n",
      "epoch: 4 - MSE: 475.71 - MSE_v: 738.483\n",
      "epoch: 5 - MSE: 428.9855 - MSE_v: 670.3594\n",
      "epoch: 6 - MSE: 387.2188 - MSE_v: 608.858\n",
      "epoch: 7 - MSE: 349.8329 - MSE_v: 553.2864\n",
      "epoch: 8 - MSE: 316.3249 - MSE_v: 503.0315\n",
      "epoch: 9 - MSE: 286.2552 - MSE_v: 457.5494\n",
      "epoch: 10 - MSE: 259.2391 - MSE_v: 416.3571\n",
      "epoch: 11 - MSE: 234.9392 - MSE_v: 379.0245\n",
      "epoch: 12 - MSE: 213.0589 - MSE_v: 345.1686\n",
      "epoch: 13 - MSE: 193.3376 - MSE_v: 314.4474\n",
      "epoch: 14 - MSE: 175.5452 - MSE_v: 286.5551\n",
      "epoch: 15 - MSE: 159.4786 - MSE_v: 261.2183\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, Y_train,\n",
    "    EPOCHS, LR, BATCH,\n",
    "    X_valid, Y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will measure the difference between the predictions of both models after training them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.095967135198154e-17"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(X_valid),\n",
    "    torch_model.forward(X_valid.unsqueeze(-1)).squeeze(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We directly measure the difference between the bias values of both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.b.clone(),\n",
    "    torch_model.layer.bias.detach()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and measure the difference between the weight values of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.w.clone(),\n",
    "    torch_model.layer.weight.detach().squeeze(0)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
