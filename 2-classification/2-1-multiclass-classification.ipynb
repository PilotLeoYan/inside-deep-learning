{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2.1 - Multiclass classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to add a crucial element: the activation function. \n",
    "This function will allow us to modify the output to suit our problem, \n",
    "in this case the classification of multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: black\">\n",
    "<img src=\"../images/perceptron-softmax.png\" alt=\"One perceptron with softmax function\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function will allow us to convert an input \n",
    "into the probability of remaining in the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: black\">\n",
    "<img src=\"../images/perceptron-softmax-2.png\" alt=\"One dense layer with a softmax layer\" width=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret the perceptron with softmax as a dense layer and an activation layer, \n",
    "this interpretation will be useful later in chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.12.6', '2.5.1+cu124')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from platform import python_version\n",
    "python_version(), torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{X} &\\in \\mathbb{R}^{m \\times n} \\\\\n",
    "\\mathbf{Y} &\\in \\mathbb{R}^{m \\times n_{1}}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $n_{1}$ is the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10100, 5)\n",
      "(10100,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "M: int = 10_100 # number of samples\n",
    "N: int = 5 # number of input features\n",
    "CLASSES: int = 3 # number of classes\n",
    "\n",
    "X, Y = make_classification(\n",
    "    n_samples=M, \n",
    "    n_features=N, \n",
    "    n_classes=CLASSES, \n",
    "    n_informative=N - 1, \n",
    "    n_redundant=0\n",
    ")\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10100, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = nn.functional.one_hot(\n",
    "    torch.tensor(Y, device=device).long(), \n",
    "    CLASSES\n",
    ").type(torch.float32)\n",
    "Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset into train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 5]), torch.Size([10000, 5]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.tensor(X[:100], device=device)\n",
    "x_valid = torch.tensor(X[100:], device=device)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 3]), torch.Size([10000, 3]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_valid = Y_hat[:100], Y_hat[100:]\n",
    "y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del Y\n",
    "del Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{W} &\\in \\mathbb{R}^{n \\times n_{1}} \\\\\n",
    "\\mathbf{b} &\\in \\mathbb{R}^{n_{1}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier:\n",
    "    def __init__(self, n_features: int, n_classes: int):\n",
    "        self.w = torch.randn(n_features, n_classes, device=device)\n",
    "        self.b = torch.randn(n_classes, device=device)\n",
    "\n",
    "    def copy_params(self, torch_layer: nn.modules.linear.Linear):\n",
    "        \"\"\"\n",
    "        Copy the parameters from a module.linear to this model.\n",
    "\n",
    "        Args:\n",
    "            torch_layer: Pytorch module from which to copy the parameters.\n",
    "        \"\"\"\n",
    "        self.b.copy_(torch_layer.bias.detach().clone())\n",
    "        self.w.copy_(torch_layer.weight.T.detach().clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weighted sum and softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Z}(\\mathbf{X}) = \\mathbf{X} \\mathbf{W} + \\mathbf{b} \\\\\n",
    "\\mathbf{Z} : \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}^{m \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(\\mathbf{z}_{i,:})_{j} = \n",
    "\\frac{\\exp(z_{ij})}\n",
    "{\\sum_{k=1}^{n_{1}}(\\exp(z_{ik}))}\n",
    "\\in \\mathbb{R}^{+}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(\\mathbf{z}_{i,:}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z}_{i,:})_{1} &\n",
    "    \\sigma(\\mathbf{z}_{i,:})_{2} &\n",
    "    \\cdots &\n",
    "    \\sigma(\\mathbf{z}_{i,:})_{n_{1}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\Sigma(Z)} = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{z}_{m,:})\n",
    "\\end{bmatrix} \\\\\n",
    "\\mathbf{\\Sigma} : \\mathbb{R}^{m \\times n_{1}} \\rightarrow \n",
    "\\mathbb{R}^{m \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Predict the output for input x.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        y_pred: Predicted output tensor of shape (n_samples, n_classes).\n",
    "    \"\"\"\n",
    "    # weighted sum\n",
    "    z = torch.matmul(x, self.w) + self.b\n",
    "    # avoid underflow and overflow\n",
    "    z_norm = z - torch.max(z, dim=1, keepdims=True)[0]\n",
    "    # softmax function\n",
    "    z_exp = torch.exp(z_norm)\n",
    "    return z_exp / z_exp.sum(1, keepdims=True) # y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = \n",
    "- \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}(\n",
    "    y_{ij} \\log_{e}(\\hat{y}_{ij})\n",
    ") \\\\\n",
    "L : \\mathbb{R}^{m \\times n_{1}} \\rightarrow \\mathbb{R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: for this case $\\mathbf{\\hat{Y}}$ is $\\mathbf{\\Sigma(Z)}$. <br>\n",
    "It is not mandatory to use softmax for cross-entropy loss, \n",
    "but some modules like PyTorch require softmax to use cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = \n",
    "- \\frac{1}{m} \\sum_{i=1}^{m} \\left(\n",
    "    \\mathbf{y}_{i,:}^\\top \\log_{e}(\\mathbf{\\hat{y}}_{i,:})\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = \n",
    "- \\frac{1}{m} \\text{sum} \\left(\n",
    "    \\mathbf{Y} \\odot \\log_{e}(\\mathbf{\\hat{Y}})\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def cross_entropy_loss(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    CE loss function between target y_true and y_pred.\n",
    "\n",
    "    Args:\n",
    "        y_true: Target tensor of shape (n_samples, n_classes).\n",
    "        y_pred: Predicted tensor of shape (n_samples, n_classes).\n",
    "\n",
    "    Returns:\n",
    "        loss: CE loss between predictions and true values.\n",
    "    \"\"\"\n",
    "    loss = y_true * torch.log(y_pred)\n",
    "    return - loss.sum().item() / len(y_true)\n",
    "\n",
    "@add_to_class(SoftmaxClassifier)\n",
    "def evaluate(self, x: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on input x and target y_true using CE.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (n_samples, n_features).\n",
    "        y_true: Target tensor of shape (n_samples, n_classes).\n",
    "\n",
    "    Returns:\n",
    "        loss: CE loss between predictions and true values.\n",
    "    \"\"\"\n",
    "    y_pred = self.predict(x)\n",
    "    return self.cross_entropy_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{pq}} =&\n",
    "-\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial \\hat{y}_{pq}} \\left(\n",
    "    y_{ij} \\log_{e}(\\hat{y}_{ij})\n",
    "\\right) \\\\\n",
    "&= -\\frac{1}{m} \\left(\\frac{y_{pq}}{\\hat{y}_{pq}} \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, ..., m$ and $q = 1, ..., n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: $\\hat{y}_{pq}$ must be different of $0$, $\\hat{y}_{pq} \\neq 0$. \n",
    "Softmax returns positive real values,\n",
    "$\\sigma(z) \\in \\mathbb{R}^{+}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{\\mathbf{Y}}} =\n",
    "-\\frac{1}{m} \\left(\n",
    "    \\mathbf{Y} \\oslash \\hat{\\mathbf{Y}}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: $\\oslash$ is element-wise divide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial z_{pq}} =&\n",
    "-\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial z_{pq}} \\left(\n",
    "    y_{ij} \\log_{e}(\\hat{y}_{ij})\n",
    "\\right) \\\\\n",
    "=& \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\sigma_{ij}}\n",
    "\\frac{\\partial \\sigma_{ij}}{\\partial z_{pq}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, ..., m$ and $q = 1, ..., n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{ij}}{\\partial z_{pq}} = \n",
    "\\begin{cases}\n",
    "\\sigma(z_{pq})(1 - \\sigma(z_{pq})) & \\text{if } i=p, j=q \\\\\n",
    "-\\sigma(z_{pq}) \\sigma(z_{ij}) & \\text{if } i=p, j \\neq q \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial z_{pq}} =&\n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\sigma_{ij}}\n",
    "\\frac{\\partial \\sigma_{ij}}{\\partial z_{pq}} \\\\\n",
    "=& \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\sigma_{pj}}\n",
    "\\begin{cases}\n",
    "\\sigma(z_{pq})(1 - \\sigma(z_{pq})) & \\text{if } j=q \\\\\n",
    "-\\sigma(z_{pq}) \\sigma(z_{pj}) & \\text{if } j \\neq q\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** [softmax function and its derivative](softmax-function-and-its-derivative.ipynb) \n",
    "for more information about the softmax derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} = \n",
    "\\mathbf{\\Sigma} \\odot \\left(\n",
    "    \\frac{\\partial L}{\\partial \\mathbf{\\Sigma}}\n",
    "    - \\left(\n",
    "        \\frac{\\partial L}{\\partial \\mathbf{\\Sigma}}\n",
    "        \\odot \\mathbf{\\Sigma}\n",
    "    \\right) \\mathbf{1}\n",
    "\\right)\n",
    "$$\n",
    "where $\\mathbf{1} \\in \\mathbb{R}^{n_{1} \\times n_{1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted sum derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### respect to bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial b_{q}} =&\n",
    "-\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial b_{q}} \\left(\n",
    "    y_{ij} \\log_{e}(\\hat{y}_{ij})\n",
    "\\right) \\\\\n",
    "&= \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial z_{ij}}\n",
    "\\frac{\\partial z_{ij}}{\\partial b_{q}} \\\\\n",
    "&= \\sum_{i=1}^{m}\n",
    "\\frac{\\partial L}{\\partial z_{iq}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $q = 1, ..., n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} = \n",
    "\\mathbf{1}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}}\n",
    "$$\n",
    "where $\\mathbf{1} \\in \\mathbb{R}^{m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### respect to weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_{pq}} =&\n",
    "-\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial}{\\partial w_{pq}} \\left(\n",
    "    y_{ij} \\log_{e}(\\hat{y}_{ij})\n",
    "\\right) \\\\\n",
    "&= \\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial z_{ij}}\n",
    "\\frac{\\partial z_{ij}}{\\partial w_{pq}} \\\\\n",
    "&= \\sum_{i=1}^{m} x_{ip}\n",
    "\\frac{\\partial L}{\\partial z_{iq}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for all $p = 1, ..., m$ and $q = 1, ..., n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} = \n",
    "\\mathbf{X}^\\top \n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def update(self, x: torch.Tensor, y_true: torch.Tensor, \n",
    "           y_pred: torch.Tensor, lr: float) -> None:\n",
    "    \"\"\"\n",
    "    Update the model parameters.\n",
    "\n",
    "    Args:\n",
    "       x: Input tensor of shape (n_samples, n_features).\n",
    "       y_true: Target tensor of shape (n_samples, n_classes).\n",
    "       y_pred: Predicted output tensor of shape (n_samples, n_classes).\n",
    "       lr: Learning rate. \n",
    "    \"\"\"\n",
    "    # cross entropy der\n",
    "    delta = -(y_true / y_pred) / len(y_true)\n",
    "    # softmax der\n",
    "    delta = y_pred * (delta - (delta * y_pred).sum(axis=1, keepdims=True))\n",
    "    # weighted sum der\n",
    "    self.b -= lr * delta.sum(axis=0)\n",
    "    self.w -= lr * (x.T @ delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metric: accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def accuracy(self, y_true, y_pred) -> float:\n",
    "    preds = y_pred.argmax(axis=-1)\n",
    "    compare = (y_true.argmax(axis=-1) == preds).type(torch.float32)\n",
    "    return compare.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def fit(self, x_train: torch.Tensor, y_train: torch.Tensor, \n",
    "        epochs: int, lr: float, batch_size: int, \n",
    "        x_valid: torch.Tensor, y_valid: torch.Tensor) -> None:\n",
    "    \"\"\"\n",
    "    Fit the model using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        x_train: Input tensor of shape (n_samples, num_features).\n",
    "        y_train: Target tensor one hot of shape (n_samples, n_classes).\n",
    "        epochs: Number of epochs to train.\n",
    "        lr: learning rate).\n",
    "        batch_size: Int number of batch.\n",
    "        x_valid: Input tensor of shape (n_valid_samples, num_features).\n",
    "        y_valid: Input tensor one hot of shape (n_valid_samples, n_valid_classes).\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        loss = []\n",
    "        for batch in range(0, len(y_train), batch_size):\n",
    "            batch_end = batch + batch_size\n",
    "\n",
    "            y_pred = self.predict(x_train[batch:batch_end])\n",
    "            loss.append(self.evaluate(\n",
    "                x_train[batch:batch_end], \n",
    "                y_train[batch:batch_end]\n",
    "            ))\n",
    "\n",
    "            self.update(\n",
    "                x_train[batch:batch_end], \n",
    "                y_train[batch:batch_end], \n",
    "                y_pred, lr\n",
    "            )\n",
    "\n",
    "        loss = round(sum(loss) / len(loss), 4)\n",
    "        loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "        acc = round(self.accuracy(y_valid, self.predict(x_valid)), 4)\n",
    "        print(f'epoch: {epoch} - CE: {loss} - CE_v: {loss_v} - acc_v: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch vs nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: nn.CrossEntropyLoss applies Softmax to input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSoftmax(nn.Module):\n",
    "    def __init__(self, n_features, n_out_features):\n",
    "        super(TorchSoftmax, self).__init__()\n",
    "        self.layer = nn.Linear(n_features, n_out_features, device=device)\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.layer(x)\n",
    "        return self.soft(z)\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.layer(x)\n",
    "            # do not use self.soft because nn.CrossEntropyLoss already uses softmax\n",
    "            return self.loss(y_pred, y).item()\n",
    "    \n",
    "    def fit(self, x, y, epochs, lr, batch_size, x_valid, y_valid):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            loss_t = []\n",
    "            for batch in range(0, len(y), batch_size):\n",
    "                batch_end = batch + batch_size\n",
    "\n",
    "                y_pred = self.layer(x[batch:batch_end])\n",
    "                loss = self.loss(y_pred, y[batch:batch_end])\n",
    "                loss_t.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            loss_t = round(sum(loss_t) / len(loss_t), 4)\n",
    "            loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "            print(f'epoch: {epoch} - CE: {loss_t} - CE_v: {loss_v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = TorchSoftmax(N, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifier(N, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### porcentual error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tools.torch_metrics import torch_mape as mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.885864787562941"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(x_valid),\n",
    "    torch_model(x_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.copy_params(torch_model.layer)\n",
    "parameters = (model.b.clone(), model.w.clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after copy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(x_valid),\n",
    "    torch_model(x_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5144195188719238e-16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.evaluate(x_valid, y_valid),\n",
    "    torch_model.evaluate(x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "EPOCHS = 16\n",
    "BATCH = len(x_train) // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - CE: 1.3285 - CE_v: 1.4406\n",
      "epoch: 1 - CE: 1.2951 - CE_v: 1.4167\n",
      "epoch: 2 - CE: 1.2645 - CE_v: 1.3942\n",
      "epoch: 3 - CE: 1.2363 - CE_v: 1.3731\n",
      "epoch: 4 - CE: 1.2103 - CE_v: 1.3533\n",
      "epoch: 5 - CE: 1.1863 - CE_v: 1.3347\n",
      "epoch: 6 - CE: 1.1641 - CE_v: 1.3171\n",
      "epoch: 7 - CE: 1.1436 - CE_v: 1.3005\n",
      "epoch: 8 - CE: 1.1245 - CE_v: 1.2847\n",
      "epoch: 9 - CE: 1.1067 - CE_v: 1.2698\n",
      "epoch: 10 - CE: 1.0901 - CE_v: 1.2555\n",
      "epoch: 11 - CE: 1.0746 - CE_v: 1.242\n",
      "epoch: 12 - CE: 1.0601 - CE_v: 1.229\n",
      "epoch: 13 - CE: 1.0464 - CE_v: 1.2167\n",
      "epoch: 14 - CE: 1.0336 - CE_v: 1.2048\n",
      "epoch: 15 - CE: 1.0214 - CE_v: 1.1935\n"
     ]
    }
   ],
   "source": [
    "torch_model.fit(\n",
    "    x_train, y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    x_valid, y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - CE: 1.3285 - CE_v: 1.4406 - acc_v: 0.2315\n",
      "epoch: 1 - CE: 1.2951 - CE_v: 1.4167 - acc_v: 0.2567\n",
      "epoch: 2 - CE: 1.2645 - CE_v: 1.3942 - acc_v: 0.281\n",
      "epoch: 3 - CE: 1.2363 - CE_v: 1.3731 - acc_v: 0.3007\n",
      "epoch: 4 - CE: 1.2103 - CE_v: 1.3533 - acc_v: 0.3178\n",
      "epoch: 5 - CE: 1.1863 - CE_v: 1.3347 - acc_v: 0.3336\n",
      "epoch: 6 - CE: 1.1641 - CE_v: 1.3171 - acc_v: 0.3467\n",
      "epoch: 7 - CE: 1.1436 - CE_v: 1.3005 - acc_v: 0.355\n",
      "epoch: 8 - CE: 1.1245 - CE_v: 1.2847 - acc_v: 0.3622\n",
      "epoch: 9 - CE: 1.1067 - CE_v: 1.2698 - acc_v: 0.3675\n",
      "epoch: 10 - CE: 1.0901 - CE_v: 1.2555 - acc_v: 0.3749\n",
      "epoch: 11 - CE: 1.0746 - CE_v: 1.242 - acc_v: 0.3799\n",
      "epoch: 12 - CE: 1.0601 - CE_v: 1.229 - acc_v: 0.3833\n",
      "epoch: 13 - CE: 1.0464 - CE_v: 1.2167 - acc_v: 0.3887\n",
      "epoch: 14 - CE: 1.0336 - CE_v: 1.2048 - acc_v: 0.392\n",
      "epoch: 15 - CE: 1.0214 - CE_v: 1.1935 - acc_v: 0.3972\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train, y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    x_valid, y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict after train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0743587313505337e-17"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.predict(x_valid),\n",
    "    torch_model.forward(x_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1582973430557319e-16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.w.clone(),\n",
    "    torch_model.layer.weight.detach().T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.947530105616679e-17"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    model.b.clone(),\n",
    "    torch_model.layer.bias.detach()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute gradient with einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\Sigma}}\n",
    "\\frac{\\partial \\mathbf{\\Sigma}}{\\partial \\mathbf{Z}}\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} =\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\Sigma}}\n",
    "\\frac{\\partial \\mathbf{\\Sigma}}{\\partial \\mathbf{Z}}\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where their shapes are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}\n",
    "{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{n \\times n_{1}} \\\\\n",
    "\\frac{\\partial L}\n",
    "{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{n_{1}} \\\\\n",
    "\\frac{\\partial L}\n",
    "{\\partial \\mathbf{\\Sigma}} &\\in \\mathbb{R}^{m \\times n_{1}} \\\\\n",
    "\\frac{\\partial \\mathbf{\\Sigma}}\n",
    "{\\partial \\mathbf{Z}} &\\in \\mathbb{R}^{(m \\times n_{1}) \\times (m \\times n_{1})} \\\\\n",
    "\\frac{\\partial \\mathbf{Z}}\n",
    "{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{(m \\times n_{1}) \\times (n \\times n_{1})} \\\\\n",
    "\\frac{\\partial \\mathbf{Z}}\n",
    "{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{(m \\times n_{1}) \\times n_{1}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have 2 cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}(\\mathbf{Z})_{i,:}}\n",
    "{\\partial \\mathbf{Z}_{p=i,:}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}(\\mathbf{Z})_{i,:}}\n",
    "{\\partial \\mathbf{Z}_{p\\neq i,:}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}(\\mathbf{Z})_{i,:}}\n",
    "{\\partial \\mathbf{Z}_{p=i,:}} = \\text{diag}(\\sigma(\\mathbf{Z}_{i,:})) \n",
    "- \\sigma(\\mathbf{Z}_{i,:}) \\sigma(\\mathbf{Z}_{i,:})^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}(\\mathbf{Z})_{i,:}}\n",
    "{\\partial \\mathbf{Z}_{p \\neq i,:}} = \\mathbf{0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted sum derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}} = \n",
    "\\mathbb{I} \\otimes \\mathbf{X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial z_{ij}}{\\partial b_{p}} = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } j=p \\\\ \n",
    "    0 & \\text{if } j\\neq p \n",
    "\\end{cases}\n",
    "$$\n",
    "for all $i = 1, ..., m$ and $j, p = 1, ..., n_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore using **Einstein summation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "{\\color{Orange} \\frac{\\partial L}{\\partial \\mathbf{Z}}} &=\n",
    "{\\color{Lime} \\frac{\\partial L}{\\partial \\mathbf{\\Sigma}}}\n",
    "{\\color{Cyan} \\frac{\\partial \\mathbf{\\Sigma}}{\\partial \\mathbf{Z}}} \\\\\n",
    "&\\in \\mathbb{R}^{\n",
    "    {\\color{Lime} (m \\times n_{1})} \\times \n",
    "    {\\color{Cyan} (m \\times n_{1} \\times m \\times n_{1})}} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Orange} {m \\times n_{1}}}\n",
    "\\end{align*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "{\\color{Magenta} \\frac{\\partial L}{\\partial \\mathbf{b}}} &=\n",
    "{\\color{Orange} \\frac{\\partial L}{\\partial \\mathbf{Z}}}\n",
    "{\\color{Cyan} \\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}}} \\\\\n",
    "&\\in \\mathbb{R}^{\n",
    "    {\\color{Orange} (m \\times n_{1})} \\times \n",
    "    {\\color{Cyan} (m \\times n_{1} \\times n_{1})}} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} {n_{1}}}\n",
    "\\end{align*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "{\\color{Magenta} \\frac{\\partial L}{\\partial \\mathbf{W}}} &=\n",
    "{\\color{Orange} \\frac{\\partial L}{\\partial \\mathbf{Z}}}\n",
    "{\\color{Cyan} \\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}}} \\\\\n",
    "&\\in \\mathbb{R}^{\n",
    "    {\\color{Orange} (m \\times n_{1})} \\times \n",
    "    {\\color{Cyan} (m \\times n_{1} \\times n \\times n_{1})}} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} {n \\times n_{1}}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EinsumSoftmaxClassifier(SoftmaxClassifier):\n",
    "    def update(self, x: torch.Tensor, y_true: torch.Tensor,\n",
    "           y_pred: torch.Tensor, lr: float) -> None:\n",
    "        \"\"\"\n",
    "        Update the model parameters.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (n_samples, n_features).\n",
    "            y_true: Target tensor of shape (n_samples, n_classes).\n",
    "            y_pred: Predicted output tensor of shape (n_samples, n_classes).\n",
    "            lr: Learning rate. \n",
    "        \"\"\"\n",
    "        m, n_classes = y_true.shape\n",
    "        # cross entropy der\n",
    "        delta = -(y_true / y_pred) / m\n",
    "        # softmax der\n",
    "        diag_a = torch.diag_embed(y_pred)\n",
    "        outer_a = torch.einsum('ij,ik->ijk', y_pred, y_pred) \n",
    "        soft_der = torch.zeros(\n",
    "            (m, n_classes, m, n_classes), \n",
    "            dtype=y_pred.dtype, \n",
    "            device=device\n",
    "        )\n",
    "        idx = torch.arange(m, device=device)\n",
    "        soft_der[idx, :, idx, :] = diag_a - outer_a\n",
    "        delta = torch.einsum('pq,pqij->ij', delta, soft_der)\n",
    "        # weighted sum der\n",
    "        self.b -= lr * delta.sum(axis=0)\n",
    "        \n",
    "        identity = torch.eye(n_classes, device=device)\n",
    "        w_der = torch.kron(\n",
    "            x.unsqueeze(1).unsqueeze(3), \n",
    "            identity.unsqueeze(0).unsqueeze(2)\n",
    "        )\n",
    "        w_der = torch.einsum('pq,pqij->ij', delta, w_der)\n",
    "        self.w -= lr * w_der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2345, -0.1731, -0.0806],\n",
       "        [-0.2677,  0.1528, -0.3038],\n",
       "        [-0.0472, -0.1723, -0.0563],\n",
       "        [-0.0540,  0.3726, -0.3358],\n",
       "        [-0.1868, -0.4083, -0.3049]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einsum_model = EinsumSoftmaxClassifier(N, CLASSES)\n",
    "einsum_model.b.copy_(parameters[0])\n",
    "einsum_model.w.copy_(parameters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - CE: 1.3285 - CE_v: 1.4406 - acc_v: 0.2315\n",
      "epoch: 1 - CE: 1.2951 - CE_v: 1.4167 - acc_v: 0.2567\n",
      "epoch: 2 - CE: 1.2645 - CE_v: 1.3942 - acc_v: 0.281\n",
      "epoch: 3 - CE: 1.2363 - CE_v: 1.3731 - acc_v: 0.3007\n",
      "epoch: 4 - CE: 1.2103 - CE_v: 1.3533 - acc_v: 0.3178\n",
      "epoch: 5 - CE: 1.1863 - CE_v: 1.3347 - acc_v: 0.3336\n",
      "epoch: 6 - CE: 1.1641 - CE_v: 1.3171 - acc_v: 0.3467\n",
      "epoch: 7 - CE: 1.1436 - CE_v: 1.3005 - acc_v: 0.355\n",
      "epoch: 8 - CE: 1.1245 - CE_v: 1.2847 - acc_v: 0.3622\n",
      "epoch: 9 - CE: 1.1067 - CE_v: 1.2698 - acc_v: 0.3675\n",
      "epoch: 10 - CE: 1.0901 - CE_v: 1.2555 - acc_v: 0.3749\n",
      "epoch: 11 - CE: 1.0746 - CE_v: 1.242 - acc_v: 0.3799\n",
      "epoch: 12 - CE: 1.0601 - CE_v: 1.229 - acc_v: 0.3833\n",
      "epoch: 13 - CE: 1.0464 - CE_v: 1.2167 - acc_v: 0.3887\n",
      "epoch: 14 - CE: 1.0336 - CE_v: 1.2048 - acc_v: 0.392\n",
      "epoch: 15 - CE: 1.0214 - CE_v: 1.1935 - acc_v: 0.3972\n"
     ]
    }
   ],
   "source": [
    "einsum_model.fit(\n",
    "    x_train, y_train, \n",
    "    EPOCHS, LR, BATCH, \n",
    "    x_valid, y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4395034483097222e-16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    einsum_model.w.clone(),\n",
    "    torch_model.layer.weight.detach().T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1842590316850036e-16"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    einsum_model.b.clone(),\n",
    "    torch_model.layer.bias.detach()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
