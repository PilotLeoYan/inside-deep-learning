{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax function and its derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PilotLeoYan/inside-deep-learning/blob/main/2-classification/softmax-function-and-its-derivative.ipynb\">\n",
    "    <img src=\"../images/colab_logo.png\" width=\"32\">Open in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://nbviewer.org/github/PilotLeoYan/inside-deep-learning/blob/main/2-classification/softmax-function-and-its-derivative.ipynb\">\n",
    "    <img src=\"../images/jupyter_logo.png\" width=\"32\">Open in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to find the explanation of the derivative of the Softmax function \n",
    "for a single sample with $n$ features, \n",
    "but finding the explanation for multiple samples with $n$ features \n",
    "becomes difficult. \n",
    "Here you will find the derivative and its vector version to optimize its computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial \\mathbf{z}}\n",
    " \\Rightarrow \\cdots  \\Rightarrow\n",
    "\\frac{\\mathrm{d} \\mathbf{\\Sigma}}\n",
    "{\\mathrm{d} \\mathbf{Z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.13.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autograd import jacobian, numpy as np\n",
    "\n",
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use $\\color{Cyan}{\\text{autograd}}$ to make a comparison between our \n",
    "scratch implementation and the automatic differentiation implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute percentage error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import importlib.util\n",
    "from collections.abc import Callable\n",
    "\n",
    "def import_mape(module_path: str = '..') -> Callable:\n",
    "    \"\"\"\n",
    "    Tries to import the 'numpy_mape' function from a local project structure.\n",
    "    If it fails (ModuleNotFoundError), it assumes a cloud environment (like Colab),\n",
    "    downloads the module from GitHub, imports it, and returns the function.\n",
    "\n",
    "    Args:\n",
    "        module_path (str): The relative path to the project's root directory\n",
    "                           for the local search.\n",
    "\n",
    "    Returns:\n",
    "        The imported 'numpy_mape' function, or None if it fails.\n",
    "    \"\"\"\n",
    "    GITHUB_RAW_URL = 'https://raw.githubusercontent.com/PilotLeoYan/inside-deep-learning/main/tools/numpy_metrics.py'\n",
    "    MODULE_NAME = 'numpy_metrics'\n",
    "    LOCAL_FILE_NAME = f'{MODULE_NAME}.py'\n",
    "\n",
    "    try:\n",
    "        # Attempt 1: Standard import (if the package is installed or in PYTHONPATH)\n",
    "        from tools.numpy_metrics import np_mape\n",
    "        print(\"âœ… Module 'tools.numpy_metrics' successfully imported from the environment.\")\n",
    "        return np_mape\n",
    "\n",
    "    except ModuleNotFoundError:\n",
    "        # Attempt 2: Search in the specified local path (original behavior)\n",
    "        # This is useful for local development without installing the package.\n",
    "        project_path = os.path.abspath(os.path.join(module_path))\n",
    "        if project_path not in sys.path:\n",
    "            sys.path.insert(0, project_path)\n",
    "\n",
    "        try:\n",
    "            from tools.numpy_metrics import np_mape\n",
    "            print(\"âœ… Local module 'tools.numpy_metrics' imported after adjusting the path.\")\n",
    "            # Remove the added path to avoid side effects\n",
    "            sys.path.pop(0)\n",
    "            return np_mape\n",
    "        except ModuleNotFoundError:\n",
    "            # If both local attempts fail, proceed with the download\n",
    "            if project_path in sys.path:\n",
    "                sys.path.pop(0) # Clean up the path if it was added\n",
    "            print(f\"âš ï¸ Local module not found. Proceeding to download from GitHub...\")\n",
    "\n",
    "    # Download and Dynamic Loading Logic}\n",
    "    if not os.path.exists(LOCAL_FILE_NAME):\n",
    "        try:\n",
    "            print(f\"â¬‡ï¸ Downloading '{LOCAL_FILE_NAME}' from GitHub...\")\n",
    "            response = requests.get(GITHUB_RAW_URL)\n",
    "            response.raise_for_status()  # This will raise an error if the HTTP request failed\n",
    "            with open(LOCAL_FILE_NAME, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text)\n",
    "            print(\"ðŸ‘ Download complete.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ Error downloading the file: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Dynamically load the module using importlib\n",
    "    spec = importlib.util.spec_from_file_location(MODULE_NAME, LOCAL_FILE_NAME)\n",
    "    dynamic_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(dynamic_module)\n",
    "    \n",
    "    print(f\"âœ… Module '{MODULE_NAME}' successfully loaded from the downloaded file.\")\n",
    "    return dynamic_module.np_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Local module 'tools.numpy_metrics' imported after adjusting the path.\n"
     ]
    }
   ],
   "source": [
    "mape = import_mape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "M: int = 1000 # number of samples\n",
    "CLASSES: int = 5 # number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 5), dtype('float64'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.random.randint(-30, 31, (M, CLASSES)) / 2\n",
    "Z.shape, Z.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single sample with $n_{1}$ features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{z} \\in \\mathbb{R}^{1 \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent $\\text{softmax}$ as $\\sigma$. Then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(\\mathbf{z})_{q} = \n",
    "\\frac{\\exp (z_{q})}\n",
    "{\\sum_{k=1}^{n_{1}} \\exp (z_{k})}\n",
    "\\in \\mathbb{R}^{+}\n",
    "$$\n",
    "where $n_{1}$ is the number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(\\mathbf{z}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z})_1 & \\sigma(\\mathbf{z})_2 & \\cdots & \\sigma(\\mathbf{z})_{n_{1}}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{1 \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.49114452e-03, 9.91822304e-01, 9.16190861e-09, 3.69617774e-06,\n",
       "         6.68284612e-03]]),\n",
       " (1, 5))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "soft_out_1 = softmax(Z[:1])\n",
    "soft_out_1, soft_out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.49114452e-03, 9.91822304e-01, 9.16190861e-09, 3.69617774e-06,\n",
       "         6.68284612e-03]]),\n",
       " (1, 5))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our softmax function\n",
    "def my_softmax_1(z: np.ndarray) -> np.ndarray:\n",
    "    exp = np.exp(z)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "my_soft_out_1 = my_softmax_1(Z[:1])\n",
    "my_soft_out_1, my_soft_out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3593059012341366e-14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_soft_out_1,\n",
    "    soft_out_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a input with $m$ samples and $n_{1}$ features \n",
    "$\\mathbf{Z} \\in \\mathbb{R}^{m \\times n_{1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Z} = \\begin{bmatrix}\n",
    "    \\mathbf{z}_{1,:} \\\\\n",
    "    \\mathbf{z}_{2,:} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{z}_{m,:}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, the softmax over each example is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\Sigma(Z)} = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{z}_{m,:}) \\\\\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{m \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We use $\\mathbf{\\Sigma(Z)}$ to denote that is a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_out_2 = softmax(Z, axis=1)\n",
    "soft_out_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_softmax_2(z: np.ndarray) -> np.ndarray:\n",
    "    exp = np.exp(z)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "my_soft_out_2 = my_softmax_2(Z)\n",
    "my_soft_out_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1147726988119203e-14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_soft_out_2,\n",
    "    soft_out_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivation of a softmax with respect to a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_selected: int = 3 # select a feature to derive\n",
    "\n",
    "def my_softmax_0(z: np.ndarray, j_feature: int) -> float:\n",
    "    exp = np.exp(z)\n",
    "    return exp[0, j_feature] / np.sum(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.51153519e-09, -3.66595152e-06, -3.38640426e-14,\n",
       "         3.69616407e-06, -2.47009870e-08]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_0 = jacobian(my_softmax_0, 0)(\n",
    "    Z[:1],\n",
    "    n_selected\n",
    ")\n",
    "grad_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial \\mathbf{z}}\n",
    "\\in \\mathbb{R}^{1 \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because $\\mathbf{z} \\in \\mathbb{R}^{1 \\times n_{1}}$ \n",
    "and $\\sigma(\\mathbf{z})_{q} \\in \\mathbb{R}$. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then its jacobian is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\sigma_{q}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma_{q}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma_{q}}{\\partial z_{n_{1}}}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are two different types of the derivatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial z_{r=q}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial z_{r\\neq q}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial z_{r=q}} = \n",
    "\\sigma(\\mathbf{z})_{q} (1 - \\sigma(\\mathbf{z})_q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial z_{r\\neq q}} =\n",
    "-\\sigma(\\mathbf{z})_{q} \\sigma(\\mathbf{z})_{r}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "    -\\sigma(\\mathbf{z})_{q} \\sigma(\\mathbf{z})_{1} &\n",
    "    \\cdots &\n",
    "    \\sigma(\\mathbf{z})_{q}(1 - \\sigma(\\mathbf{z})_j) &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{q} \\sigma(\\mathbf{z})_{n_{1}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or as vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial \\mathbf{z}} =\n",
    "\\sigma(\\mathbf{z})_{q}\n",
    "\\begin{bmatrix}\n",
    "    -\\sigma(\\mathbf{z})_{1} &\n",
    "    \\cdots &\n",
    "    1 - \\sigma(\\mathbf{z})_{q} &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{n_{1}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.51153519e-09, -3.66595152e-06, -3.38640426e-14,\n",
       "         3.69616407e-06, -2.47009870e-08]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_softmax_0(z: np.ndarray, j_feature) -> np.ndarray:\n",
    "    soft = my_softmax_1(z)\n",
    "    soft_j = soft[0, j_feature]\n",
    "    soft *= -1\n",
    "    soft[0, j_feature] += 1\n",
    "    return soft_j * soft\n",
    "\n",
    "my_grad_0 = my_der_softmax_0(Z[:1], n_selected)\n",
    "my_grad_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3284483319208167e-14"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_grad_0,\n",
    "    grad_0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivative of a softmax with respect to a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 1, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_1 = jacobian(my_softmax_1, 0)(Z[:1])\n",
    "grad_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{z}} \\in\n",
    "\\mathbb{R}^{(1 \\times n_{1}) \\times (1 \\times n_{1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to simplify the derivative, we will ignore the axes of 1 for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{z}} \\in \n",
    "\\mathbb{R}^{n_{1} \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this derivative is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{z}} &=\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\sigma_{1}}{\\partial \\mathbf{z}} \\\\\n",
    "    \\frac{\\partial \\sigma_{2}}{\\partial \\mathbf{z}} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial \\sigma_{n_{1}}}{\\partial \\mathbf{z}}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    \\frac{\\partial \\sigma_{1}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma_{1}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma_{1}}{\\partial z_{n_{1}}} \\\\\n",
    "    \\frac{\\partial \\sigma_{2}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma_{2}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma_{2}}{\\partial z_{n_{1}}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial \\sigma_{n_{1}}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma_{n_{1}}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma_{n_{1}}}{\\partial z_{n_{1}}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma_{q}}{\\partial z_{r=q}} &= \n",
    "\\sigma(\\mathbf{z})_{q} (1 - \\sigma(\\mathbf{z})_{q}) \\\\\n",
    "\\frac{\\partial \\sigma_{q}}{\\partial z_{r \\neq q}} &=\n",
    "-\\sigma(\\mathbf{z})_{q} \\sigma(\\mathbf{z})_{r}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z})_{1} (1 - \\sigma(\\mathbf{z})_1) &\n",
    "    -\\sigma(\\mathbf{z})_{1} \\sigma(\\mathbf{z})_{2} &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{1} \\sigma(\\mathbf{z})_{n_{1}} \\\\\n",
    "    -\\sigma(\\mathbf{z})_{2} \\sigma(\\mathbf{z})_{1} &\n",
    "    \\sigma(\\mathbf{z})_{2} (1 - \\sigma(\\mathbf{z})_2) &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{2} \\sigma(\\mathbf{z})_{n_{1}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -\\sigma(\\mathbf{z})_{n_{1}} \\sigma(\\mathbf{z})_{1} &\n",
    "    -\\sigma(\\mathbf{z})_{n_{1}} \\sigma(\\mathbf{z})_{2} &\n",
    "    \\cdots &\n",
    "    \\sigma(\\mathbf{z})_{n_{1}} (1 - \\sigma(\\mathbf{z})_{n_{1}})\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or as vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{z}} =\n",
    "\\text{diag}(\\sigma(\\mathbf{z})) - \\sigma(\\mathbf{z}) \\sigma(\\mathbf{z})^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_softmax_1(z: np.ndarray) -> np.ndarray:\n",
    "    soft = my_softmax_1(z).squeeze() # is necesary for numpy to work\n",
    "    return np.diag(soft) - np.outer(soft, soft)\n",
    "\n",
    "my_grad_1 = my_der_softmax_1(Z[:1])\n",
    "my_grad_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.3673060562124684e-14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_grad_1,\n",
    "    grad_1.squeeze()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivation of multiple softmaxes with respect to multiple samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Z} \\in \\mathbb{R}^{m \\times n_{1}}\n",
    "$$\n",
    "where $m$ is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then softmax function is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\Sigma(Z)} = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{z}_{m,:})\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{z}_{i,:} = \\begin{bmatrix}\n",
    "    z_{i1} & z_{i2} & \\cdots & z_{in_{1}}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{1 \\times n_{1}}\n",
    "$$\n",
    "for all $i = 1, \\ldots, m$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(\\mathbf{z}_{i,:}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z}_{i,:})_{1} & \n",
    "    \\sigma(\\mathbf{z}_{i,:})_{2} & \n",
    "    \\cdots & \n",
    "    \\sigma(\\mathbf{z}_{i,:})_{n_{1}}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{1 \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5, 1000, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_2 = jacobian(my_softmax_2, 0)(Z)\n",
    "grad_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\mathrm{d} {\\color{Cyan} {\\mathbf{\\Sigma}}}}\n",
    "{\\mathrm{d} {\\color{Orange} {\\mathbf{Z}}}} \\in \n",
    "\\mathbb{R}^{{\\color{Cyan} {(m \\times n_{1})}} \n",
    "\\times {\\color{Orange} {(m \\times n_{1})}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial {\\color{Cyan} {\\mathbf{\\Sigma}_{pq}}}}\n",
    "{\\partial {\\color{Orange} {\\mathbf{Z}_{ij}}}} \\in \n",
    "\\mathbb{R}^{{\\color{Cyan} {(1 \\times 1)} }\n",
    "\\times {\\color{Orange} {(1 \\times 1)}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore, the last derivative is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}_{pq}}\n",
    "{\\partial \\mathbf{Z}_{ij}} =\n",
    "\\begin{cases}\n",
    "    \\sigma(\\mathbf{Z})_{pq}(1 - \\sigma(\\mathbf{Z})_{ij}) & \\text{if } p=i, q=j \\\\ \n",
    "    -\\sigma(\\mathbf{Z})_{pq} \\sigma(\\mathbf{Z})_{ij} & \\text{if } p=i, q\\neq j \\\\\n",
    "    0 & \\text{if } p\\neq i\n",
    "\\end{cases}\n",
    "$$\n",
    "for all $p, i = 1, \\ldots, m$ and $q, j = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the first 2 cases looks similar to \n",
    "$\\frac{\\partial \\sigma_{q}}{\\partial \\mathbf{z}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5, 1000, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_softmax_low_2(z: np.ndarray) -> np.ndarray:\n",
    "    m, classes = z.shape\n",
    "    soft = my_softmax_2(z)\n",
    "\n",
    "    der = np.zeros((m, classes, m, classes), dtype=soft.dtype)\n",
    "\n",
    "    for i in range(m):\n",
    "        for q in range(classes):\n",
    "            for j in range(classes):\n",
    "                if q == j:\n",
    "                    der[i, q, i, j] = soft[i, q] * (1 - soft[i, q])\n",
    "                else:\n",
    "                    der[i, q, i, j] = -soft[i, q] * soft[i, j]\n",
    "    return der\n",
    "\n",
    "my_grad_low_2 = my_der_softmax_low_2(Z)\n",
    "my_grad_low_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.475167415552228e-13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_grad_low_2,\n",
    "    grad_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is too slow, its efficiency is $\\Theta(mn_{1}^2)$, \n",
    "but we can observe some similarities between this derivative and a previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}_{p,:}}\n",
    "{\\partial \\mathbf{Z}_{p,:}}\n",
    "\\approx \\frac{\\partial \\sigma}\n",
    "{\\partial \\mathbf{z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial {\\color{Cyan} {\\mathbf{\\Sigma}_{p,:}}}}\n",
    "{\\partial {\\color{Orange} {\\mathbf{Z}_{i,:}}}} \\in \n",
    "\\mathbb{R}^{{\\color{Cyan} {(1 \\times n_{1})}} \n",
    "\\times {\\color{Orange} {(1 \\times n_{1})}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: yes, we need 1's axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have 2 cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}_{p,:}}\n",
    "{\\partial \\mathbf{Z}_{i=p,:}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}_{p,:}}\n",
    "{\\partial \\mathbf{Z}_{i\\neq p,:}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}_{p,:}}\n",
    "{\\partial \\mathbf{Z}_{i=p,:}} = \n",
    "\\text{diag}(\\sigma(\\mathbf{Z}_{p,:})) \n",
    "- \\sigma(\\mathbf{Z}_{p,:}) \\sigma(\\mathbf{Z}_{p,:})^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}_{p,:}}\n",
    "{\\partial \\mathbf{Z}_{i \\neq p,:}} = \\mathbf{0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5, 1000, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_softmax_2(z: np.ndarray) -> np.ndarray:\n",
    "    m, classes = z.shape\n",
    "    der = np.zeros((m, classes, m, classes), dtype=z.dtype)\n",
    "\n",
    "    for i in range(m):\n",
    "        der[i, :, i, :] = my_der_softmax_1(z[np.newaxis, i])\n",
    "    return der\n",
    "\n",
    "my_grad_2 = my_der_softmax_2(Z)\n",
    "my_grad_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.602978798473835e-13"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_grad_2,\n",
    "    grad_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient using loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can often use properties of the loss function to optimize our gradients. <br>\n",
    "For any loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L: \\mathbb{R}^{m \\times n_{1}} \\rightarrow\n",
    "\\mathbb{R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can compute the derivative using the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{pq}} =\n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\sigma_{ij}}\n",
    "\\frac{\\partial \\sigma_{ij}}{\\partial z_{pq}}\n",
    "$$\n",
    "for all $p = 1, \\ldots, m$ and $q = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: We are going to focus on computing \n",
    "$\\frac{\\partial \\sigma_{ij}}{\\partial z_{pq}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{ij}}{\\partial z_{pq}} = \n",
    "\\begin{cases}\n",
    "\\sigma(\\mathbf{z}_{p,:})_{q}(1 - \\sigma(\\mathbf{z}_{p,:})_{q}) & \\text{if } i=p, j=q \\\\\n",
    "-\\sigma(\\mathbf{z}_{p,:})_{q} \\sigma(\\mathbf{z}_{i,:})_{j} & \\text{if } i=p, j \\neq q \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial z_{pq}} =&\n",
    "\\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\sigma_{pj}}\n",
    "\\frac{\\partial \\sigma_{pj}}{\\partial z_{pq}} \\\\\n",
    "=& \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\sigma_{pj}}\n",
    "\\begin{cases}\n",
    "\\sigma(z_{pq})(1 - \\sigma(z_{pq})) & \\text{if } j=q \\\\\n",
    "-\\sigma(z_{pq}) \\sigma(z_{pj}) & \\text{if } j \\neq q\n",
    "\\end{cases} \\\\\n",
    "=& \\sigma(\\mathbf{z}_{p,:})_{q} \\left(\n",
    "    \\frac{\\partial L}{\\partial \\sigma_{pq}}\n",
    "    - \\sum_{j=1}^{n_{1}}\n",
    "    \\frac{\\partial L}{\\partial \\sigma_{pj}}\n",
    "    \\sigma(\\mathbf{z}_{p,:})_{j}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} = \n",
    "\\mathbf{\\Sigma} \\odot \\left(\n",
    "    \\frac{\\partial L}{\\partial \\mathbf{\\Sigma}}\n",
    "    - \\left(\n",
    "        \\frac{\\partial L}{\\partial \\mathbf{\\Sigma}}\n",
    "        \\odot \\mathbf{\\Sigma}\n",
    "    \\right) \\mathbf{1}\n",
    "\\right)\n",
    "$$\n",
    "where $\\mathbf{1} \\in \\mathbb{R}^{n_{1} \\times n_{1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(a: np.ndarray) -> np.ndarray:\n",
    "    return np.sum(a ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_soft_grad = jacobian(lambda z: loss_function(my_softmax_2(z)))(Z)\n",
    "loss_soft_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_grad = jacobian(loss_function)(soft_out_2)\n",
    "loss_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_loss_soft_der(loss_grad: np.ndarray, soft: np.ndarray) -> np.ndarray:\n",
    "    return soft * (loss_grad - np.sum(loss_grad * soft, axis=1, keepdims=True))\n",
    "\n",
    "my_loss_soft_grad = my_loss_soft_der(loss_grad, soft_out_2)\n",
    "my_loss_soft_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.983099786639593e-09"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_loss_soft_grad,\n",
    "    loss_soft_grad\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underflow and Overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_flow = Z[:5] * 100\n",
    "z_flow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lords\\Documents\\inside-deep-learning\\.venv7\\Lib\\site-packages\\autograd\\tracer.py:54: RuntimeWarning: overflow encountered in exp\n",
      "  return f_raw(*args, **kwargs)\n",
      "C:\\Users\\lords\\AppData\\Local\\Temp\\ipykernel_9392\\706199401.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  return exp / np.sum(exp, axis=1, keepdims=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+000,             nan, 0.00000000e+000,\n",
       "        0.00000000e+000,             nan],\n",
       "       [            nan,             nan, 0.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "                    nan, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        1.00000000e+000, 3.69388307e-196],\n",
       "       [0.00000000e+000, 1.92874985e-022, 2.65039655e-261,\n",
       "        0.00000000e+000, 1.00000000e+000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_softmax_2(z_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If $c$ is very negative, then $\\exp(c)$ will underflow. This means the denominator of the softmax will become $0$, so the final result is undefined. When $c$ is very large and positive, $\\exp(c)$ will overflow, again resulting in the expression as a whole being undefined.\n",
    "\n",
    "**Reference:** Goodfellow, I. J., Bengio, Y., \\& Courville, A. (2016). *Deep Learning*. MIT Press, p. 81."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this, we can use a subtract.\n",
    "$$\n",
    "\\sigma(\\mathbf{z}) = \\sigma(\\mathbf{z} - \\max_{i} \\mathbf{z}_{i})\n",
    "$$\n",
    "this analytically does not change, because the probability of $z$ is equal to the probability of $z - \\max z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.11195195e-283, 1.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 7.12457641e-218],\n",
       "       [1.00000000e+000, 1.92874985e-022, 0.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        1.00000000e+000, 9.85967654e-305],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        1.00000000e+000, 3.69388307e-196],\n",
       "       [0.00000000e+000, 1.92874985e-022, 2.65039655e-261,\n",
       "        0.00000000e+000, 1.00000000e+000]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_softmax_2(z_flow - np.max(z_flow, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when $r = q$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma_{q}}{\\partial z_{r=q}} &=\n",
    "\\frac{\\exp(z_{q})(\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})) - \\exp(z_{q})^{2}}\n",
    "{(\\sum_{k=1}^{{n_{1}}}\\exp(z_{k}))^{2}} \\\\\n",
    "&= \\frac{\\exp(z_{q})(\\sum_{k=1}^{{n_{1}}}\\exp(z_{k}) - \\exp(z_{q}))}\n",
    "{(\\sum_{k=1}^{{n_{1}}}\\exp(z_{k}))^{2}} \\\\\n",
    "&= \\frac{\\exp(z_{q})}{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})}\n",
    "\\left( \n",
    "    \\frac{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k}) - \\exp(z_{q})}\n",
    "    {\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})}\n",
    "\\right) \\\\\n",
    "&= \\sigma(\\mathbf{z})_{q} \\left(\n",
    "    \\frac{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})}{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})} -\n",
    "    \\frac{\\exp(z_{q})}{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})}\n",
    "\\right) \\\\\n",
    "&= \\sigma(\\mathbf{z})_{q} (1 - \\sigma(\\mathbf{z})_{q})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when $r \\neq q$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma_{q}}{\\partial z_{r \\neq q}} &=\n",
    "- \\frac{\\exp(z_{q}) \\exp(z_{r})}\n",
    "{(\\sum_{k=1}^{{n_{1}}}\\exp(z_{k}))^{2}} \\\\\n",
    "&= - \\frac{\\exp(z_{q})}{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})} \\left(\n",
    "    \\frac{\\exp(z_{r})}{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})}\n",
    "\\right) \\\\\n",
    "&= - \\sigma(\\mathbf{z})_{q} \\sigma(\\mathbf{z})_{r}\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv7 (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
