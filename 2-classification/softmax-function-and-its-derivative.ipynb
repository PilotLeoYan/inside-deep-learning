{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Softmax function and its derivative</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to find the explanation of the derivative of the Softmax function \n",
    "for a single sample with $n$ features, \n",
    "but finding the explanation for multiple samples with $n$ characteristics \n",
    "becomes difficult. \n",
    "Here you will find the derivative and its vector version to optimize its computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial \\mathbf{z}}\n",
    " \\Rightarrow \\cdots  \\Rightarrow\n",
    "\\frac{\\mathrm{d} \\mathbf{\\Sigma}}\n",
    "{\\mathrm{d} \\mathbf{Z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import jacobian, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use $\\color{Cyan}{\\text{autograd}}$ to make a comparison between our \n",
    "scratch implementation and the automatic differentiation implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute percentage error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tools.numpy_metrics import np_mape as mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "M: int = 100 # number of samples\n",
    "CLASSES: int = 5 # number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.random.randint(-30, 31, (M, CLASSES)) / 2\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single sample with $n_{1}$ features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{z} \\in \\mathbb{R}^{1 \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent $\\text{softmax}$ as $\\sigma$. Then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(\\mathbf{z})_{q} = \n",
    "\\frac{\\exp (z_{q})}\n",
    "{\\sum_{k=1}^{n_{1}} \\exp (z_{k})}\n",
    "\\in \\mathbb{R}^{+}\n",
    "$$\n",
    "where $n_{1}$ is the number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(\\mathbf{z}) = \\begin{bmatrix*}\n",
    "    \\sigma(\\mathbf{z})_1 & \\sigma(\\mathbf{z})_2 & \\cdots & \\sigma(\\mathbf{z})_{n_{1}}\n",
    "\\end{bmatrix*}\n",
    "\\in \\mathbb{R}^{1 \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.50087733e-03, 2.28583314e-11, 4.59121859e-10, 9.98296001e-01,\n",
       "         2.03121659e-04]]),\n",
       " (1, 5))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "soft_out_1 = softmax(Z[:1])\n",
    "soft_out_1, soft_out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.50087733e-03, 2.28583314e-11, 4.59121859e-10, 9.98296001e-01,\n",
       "         2.03121659e-04]]),\n",
       " (1, 5))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our softmax function\n",
    "def my_softmax_1(z: np.ndarray) -> np.ndarray:\n",
    "    exp = np.exp(z)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "my_soft_out_1 = my_softmax_1(Z[:1])\n",
    "my_soft_out_1, my_soft_out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.4959797233718685e-17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_soft_out_1,\n",
    "    soft_out_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a input with $m$ samples and $n_{1}$ features \n",
    "$\\mathbf{Z} \\in \\mathbb{R}^{m \\times n_{1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Z} = \\begin{bmatrix}\n",
    "    \\mathbf{z}_{1,:} \\\\\n",
    "    \\mathbf{z}_{2,:} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{z}_{m,:}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, the softmax over each example is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\Sigma(Z)} = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{z}_{m,:}) \\\\\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{m \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We use $\\mathbf{\\Sigma(Z)}$ to denote that is a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_out_2 = softmax(Z, axis=1)\n",
    "soft_out_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_softmax_2(z: np.ndarray) -> np.ndarray:\n",
    "    exp = np.exp(z)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "my_soft_out_2 = my_softmax_2(Z)\n",
    "my_soft_out_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2063126821525917e-16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_soft_out_2,\n",
    "    soft_out_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivation of a softmax with respect to a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_selected: int = 3 # select a feature to derive\n",
    "\n",
    "def my_softmax_0(z: np.ndarray, j_feature: int) -> float:\n",
    "    exp = np.exp(z)\n",
    "    return exp[0, j_feature] / np.sum(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.49831984e-03, -2.28193808e-11, -4.58339516e-10,\n",
       "         1.70109586e-03, -2.02775540e-04]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_0 = jacobian(my_softmax_0, 0)(\n",
    "    Z[:1],\n",
    "    n_selected\n",
    ")\n",
    "grad_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial \\mathbf{z}}\n",
    "\\in \\mathbb{R}^{1 \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because $\\mathbf{z} \\in \\mathbb{R}^{1 \\times n_{1}}$ \n",
    "and $\\sigma(\\mathbf{z})_{q} \\in \\mathbb{R}$. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then its jacobian is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\sigma_{q}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma_{q}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma_{q}}{\\partial z_{n_{1}}}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are two different types of the derivatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial z_{r=q}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial z_{r\\neq q}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial z_{r=q}} = \n",
    "\\sigma(\\mathbf{z})_{q} (1 - \\sigma(\\mathbf{z})_q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial z_{r\\neq q}} =\n",
    "-\\sigma(\\mathbf{z})_{q} \\sigma(\\mathbf{z})_{r}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "    -\\sigma(\\mathbf{z})_{q} \\sigma(\\mathbf{z})_{1} &\n",
    "    \\cdots &\n",
    "    \\sigma(\\mathbf{z})_{q}(1 - \\sigma(\\mathbf{z})_j) &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{q} \\sigma(\\mathbf{z})_{n_{1}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or as vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{q}}\n",
    "{\\partial \\mathbf{z}} =\n",
    "\\sigma(\\mathbf{z})_{q} \\odot\n",
    "\\begin{bmatrix}\n",
    "    -\\sigma(\\mathbf{z})_{1} &\n",
    "    \\cdots &\n",
    "    1 - \\sigma(\\mathbf{z})_{q} &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{n_{1}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.49831984e-03, -2.28193808e-11, -4.58339516e-10,\n",
       "         1.70109586e-03, -2.02775540e-04]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_softmax_0(z: np.ndarray, j_feature) -> np.ndarray:\n",
    "    soft = my_softmax_1(z)\n",
    "    soft_j = soft[0, j_feature]\n",
    "    soft *= -1\n",
    "    soft[0, j_feature] += 1\n",
    "    return soft_j * soft\n",
    "\n",
    "my_grad_0 = my_der_softmax_0(Z[:1], n_selected)\n",
    "my_grad_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.174488516035218e-15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_grad_0,\n",
    "    grad_0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivative of a softmax with respect to a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 1, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_1 = jacobian(my_softmax_1, 0)(Z[:1])\n",
    "grad_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{z}} \\in\n",
    "\\mathbb{R}^{(1 \\times n_{1}) \\times (1 \\times n_{1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to simplify the derivative, we will ignore the axes of 1 for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{z}} \\in \n",
    "\\mathbb{R}^{n_{1} \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this derivative is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{z}} &=\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\sigma_{1}}{\\partial \\mathbf{z}} \\\\\n",
    "    \\frac{\\partial \\sigma_{2}}{\\partial \\mathbf{z}} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial \\sigma_{n_{1}}}{\\partial \\mathbf{z}}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    \\frac{\\partial \\sigma_{1}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma_{1}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma_{1}}{\\partial z_{n_{1}}} \\\\\n",
    "    \\frac{\\partial \\sigma_{2}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma_{2}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma_{2}}{\\partial z_{n_{1}}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial \\sigma_{n_{1}}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma_{n_{1}}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma_{n_{1}}}{\\partial z_{n_{1}}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma_{q}}{\\partial z_{r=q}} &= \n",
    "\\sigma(\\mathbf{z})_{q} (1 - \\sigma(\\mathbf{z})_{q}) \\\\\n",
    "\\frac{\\partial \\sigma_{q}}{\\partial z_{r \\neq q}} &=\n",
    "-\\sigma(\\mathbf{z})_{q} \\sigma(\\mathbf{z})_{r}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z})_{1} (1 - \\sigma(\\mathbf{z})_1) &\n",
    "    -\\sigma(\\mathbf{z})_{1} \\sigma(\\mathbf{z})_{2} &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{1} \\sigma(\\mathbf{z})_{n_{1}} \\\\\n",
    "    -\\sigma(\\mathbf{z})_{2} \\sigma(\\mathbf{z})_{1} &\n",
    "    \\sigma(\\mathbf{z})_{2} (1 - \\sigma(\\mathbf{z})_2) &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{2} \\sigma(\\mathbf{z})_{n_{1}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -\\sigma(\\mathbf{z})_{n_{1}} \\sigma(\\mathbf{z})_{1} &\n",
    "    -\\sigma(\\mathbf{z})_{n_{1}} \\sigma(\\mathbf{z})_{2} &\n",
    "    \\cdots &\n",
    "    \\sigma(\\mathbf{z})_{n_{1}} (1 - \\sigma(\\mathbf{z})_{n_{1}})\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or as vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{z}} =\n",
    "\\text{diag}(\\sigma(\\mathbf{z})) - \\sigma(\\mathbf{z}) \\sigma(\\mathbf{z})^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_softmax_1(z: np.ndarray) -> np.ndarray:\n",
    "    soft = my_softmax_1(z).squeeze() # is necesary for numpy to work\n",
    "    return np.diag(soft) - np.outer(soft, soft)\n",
    "\n",
    "my_grad_1 = my_der_softmax_1(Z[:1])\n",
    "my_grad_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1082264893497129e-15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_grad_1,\n",
    "    grad_1.squeeze()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivation of multiple softmaxes with respect to multiple samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Z} \\in \\mathbb{R}^{m \\times n_{1}}\n",
    "$$\n",
    "where $m$ is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then softmax function is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\Sigma(Z)} = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{z}_{m,:})\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{z}_{i,:} = \\begin{bmatrix}\n",
    "    z_{i1} & z_{i2} & \\cdots & z_{in_{1}}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{1 \\times n_{1}}\n",
    "$$\n",
    "for all $i = 1, \\ldots, m$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(\\mathbf{z}_{i,:}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z}_{i,:})_{1} & \n",
    "    \\sigma(\\mathbf{z}_{i,:})_{2} & \n",
    "    \\cdots & \n",
    "    \\sigma(\\mathbf{z}_{i,:})_{n_{1}}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{1 \\times n_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5, 100, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_2 = jacobian(my_softmax_2, 0)(Z)\n",
    "grad_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\mathrm{d} {\\color{Cyan} \\mathbf{\\Sigma}}}\n",
    "{\\mathrm{d} {\\color{Orange}\\mathbf{Z}}} \\in \n",
    "\\mathbb{R}^\n",
    "{{\\color{cyan}(m \\times n_{1})} \\times {\\color{Orange}(m \\times n_{1})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial {\\color{cyan} \\mathbf{\\Sigma}_{pq}}}\n",
    "{\\partial {\\color{Orange}\\mathbf{Z}_{ij}}} \\in \n",
    "\\mathbb{R}^\n",
    "{{\\color{cyan}(1 \\times 1)} \\times {\\color{Orange}(1 \\times 1)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore, the last derivative is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}_{pq}}\n",
    "{\\partial \\mathbf{Z}_{ij}} =\n",
    "\\begin{cases}\n",
    "    \\sigma(\\mathbf{Z})_{pq}(1 - \\sigma(\\mathbf{Z})_{ij}) & \\text{if } p=i, q=j \\\\ \n",
    "    -\\sigma(\\mathbf{Z})_{pq} \\sigma(\\mathbf{Z})_{ij} & \\text{if } p=i, q\\neq j \\\\\n",
    "    0 & \\text{if } p\\neq i\n",
    "\\end{cases}\n",
    "$$\n",
    "for all $p, i = 1, \\ldots, m$ and $q, j = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the first 2 cases looks similar to \n",
    "$\\frac{\\partial \\sigma_{q}}{\\partial \\mathbf{z}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5, 100, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_softmax_low_2(z: np.ndarray) -> np.ndarray:\n",
    "    m, classes = z.shape\n",
    "    soft = my_softmax_2(z)\n",
    "\n",
    "    der = np.zeros((m, classes, m, classes), dtype=soft.dtype)\n",
    "\n",
    "    for i in range(m):\n",
    "        for q in range(classes):\n",
    "            for j in range(classes):\n",
    "                if q == j:\n",
    "                    der[i, q, i, j] = soft[i, q] * (1 - soft[i, q])\n",
    "                else:\n",
    "                    der[i, q, i, j] = -soft[i, q] * soft[i, j]\n",
    "    return der\n",
    "\n",
    "my_grad_low_2 = my_der_softmax_low_2(Z)\n",
    "my_grad_low_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0391532847369417e-13"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_grad_low_2,\n",
    "    grad_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is too slow, its efficiency is $\\Theta(mn_{1}^2)$, \n",
    "but we can observe some similarities between this derivative and a previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}_{p,:}}\n",
    "{\\partial \\mathbf{Z}_{p,:}}\n",
    "\\approx \\frac{\\partial \\sigma}\n",
    "{\\partial \\mathbf{z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial {\\color{cyan} \\mathbf{\\Sigma}_{p,:}}}\n",
    "{\\partial {\\color{Orange}\\mathbf{Z}_{i,:}}} \\in \n",
    "\\mathbb{R}^{{\\color{cyan}(1 \\times n_{1})} \n",
    "\\times {\\color{Orange}(1 \\times n_{1})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: yes, we need 1's axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have 2 cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}_{p,:}}\n",
    "{\\partial \\mathbf{Z}_{i=p,:}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}_{p,:}}\n",
    "{\\partial \\mathbf{Z}_{i\\neq p,:}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}_{p,:}}\n",
    "{\\partial \\mathbf{Z}_{i=p,:}} = \n",
    "\\text{diag}(\\sigma(\\mathbf{Z}_{p,:})) \n",
    "- \\sigma(\\mathbf{Z}_{p,:}) \\sigma(\\mathbf{Z}_{p,:})^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf{\\Sigma}_{p,:}}\n",
    "{\\partial \\mathbf{Z}_{i \\neq p,:}} = \\mathbf{0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5, 100, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_softmax_2(z: np.ndarray) -> np.ndarray:\n",
    "    m, classes = z.shape\n",
    "    der = np.zeros((m, classes, m, classes), dtype=z.dtype)\n",
    "\n",
    "    for i in range(m):\n",
    "        der[i, :, i, :] = my_der_softmax_1(z[np.newaxis, i])\n",
    "    return der\n",
    "\n",
    "my_grad_2 = my_der_softmax_2(Z)\n",
    "my_grad_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2590338526445292e-13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_grad_2,\n",
    "    grad_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient using loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can often use properties of the loss function to optimize our gradients. <br>\n",
    "For any loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L: \\mathbb{R}^{m \\times n_{1}} \\rightarrow\n",
    "\\mathbb{R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can compute the derivative using the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial z_{pq}} =\n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\sigma_{ij}}\n",
    "\\frac{\\partial \\sigma_{ij}}{\\partial z_{pq}}\n",
    "$$\n",
    "for all $p = 1, \\ldots, m$ and $q = 1, \\ldots, n_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: We are going to focus on computing \n",
    "$\\frac{\\partial \\sigma_{ij}}{\\partial z_{pq}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma_{ij}}{\\partial z_{pq}} = \n",
    "\\begin{cases}\n",
    "\\sigma(\\mathbf{z}_{p,:})_{q}(1 - \\sigma(\\mathbf{z}_{p,:})_{q}) & \\text{if } i=p, j=q \\\\\n",
    "-\\sigma(\\mathbf{z}_{p,:})_{q} \\sigma(\\mathbf{z}_{i,:})_{j} & \\text{if } i=p, j \\neq q \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial z_{pq}} =&\n",
    "\\sum_{j=1}^{n_{1}}\n",
    "\\frac{\\partial L}{\\partial \\sigma_{pj}}\n",
    "\\frac{\\partial \\sigma_{pj}}{\\partial z_{pq}} \\\\\n",
    "=& \\sigma(\\mathbf{z}_{p,:})_{q} \\left(\n",
    "    \\frac{\\partial L}{\\partial \\sigma_{pq}}\n",
    "    - \\sum_{j=1}^{n_{1}}\n",
    "    \\frac{\\partial L}{\\partial \\sigma_{pj}}\n",
    "    \\sigma(\\mathbf{z}_{p,:})_{j}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} = \n",
    "\\mathbf{\\Sigma} \\odot \\left(\n",
    "    \\frac{\\partial L}{\\partial \\mathbf{\\Sigma}}\n",
    "    - \\left(\n",
    "        \\frac{\\partial L}{\\partial \\mathbf{\\Sigma}}\n",
    "        \\odot \\mathbf{\\Sigma}\n",
    "    \\right) \\mathbf{1}\n",
    "\\right)\n",
    "$$\n",
    "where $\\mathbf{1} \\in \\mathbb{R}^{n_{1} \\times n_{1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(a: np.ndarray) -> np.ndarray:\n",
    "    return np.sum(a ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_soft_grad = jacobian(lambda z: loss_function(my_softmax_2(z)))(Z)\n",
    "loss_soft_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_grad = jacobian(loss_function)(soft_out_2)\n",
    "loss_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_loss_soft_der(loss_grad: np.ndarray, soft: np.ndarray) -> np.ndarray:\n",
    "    return soft * (loss_grad - np.sum(loss_grad * soft, axis=1, keepdims=True))\n",
    "\n",
    "my_loss_soft_grad = my_loss_soft_der(loss_grad, soft_out_2)\n",
    "my_loss_soft_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.49319861137162e-10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(\n",
    "    my_loss_soft_grad,\n",
    "    loss_soft_grad\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underflow and Overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_flow = Z[:5] * 100\n",
    "z_flow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lords\\Desktop\\inside-deep-learning\\venv\\Lib\\site-packages\\autograd\\tracer.py:48: RuntimeWarning: overflow encountered in exp\n",
      "  return f_raw(*args, **kwargs)\n",
      "C:\\Users\\lords\\AppData\\Local\\Temp\\ipykernel_1576\\706199401.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  return exp / np.sum(exp, axis=1, keepdims=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., nan,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., nan, nan],\n",
       "       [nan,  0.,  0., nan,  0.],\n",
       "       [ 0.,  0., nan,  0.,  0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_softmax_2(z_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If $c$ is very negative, then $\\exp(c)$ will underflow. This means the denominator of the softmax will become $0$, so the final result is undefined. When $c$ is very large and positive, $\\exp(c)$ will overflow, again resulting in the expression as a whole being undefined.\n",
    "\n",
    "**Reference:** Goodfellow, I. J., Bengio, Y., \\& Courville, A. (2016). *Deep Learning*. MIT Press, p. 81."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this, we can use a subtract.\n",
    "$$\n",
    "\\sigma(\\mathbf{z}) = \\sigma(\\mathbf{z} - \\max_{i} \\mathbf{z}_{i})\n",
    "$$\n",
    "this analytically does not change, because the probability of $z$ is equal to the probability of $z - \\max z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.11195195e-283, 0.00000000e+000, 0.00000000e+000,\n",
       "        1.00000000e+000, 0.00000000e+000],\n",
       "       [0.00000000e+000, 5.11195195e-283, 1.00000000e+000,\n",
       "        9.85967654e-305, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        1.00000000e+000, 1.92874985e-022],\n",
       "       [1.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        5.14820022e-131, 0.00000000e+000],\n",
       "       [7.17509597e-066, 0.00000000e+000, 1.00000000e+000,\n",
       "        7.12457641e-218, 0.00000000e+000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_softmax_2(z_flow - np.max(z_flow, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when $r = q$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma_{q}}{\\partial z_{r=q}} &=\n",
    "\\frac{\\exp(z_{q})(\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})) - \\exp(z_{q})^{2}}\n",
    "{(\\sum_{k=1}^{{n_{1}}}\\exp(z_{k}))^{2}} \\\\\n",
    "&= \\frac{\\exp(z_{q})(\\sum_{k=1}^{{n_{1}}}\\exp(z_{k}) - \\exp(z_{q}))}\n",
    "{(\\sum_{k=1}^{{n_{1}}}\\exp(z_{k}))^{2}} \\\\\n",
    "&= \\frac{\\exp(z_{q})}{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})}\n",
    "\\left( \n",
    "    \\frac{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k}) - \\exp(z_{q})}\n",
    "    {\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})}\n",
    "\\right) \\\\\n",
    "&= \\sigma(\\mathbf{z})_{q} \\left(\n",
    "    \\frac{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})}{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})} -\n",
    "    \\frac{\\exp(z_{q})}{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})}\n",
    "\\right) \\\\\n",
    "&= \\sigma(\\mathbf{z})_{q} (1 - \\sigma(\\mathbf{z})_{q})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when $r \\neq q$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma_{q}}{\\partial z_{r \\neq q}} &=\n",
    "- \\frac{\\exp(z_{q}) \\exp(z_{r})}\n",
    "{(\\sum_{k=1}^{{n_{1}}}\\exp(z_{k}))^{2}} \\\\\n",
    "&= - \\frac{\\exp(z_{q})}{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})} \\left(\n",
    "    \\frac{\\exp(z_{r})}{\\sum_{k=1}^{{n_{1}}}\\exp(z_{k})}\n",
    "\\right) \\\\\n",
    "&= - \\sigma(\\mathbf{z})_{q} \\sigma(\\mathbf{z})_{r}\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
